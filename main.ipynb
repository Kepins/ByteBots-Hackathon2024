{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cea473-23b3-4806-8a99-0c1e59dc8dea",
   "metadata": {},
   "source": [
    "# Thyroid Disease Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c385569-6843-4602-bccd-8efdb9bf0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "df = pd.read_csv(\"thyroidDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14924645-c9c9-49bc-b268-80d1f13ed085",
   "metadata": {},
   "source": [
    "## Data feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93fd52-25c5-4241-9086-cefdf7707859",
   "metadata": {},
   "source": [
    "### Initital columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb966f50-3ed5-4c90-b4fd-362f1287db2b",
   "metadata": {},
   "source": [
    "These are all the columns present in the dataset:\n",
    "\n",
    "1. age - age of the patient (int)\n",
    "2. sex - sex patient identifies (str)\n",
    "3. on_thyroxine - whether patient is on thyroxine (bool)\n",
    "4. query on thyroxine - *whether patient is on thyroxine (bool)\n",
    "5. on antithyroid meds - whether patient is on antithyroid meds (bool)\n",
    "6. sick - whether patient is sick (bool)\n",
    "7. pregnant - whether patient is pregnant (bool)\n",
    "8. thyroid_surgery - whether patient has undergone thyroid surgery (bool)\n",
    "9. I131_treatment - whether patient is undergoing I131 treatment (bool)\n",
    "10. query_hypothyroid - whether patient believes they have hypothyroid (bool)\n",
    "11. query_hyperthyroid - whether patient believes they have hyperthyroid (bool)\n",
    "12. lithium - whether patient * lithium (bool)\n",
    "13. goitre - whether patient has goitre (bool)\n",
    "14. tumor - whether patient has tumor (bool)\n",
    "15. hypopituitary - whether patient * hyperpituitary gland (float)\n",
    "16. psych - whether patient * psych (bool)\n",
    "17. TSH_measured - whether TSH was measured in the blood (bool)\n",
    "18. TSH - TSH level in blood from lab work (float)\n",
    "19. T3_measured - whether T3 was measured in the blood (bool)\n",
    "20. T3 - T3 level in blood from lab work (float)\n",
    "21. TT4_measured - whether TT4 was measured in the blood (bool)\n",
    "22. TT4 - TT4 level in blood from lab work (float)\n",
    "23. T4U_measured - whether T4U was measured in the blood (bool)\n",
    "24. T4U - T4U level in blood from lab work (float)\n",
    "25. FTI_measured - whether FTI was measured in the blood (bool)\n",
    "26. FTI - FTI level in blood from lab work (float)\n",
    "27. TBG_measured - whether TBG was measured in the blood (bool)\n",
    "28. TBG - TBG level in blood from lab work (float)\n",
    "29. referral_source - (str)\n",
    "30. target - hyperthyroidism medical diagnosis (str)\n",
    "31. patient_id - unique id of the patient (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5bbd0-7678-4447-872d-18d8a0ac7f1e",
   "metadata": {},
   "source": [
    "### Targets from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13899f3a-f706-4023-aaea-5a787d2cc06c",
   "metadata": {},
   "source": [
    "This are the initial target defined in df:\n",
    "1. hyperthyroid conditions:\n",
    "- A   hyperthyroid\n",
    "- B   T3 toxic\n",
    "- C   toxic goitre\n",
    "- D   secondary toxic\n",
    "  \n",
    "2. hypothyroid conditions:\n",
    "- E   hypothyroid\n",
    "- F   primary hypothyroid\n",
    "- G   compensated hypothyroid\n",
    "- H   secondary hypothyroid\n",
    "\n",
    "3. binding protein:\n",
    "- I   increased binding protein\n",
    "- J   decreased binding protein\n",
    "\n",
    "4. general health:\n",
    "- K   concurrent non-thyroidal illness\n",
    "\n",
    "5. replacement therapy:\n",
    "- L   consistent with replacement therapy\n",
    "- M   underreplaced\n",
    "- N   overreplaced\n",
    "\n",
    "6. antithyroid treatment:\n",
    "- O   antithyroid drugs\n",
    "- P   I131 treatment\n",
    "- Q   surgery\n",
    "\n",
    "7. miscellaneous:\n",
    "- R   discordant assay results\n",
    "- S   elevated TBG\n",
    "- T   elevated thyroid hormones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb3fb6-96c4-47f2-acbf-4e498ed355d6",
   "metadata": {},
   "source": [
    "Since there is little data we are focusing on distinguishing patients with <b>hyperthyroid</b>, <b>hypothyroid</b> condtions or <b>none of these</b>.\n",
    "\n",
    "There needs to be conversion to new <b>targets</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb2bb81-0b12-4693-835e-621d2d7320c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_targets = [\"-\"]\n",
    "hyperthyroid_targets = [\"A\", \"B\", \"C\", \"D\"]\n",
    "hypothyroid_targets = [\"E\", \"F\", \"G\", \"H\"]\n",
    "\n",
    "def convert_to_new_target(old_target):\n",
    "    if any([x in old_target for x in negative_targets]):\n",
    "        return \"negative\"\n",
    "    if any([x in old_target for x in hyperthyroid_targets]):\n",
    "        return \"hyperthyroid\"\n",
    "    if any([x in old_target for x in hypothyroid_targets]):\n",
    "        return \"hypothyroid\"\n",
    "    return None\n",
    "\n",
    "df[\"target\"] = df[\"target\"].map(convert_to_new_target)\n",
    "df.dropna(subset = ['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2541212b-7851-4b5f-a7c5-c65753d72824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "negative        6771\n",
       "hypothyroid      667\n",
       "hyperthyroid     241\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caad193-bfc0-4474-a8d0-0461d1405b38",
   "metadata": {},
   "source": [
    "### Removing redundant columns\n",
    "Columns that have artifial values and information about measurement that is repesented by empty value in associated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd29c10-ed57-4a5d-8769-9e551f98c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_columns = [\n",
    "    'TSH_measured',\n",
    "    'T3_measured',\n",
    "    'TT4_measured',\n",
    "    'T4U_measured',\n",
    "    'FTI_measured',\n",
    "    'TBG_measured',\n",
    "    'referral_source',\n",
    "    'patient_id',\n",
    "]\n",
    "df.drop(rem_columns, axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9134cf6a-0053-4e4b-b0e8-be01cc9d616d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'on_thyroxine', 'query_on_thyroxine',\n",
       "       'on_antithyroid_meds', 'sick', 'pregnant', 'thyroid_surgery',\n",
       "       'I131_treatment', 'query_hypothyroid', 'query_hyperthyroid', 'lithium',\n",
       "       'goitre', 'tumor', 'hypopituitary', 'psych', 'TSH', 'T3', 'TT4', 'T4U',\n",
       "       'FTI', 'TBG', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20fd66-48a9-4206-8bba-fae71d29380a",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4cfde7-00b7-41fd-bc67-14f6fff47fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>7679.0</td>\n",
       "      <td>77.640839</td>\n",
       "      <td>1293.909497</td>\n",
       "      <td>1.000</td>\n",
       "      <td>37.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>65526.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>6955.0</td>\n",
       "      <td>5.500684</td>\n",
       "      <td>25.978304</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>530.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>5470.0</td>\n",
       "      <td>2.010773</td>\n",
       "      <td>0.818738</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>7325.0</td>\n",
       "      <td>105.497565</td>\n",
       "      <td>33.125317</td>\n",
       "      <td>2.000</td>\n",
       "      <td>87.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>6998.0</td>\n",
       "      <td>0.967297</td>\n",
       "      <td>0.164388</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>7005.0</td>\n",
       "      <td>110.941312</td>\n",
       "      <td>37.167537</td>\n",
       "      <td>1.400</td>\n",
       "      <td>93.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>839.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>259.0</td>\n",
       "      <td>22.955019</td>\n",
       "      <td>6.088392</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        mean          std    min    25%     50%     75%       max\n",
       "age  7679.0   77.640839  1293.909497  1.000  37.00   55.00   67.00  65526.00\n",
       "TSH  6955.0    5.500684    25.978304  0.005   0.55    1.40    2.70    530.00\n",
       "T3   5470.0    2.010773     0.818738  0.050   1.60    1.90    2.30     18.00\n",
       "TT4  7325.0  105.497565    33.125317  2.000  87.00  103.00  121.00    430.00\n",
       "T4U  6998.0    0.967297     0.164388  0.190   0.87    0.96    1.06      2.12\n",
       "FTI  7005.0  110.941312    37.167537  1.400  93.00  108.00  125.00    839.00\n",
       "TBG   259.0   22.955019     6.088392  0.100  20.00   23.00   27.00     45.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc6868-0ae2-422a-8df8-c8e64c32be33",
   "metadata": {},
   "source": [
    "## Fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2922f-73df-4a42-a125-d2e63721715a",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc96b4-814a-41fc-b129-437192c2e8b0",
   "metadata": {},
   "source": [
    "Without deep analysis we can see there is a problem with age column since max is 65526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f863b88-f898-4225-afd3-c65a093af62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>455</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5710</th>\n",
       "      <td>65511</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>65512</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8105</th>\n",
       "      <td>65526</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    target\n",
       "7355     97  negative\n",
       "790      97  negative\n",
       "7356     97  negative\n",
       "2976    455  negative\n",
       "5710  65511  negative\n",
       "6392  65512  negative\n",
       "8105  65526  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"age\")[-7:][[\"age\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99ea855-c2c3-43a7-8a25-b647f452f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['age'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fb72ec-c6ca-4ae1-88c0-501baf11c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>7675.0</td>\n",
       "      <td>52.013029</td>\n",
       "      <td>18.654684</td>\n",
       "      <td>1.000</td>\n",
       "      <td>37.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>97.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>6951.0</td>\n",
       "      <td>5.503416</td>\n",
       "      <td>25.985525</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>530.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>5467.0</td>\n",
       "      <td>2.010633</td>\n",
       "      <td>0.818893</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>7321.0</td>\n",
       "      <td>105.490324</td>\n",
       "      <td>33.132392</td>\n",
       "      <td>2.000</td>\n",
       "      <td>87.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>6994.0</td>\n",
       "      <td>0.967268</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>7001.0</td>\n",
       "      <td>110.937565</td>\n",
       "      <td>37.176408</td>\n",
       "      <td>1.400</td>\n",
       "      <td>93.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>839.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>259.0</td>\n",
       "      <td>22.955019</td>\n",
       "      <td>6.088392</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        mean        std    min    25%     50%     75%     max\n",
       "age  7675.0   52.013029  18.654684  1.000  37.00   55.00   67.00   97.00\n",
       "TSH  6951.0    5.503416  25.985525  0.005   0.55    1.40    2.70  530.00\n",
       "T3   5467.0    2.010633   0.818893  0.050   1.60    1.90    2.30   18.00\n",
       "TT4  7321.0  105.490324  33.132392  2.000  87.00  103.00  121.00  430.00\n",
       "T4U  6994.0    0.967268   0.164410  0.190   0.87    0.96    1.06    2.12\n",
       "FTI  7001.0  110.937565  37.176408  1.400  93.00  108.00  125.00  839.00\n",
       "TBG   259.0   22.955019   6.088392  0.100  20.00   23.00   27.00   45.00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19658207-fff5-4a0b-a2f7-eccf41fb20df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>7421</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>5006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on_thyroxine</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>6825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on_antithyroid_meds</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sick</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnant</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I131_treatment</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_hyperthyroid</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lithium</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goitre</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tumor</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypopituitary</th>\n",
       "      <td>7675</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>7675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psych</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>7675</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>6767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count unique       top  freq\n",
       "sex                  7421      2         F  5006\n",
       "on_thyroxine         7675      2         f  6825\n",
       "query_on_thyroxine   7675      2         f  7552\n",
       "on_antithyroid_meds  7675      2         f  7583\n",
       "sick                 7675      2         f  7388\n",
       "pregnant             7675      2         f  7635\n",
       "thyroid_surgery      7675      2         f  7569\n",
       "I131_treatment       7675      2         f  7534\n",
       "query_hypothyroid    7675      2         f  7155\n",
       "query_hyperthyroid   7675      2         f  7115\n",
       "lithium              7675      2         f  7589\n",
       "goitre               7675      2         f  7601\n",
       "tumor                7675      2         f  7472\n",
       "hypopituitary        7675      1         f  7675\n",
       "psych                7675      2         f  7294\n",
       "target               7675      3  negative  6767"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a74c71-fcda-4aff-8dc9-9be304cf39c8",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d218ad32-06c9-41dc-a255-5437732395c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>3.309446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>9.433225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>28.768730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>4.612378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>8.872964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>8.781759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>96.625407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing Values %\n",
       "sex          3.309446\n",
       "TSH          9.433225\n",
       "T3          28.768730\n",
       "TT4          4.612378\n",
       "T4U          8.872964\n",
       "FTI          8.781759\n",
       "TBG         96.625407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847d42b-7e10-4038-81dc-1ff49bc04e53",
   "metadata": {},
   "source": [
    "Drop TBG since almost whole column is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef1622c-4a43-4409-89dd-0f76f739b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"TBG\"], axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6206922d-4bc7-4af4-960e-6771ab272313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>on_antithyroid_meds</th>\n",
       "      <th>sick</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <th>I131_treatment</th>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <th>...</th>\n",
       "      <th>goitre</th>\n",
       "      <th>tumor</th>\n",
       "      <th>hypopituitary</th>\n",
       "      <th>psych</th>\n",
       "      <th>TSH</th>\n",
       "      <th>T3</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9166</th>\n",
       "      <td>70</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>119.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9167</th>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>77.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9168</th>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>99.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9170</th>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>88.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9171</th>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>65.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7675 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age sex on_thyroxine query_on_thyroxine on_antithyroid_meds sick  \\\n",
       "0      29   F            f                  f                   f    f   \n",
       "1      29   F            f                  f                   f    f   \n",
       "2      41   F            f                  f                   f    f   \n",
       "3      36   F            f                  f                   f    f   \n",
       "5      60   F            f                  f                   f    f   \n",
       "...   ...  ..          ...                ...                 ...  ...   \n",
       "9166   70   F            f                  f                   f    f   \n",
       "9167   56   M            f                  f                   f    f   \n",
       "9168   22   M            f                  f                   f    f   \n",
       "9170   47   F            f                  f                   f    f   \n",
       "9171   31   M            f                  f                   f    f   \n",
       "\n",
       "     pregnant thyroid_surgery I131_treatment query_hypothyroid  ... goitre  \\\n",
       "0           f               f              f                 t  ...      f   \n",
       "1           f               f              f                 f  ...      f   \n",
       "2           f               f              f                 f  ...      f   \n",
       "3           f               f              f                 f  ...      f   \n",
       "5           f               f              f                 f  ...      f   \n",
       "...       ...             ...            ...               ...  ...    ...   \n",
       "9166        f               f              f                 f  ...      f   \n",
       "9167        f               f              f                 f  ...      f   \n",
       "9168        f               f              f                 f  ...      f   \n",
       "9170        f               f              f                 f  ...      f   \n",
       "9171        f               f              f                 t  ...      f   \n",
       "\n",
       "     tumor hypopituitary psych  TSH   T3    TT4   T4U    FTI    target  \n",
       "0        f             f     f  0.3  NaN    NaN   NaN    NaN  negative  \n",
       "1        f             f     f  1.6  1.9  128.0   NaN    NaN  negative  \n",
       "2        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "3        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "5        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "...    ...           ...   ...  ...  ...    ...   ...    ...       ...  \n",
       "9166     f             f     f  NaN  NaN   88.0  0.74  119.0  negative  \n",
       "9167     f             f     f  NaN  NaN   64.0  0.83   77.0  negative  \n",
       "9168     f             f     f  NaN  NaN   91.0  0.92   99.0  negative  \n",
       "9170     f             f     f  NaN  NaN   75.0  0.85   88.0  negative  \n",
       "9171     f             f     f  NaN  NaN   66.0  1.02   65.0  negative  \n",
       "\n",
       "[7675 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4cb293-178a-44da-a0d7-deb4454fcb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('f', 0, inplace=True)\n",
    "df.replace('t', 1, inplace=True)\n",
    "df.replace('M', 0, inplace=True)\n",
    "df.replace('F', 1, inplace=True)\n",
    "\n",
    "target_map = {\n",
    "    \"negative\": 1,\n",
    "    \"hyperthyroid\": 2,\n",
    "    \"hypothyroid\": 0,\n",
    "}\n",
    "\n",
    "df[\"target\"] = df[\"target\"].map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ab1b3c-c3bd-47fc-b926-5f5150054674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>3.309446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>9.433225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>28.768730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>4.612378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>8.872964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>8.781759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing Values %\n",
       "sex          3.309446\n",
       "TSH          9.433225\n",
       "T3          28.768730\n",
       "TT4          4.612378\n",
       "T4U          8.872964\n",
       "FTI          8.781759"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eff832-dfb9-47c7-ae36-c44dbfdf3a5a",
   "metadata": {},
   "source": [
    "### Correlation to target\n",
    "bigger value suggests hyperthyroid </br>\n",
    "lower value suggests hypothyroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1733ff6-2c48-406b-8931-8d3fe747b9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                   -0.053016\n",
       "sex                   -0.026486\n",
       "on_thyroxine           0.033243\n",
       "query_on_thyroxine     0.002528\n",
       "on_antithyroid_meds    0.028592\n",
       "sick                   0.001881\n",
       "pregnant               0.065143\n",
       "thyroid_surgery        0.006195\n",
       "I131_treatment        -0.003355\n",
       "query_hypothyroid     -0.094901\n",
       "query_hyperthyroid     0.096053\n",
       "lithium               -0.004473\n",
       "goitre                 0.016134\n",
       "tumor                  0.058048\n",
       "hypopituitary               NaN\n",
       "psych                  0.016165\n",
       "TSH                   -0.399907\n",
       "T3                     0.447307\n",
       "TT4                    0.505067\n",
       "T4U                   -0.068169\n",
       "FTI                    0.534878\n",
       "target                 1.000000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f162555-3d09-40a3-a55e-ec64ff80b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "\n",
    "seed(1337)\n",
    "\n",
    "def get_impute_value(df, col, target):\n",
    "    match col:\n",
    "        case \"age\" | \"TSH\" | \"T3\" | \"TT4\" | \"T4U\" | \"FTI\" | \"target\":\n",
    "            median_for_target = df.loc[df['target'] == target][col].median()\n",
    "            return median_for_target\n",
    "        case _:\n",
    "            prob = df.loc[df['target'] == target][col].value_counts(normalize=True)\n",
    "            return 1 if random() < prob[1] else 0\n",
    "\n",
    "def imputed_df(df):\n",
    "    df = df.copy()\n",
    "    null_cols = df.columns[df.isnull().any()]\n",
    "    for null_col in null_cols:\n",
    "        print(f\"Imputing column: {null_col}\")\n",
    "        df[null_col] = df.apply(\n",
    "            lambda row: get_impute_value(df, null_col, row[\"target\"]) if pd.isnull(row[null_col]) else row[null_col],\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e295ad91-a7ea-4534-a063-7990484a1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing column: sex\n",
      "Imputing column: TSH\n",
      "Imputing column: T3\n",
      "Imputing column: TT4\n",
      "Imputing column: T4U\n",
      "Imputing column: FTI\n"
     ]
    }
   ],
   "source": [
    "df = imputed_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1698dbab-43e0-4913-9918-f945b900a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Values %]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93f2a5-ab65-4b94-a54a-bbe92c0256da",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f47a658-a265-45d2-acbc-94388dd4da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1).copy()\n",
    "y = df['target'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be826f-4d8a-4bdb-bd68-50da18acbbb6",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "404616de-5fee-4f08-ad06-f20a77ae4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63a0c6ef-42e4-483d-b4fa-3d5a440cc6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9869723814486712"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "430edea8-1b14-4a31-aff8-313c8a55de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.95808383, 0.99468085, 0.85      ])\n",
      "specificity=array([0.99771689, 0.93832599, 0.99623453])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ae4dc2bcbc0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"hypothyroid\", \"negative\", \"hyperthyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "220fb68f-2eed-4c7b-b04e-05cb84cce3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9868845880635195)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test , y_test_pred,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9465832-9628-415e-a0a2-c86efba0c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"decisiontree.bin\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d03c4e-0124-494e-b108-4e02d91f81ab",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0f7b9a3f-3fff-42da-9f17-2582e13031da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier(5)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9e9df1f-33c0-4e3b-a16d-382a37af465b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948931735278791"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75e375ef-61ea-49b7-a336-769422f27550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.56886228, 0.99349882, 0.75      ])\n",
      "specificity=array([0.99657534, 0.61674009, 0.99731038])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ae44f11ffb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"hypothyroid\", \"negative\", \"hyperthyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ac9b1bd-65da-4d7a-9d23-cc90d08acbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9440139302016692)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test , y_test_pred,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd860e01-556f-4e48-bd3a-cc9c856cd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"knn.bin\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781d8ba-bf09-416a-9e5a-522b39bc470b",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f40de43-e8d0-4f5d-8632-26ca594d917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5eb77b8-69bb-4ace-a95e-d434d25ef311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 08:04:47.448017: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-29 08:04:47.463716: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732863887.483026   54158 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732863887.488787   54158 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 08:04:47.508693: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/szymon/hackathon2025/repo/venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1732863890.830530   54158 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6264 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732863893.109376   54290 service.cc:148] XLA service 0x76ff980046d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732863893.109406   54290 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-11-29 08:04:53.137538: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732863893.289825   54290 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-29 08:04:54.414310: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 104 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:54.749802: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:54.958363: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 824 bytes spill stores, 544 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.253354: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 284 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.344525: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.432018: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.445654: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.489473: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.525370: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188_0', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.752184: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:55.944265: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.104490: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 300 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.190648: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 44 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.457520: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_195', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.583932: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.635904: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 276 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.666579: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.683942: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 4480 bytes spill stores, 4476 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.712977: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:56.960831: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:57.453329: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:57.693132: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 200 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:57.733725: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:57.878793: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 2060 bytes spill stores, 1928 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:57.977405: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:58.257856: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:58.487219: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_312', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:58.705493: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:58.782531: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350_0', 736 bytes spill stores, 664 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:58.932384: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 928 bytes spill stores, 932 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:58.932431: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352_0', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:58.958659: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 1412 bytes spill stores, 1300 bytes spill loads\n",
      "\n",
      "2024-11-29 08:04:59.059371: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352_0', 664 bytes spill stores, 664 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.1023 - loss: 1.5916"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732863901.111780   54290 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-29 08:05:02.314043: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:02.464222: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:02.519363: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:02.629354: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:03.082666: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49_0', 784 bytes spill stores, 736 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:03.292056: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:03.334935: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:03.489346: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 272 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:03.753272: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:03.753622: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56_0', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:04.008536: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 280 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:04.128825: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 248 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2024-11-29 08:05:04.295822: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 1304 bytes spill stores, 1444 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88976, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13s/step - accuracy: 0.1023 - loss: 1.5916 - val_accuracy: 0.8898 - val_loss: 2.1878\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8847 - loss: 2.4658\n",
      "Epoch 2: val_accuracy improved from 0.88976 to 0.90365, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.8847 - loss: 2.4658 - val_accuracy: 0.9036 - val_loss: 1.8678\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8964 - loss: 2.1525\n",
      "Epoch 3: val_accuracy did not improve from 0.90365\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8964 - loss: 2.1525 - val_accuracy: 0.0816 - val_loss: 4.0932\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.0882 - loss: 4.3620\n",
      "Epoch 4: val_accuracy improved from 0.90365 to 0.90538, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.0882 - loss: 4.3620 - val_accuracy: 0.9054 - val_loss: 2.3533\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8977 - loss: 2.6748\n",
      "Epoch 5: val_accuracy did not improve from 0.90538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8977 - loss: 2.6748 - val_accuracy: 0.9010 - val_loss: 3.0693\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8955 - loss: 3.4308\n",
      "Epoch 6: val_accuracy did not improve from 0.90538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8955 - loss: 3.4308 - val_accuracy: 0.9010 - val_loss: 3.3287\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8953 - loss: 3.6983\n",
      "Epoch 7: val_accuracy did not improve from 0.90538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8953 - loss: 3.6983 - val_accuracy: 0.8984 - val_loss: 3.3123\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8949 - loss: 3.6589\n",
      "Epoch 8: val_accuracy did not improve from 0.90538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.8949 - loss: 3.6589 - val_accuracy: 0.9010 - val_loss: 3.0976\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.8955 - loss: 3.4017\n",
      "Epoch 9: val_accuracy did not improve from 0.90538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.8955 - loss: 3.4017 - val_accuracy: 0.9010 - val_loss: 2.7168\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8960 - loss: 2.9604\n",
      "Epoch 10: val_accuracy did not improve from 0.90538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8960 - loss: 2.9604 - val_accuracy: 0.9010 - val_loss: 2.1772\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8957 - loss: 2.3570\n",
      "Epoch 11: val_accuracy did not improve from 0.90538\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.8957 - loss: 2.3570 - val_accuracy: 0.9054 - val_loss: 1.5454\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8979 - loss: 1.6599\n",
      "Epoch 12: val_accuracy improved from 0.90538 to 0.90885, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8979 - loss: 1.6599 - val_accuracy: 0.9089 - val_loss: 0.9010\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9020 - loss: 0.9502\n",
      "Epoch 13: val_accuracy did not improve from 0.90885\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9020 - loss: 0.9502 - val_accuracy: 0.7960 - val_loss: 0.5790\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.7917 - loss: 0.5849\n",
      "Epoch 14: val_accuracy did not improve from 0.90885\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.7917 - loss: 0.5849 - val_accuracy: 0.3585 - val_loss: 1.4496\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3427 - loss: 1.4636\n",
      "Epoch 15: val_accuracy improved from 0.90885 to 0.91319, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.3427 - loss: 1.4636 - val_accuracy: 0.9132 - val_loss: 0.3436\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9105 - loss: 0.3508\n",
      "Epoch 16: val_accuracy improved from 0.91319 to 0.91753, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9105 - loss: 0.3508 - val_accuracy: 0.9175 - val_loss: 0.5530\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9136 - loss: 0.5835\n",
      "Epoch 17: val_accuracy did not improve from 0.91753\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9136 - loss: 0.5835 - val_accuracy: 0.9175 - val_loss: 0.7023\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9131 - loss: 0.7507\n",
      "Epoch 18: val_accuracy improved from 0.91753 to 0.91840, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9131 - loss: 0.7507 - val_accuracy: 0.9184 - val_loss: 0.7454\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9133 - loss: 0.8013\n",
      "Epoch 19: val_accuracy improved from 0.91840 to 0.92274, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9133 - loss: 0.8013 - val_accuracy: 0.9227 - val_loss: 0.7087\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9172 - loss: 0.7653\n",
      "Epoch 20: val_accuracy did not improve from 0.92274\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9172 - loss: 0.7653 - val_accuracy: 0.9210 - val_loss: 0.6534\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9196 - loss: 0.7031\n",
      "Epoch 21: val_accuracy did not improve from 0.92274\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9196 - loss: 0.7031 - val_accuracy: 0.8941 - val_loss: 0.7381\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.8977 - loss: 0.7717\n",
      "Epoch 22: val_accuracy did not improve from 0.92274\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.8977 - loss: 0.7717 - val_accuracy: 0.9210 - val_loss: 0.6253\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9207 - loss: 0.6541\n",
      "Epoch 23: val_accuracy improved from 0.92274 to 0.92795, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.9207 - loss: 0.6541 - val_accuracy: 0.9280 - val_loss: 0.5755\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9225 - loss: 0.5976\n",
      "Epoch 24: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9225 - loss: 0.5976 - val_accuracy: 0.9210 - val_loss: 0.5491\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9212 - loss: 0.5606\n",
      "Epoch 25: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9212 - loss: 0.5606 - val_accuracy: 0.9219 - val_loss: 0.4535\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9201 - loss: 0.4497\n",
      "Epoch 26: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.9201 - loss: 0.4497 - val_accuracy: 0.9236 - val_loss: 0.3555\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9209 - loss: 0.3396\n",
      "Epoch 27: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9209 - loss: 0.3396 - val_accuracy: 0.8733 - val_loss: 0.4220\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8788 - loss: 0.4110\n",
      "Epoch 28: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8788 - loss: 0.4110 - val_accuracy: 0.8325 - val_loss: 0.4477\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8380 - loss: 0.4389\n",
      "Epoch 29: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8380 - loss: 0.4389 - val_accuracy: 0.8637 - val_loss: 0.4017\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.8701 - loss: 0.3907\n",
      "Epoch 30: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.8701 - loss: 0.3907 - val_accuracy: 0.8984 - val_loss: 0.4087\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9025 - loss: 0.3949\n",
      "Epoch 31: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9025 - loss: 0.3949 - val_accuracy: 0.9123 - val_loss: 0.4255\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9120 - loss: 0.4124\n",
      "Epoch 32: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.9120 - loss: 0.4124 - val_accuracy: 0.9167 - val_loss: 0.3938\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9162 - loss: 0.3832\n",
      "Epoch 33: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9162 - loss: 0.3832 - val_accuracy: 0.9219 - val_loss: 0.3236\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9220 - loss: 0.3167\n",
      "Epoch 34: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9220 - loss: 0.3167 - val_accuracy: 0.9132 - val_loss: 0.3182\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9205 - loss: 0.3139\n",
      "Epoch 35: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9205 - loss: 0.3139 - val_accuracy: 0.8828 - val_loss: 0.3989\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8892 - loss: 0.3913\n",
      "Epoch 36: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8892 - loss: 0.3913 - val_accuracy: 0.9280 - val_loss: 0.2984\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9292 - loss: 0.2971\n",
      "Epoch 37: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9292 - loss: 0.2971 - val_accuracy: 0.9280 - val_loss: 0.3128\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9279 - loss: 0.3144\n",
      "Epoch 38: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9279 - loss: 0.3144 - val_accuracy: 0.9262 - val_loss: 0.3370\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9235 - loss: 0.3407\n",
      "Epoch 39: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9235 - loss: 0.3407 - val_accuracy: 0.9245 - val_loss: 0.3164\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9229 - loss: 0.3200\n",
      "Epoch 40: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9229 - loss: 0.3200 - val_accuracy: 0.9253 - val_loss: 0.2795\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9238 - loss: 0.2846\n",
      "Epoch 41: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9238 - loss: 0.2846 - val_accuracy: 0.9089 - val_loss: 0.2959\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9088 - loss: 0.3054\n",
      "Epoch 42: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9088 - loss: 0.3054 - val_accuracy: 0.9106 - val_loss: 0.2996\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9103 - loss: 0.3089\n",
      "Epoch 43: val_accuracy improved from 0.92795 to 0.92882, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.9103 - loss: 0.3089 - val_accuracy: 0.9288 - val_loss: 0.2618\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9309 - loss: 0.2670\n",
      "Epoch 44: val_accuracy did not improve from 0.92882\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9309 - loss: 0.2670 - val_accuracy: 0.9280 - val_loss: 0.2764\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9285 - loss: 0.2810\n",
      "Epoch 45: val_accuracy improved from 0.92882 to 0.93056, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - accuracy: 0.9285 - loss: 0.2810 - val_accuracy: 0.9306 - val_loss: 0.2868\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9290 - loss: 0.2919\n",
      "Epoch 46: val_accuracy improved from 0.93056 to 0.93490, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.9290 - loss: 0.2919 - val_accuracy: 0.9349 - val_loss: 0.2717\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9335 - loss: 0.2762\n",
      "Epoch 47: val_accuracy improved from 0.93490 to 0.94184, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.9335 - loss: 0.2762 - val_accuracy: 0.9418 - val_loss: 0.2670\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9433 - loss: 0.2701\n",
      "Epoch 48: val_accuracy improved from 0.94184 to 0.94271, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9433 - loss: 0.2701 - val_accuracy: 0.9427 - val_loss: 0.2748\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9442 - loss: 0.2766\n",
      "Epoch 49: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9442 - loss: 0.2766 - val_accuracy: 0.9332 - val_loss: 0.2537\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9370 - loss: 0.2549\n",
      "Epoch 50: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9370 - loss: 0.2549 - val_accuracy: 0.9280 - val_loss: 0.2577\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9288 - loss: 0.2566\n",
      "Epoch 51: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9288 - loss: 0.2566 - val_accuracy: 0.9253 - val_loss: 0.2668\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9272 - loss: 0.2642\n",
      "Epoch 52: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9272 - loss: 0.2642 - val_accuracy: 0.9253 - val_loss: 0.2631\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9270 - loss: 0.2603\n",
      "Epoch 53: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9270 - loss: 0.2603 - val_accuracy: 0.9253 - val_loss: 0.2576\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9301 - loss: 0.2552\n",
      "Epoch 54: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9301 - loss: 0.2552 - val_accuracy: 0.9323 - val_loss: 0.2543\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9353 - loss: 0.2522\n",
      "Epoch 55: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9353 - loss: 0.2522 - val_accuracy: 0.9375 - val_loss: 0.2487\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9398 - loss: 0.2463\n",
      "Epoch 56: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9398 - loss: 0.2463 - val_accuracy: 0.9418 - val_loss: 0.2495\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9396 - loss: 0.2486\n",
      "Epoch 57: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9396 - loss: 0.2486 - val_accuracy: 0.9418 - val_loss: 0.2507\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9394 - loss: 0.2506\n",
      "Epoch 58: val_accuracy did not improve from 0.94271\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9394 - loss: 0.2506 - val_accuracy: 0.9427 - val_loss: 0.2471\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9407 - loss: 0.2472\n",
      "Epoch 59: val_accuracy improved from 0.94271 to 0.94444, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9407 - loss: 0.2472 - val_accuracy: 0.9444 - val_loss: 0.2419\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9414 - loss: 0.2423\n",
      "Epoch 60: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9414 - loss: 0.2423 - val_accuracy: 0.9410 - val_loss: 0.2378\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9427 - loss: 0.2390\n",
      "Epoch 61: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9427 - loss: 0.2390 - val_accuracy: 0.9392 - val_loss: 0.2362\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9403 - loss: 0.2382\n",
      "Epoch 62: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9403 - loss: 0.2382 - val_accuracy: 0.9349 - val_loss: 0.2365\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9368 - loss: 0.2393\n",
      "Epoch 63: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - accuracy: 0.9368 - loss: 0.2393 - val_accuracy: 0.9349 - val_loss: 0.2364\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9355 - loss: 0.2394\n",
      "Epoch 64: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9355 - loss: 0.2394 - val_accuracy: 0.9358 - val_loss: 0.2334\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9385 - loss: 0.2368\n",
      "Epoch 65: val_accuracy did not improve from 0.94444\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.9385 - loss: 0.2368 - val_accuracy: 0.9401 - val_loss: 0.2301\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9422 - loss: 0.2340\n",
      "Epoch 66: val_accuracy improved from 0.94444 to 0.94531, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.9422 - loss: 0.2340 - val_accuracy: 0.9453 - val_loss: 0.2297\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9440 - loss: 0.2332\n",
      "Epoch 67: val_accuracy improved from 0.94531 to 0.94792, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - accuracy: 0.9440 - loss: 0.2332 - val_accuracy: 0.9479 - val_loss: 0.2297\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9448 - loss: 0.2328\n",
      "Epoch 68: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9448 - loss: 0.2328 - val_accuracy: 0.9479 - val_loss: 0.2299\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9440 - loss: 0.2324\n",
      "Epoch 69: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.9440 - loss: 0.2324 - val_accuracy: 0.9470 - val_loss: 0.2292\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9433 - loss: 0.2309\n",
      "Epoch 70: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.9433 - loss: 0.2309 - val_accuracy: 0.9470 - val_loss: 0.2271\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9442 - loss: 0.2282\n",
      "Epoch 71: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9442 - loss: 0.2282 - val_accuracy: 0.9436 - val_loss: 0.2264\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9444 - loss: 0.2272\n",
      "Epoch 72: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9444 - loss: 0.2272 - val_accuracy: 0.9418 - val_loss: 0.2261\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9431 - loss: 0.2267\n",
      "Epoch 73: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9431 - loss: 0.2267 - val_accuracy: 0.9410 - val_loss: 0.2251\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9418 - loss: 0.2252\n",
      "Epoch 74: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9418 - loss: 0.2252 - val_accuracy: 0.9401 - val_loss: 0.2249\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9418 - loss: 0.2239\n",
      "Epoch 75: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9418 - loss: 0.2239 - val_accuracy: 0.9427 - val_loss: 0.2233\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9435 - loss: 0.2222\n",
      "Epoch 76: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.9435 - loss: 0.2222 - val_accuracy: 0.9436 - val_loss: 0.2220\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9459 - loss: 0.2214\n",
      "Epoch 77: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9459 - loss: 0.2214 - val_accuracy: 0.9436 - val_loss: 0.2211\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9470 - loss: 0.2210\n",
      "Epoch 78: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9470 - loss: 0.2210 - val_accuracy: 0.9427 - val_loss: 0.2193\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9455 - loss: 0.2193\n",
      "Epoch 79: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9455 - loss: 0.2193 - val_accuracy: 0.9410 - val_loss: 0.2183\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9444 - loss: 0.2186\n",
      "Epoch 80: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9444 - loss: 0.2186 - val_accuracy: 0.9401 - val_loss: 0.2171\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9442 - loss: 0.2180\n",
      "Epoch 81: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9442 - loss: 0.2180 - val_accuracy: 0.9427 - val_loss: 0.2158\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9461 - loss: 0.2171\n",
      "Epoch 82: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9461 - loss: 0.2171 - val_accuracy: 0.9444 - val_loss: 0.2151\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9470 - loss: 0.2159\n",
      "Epoch 83: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9470 - loss: 0.2159 - val_accuracy: 0.9444 - val_loss: 0.2147\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9468 - loss: 0.2148\n",
      "Epoch 84: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9468 - loss: 0.2148 - val_accuracy: 0.9453 - val_loss: 0.2148\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9468 - loss: 0.2143\n",
      "Epoch 85: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9468 - loss: 0.2143 - val_accuracy: 0.9444 - val_loss: 0.2137\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9466 - loss: 0.2131\n",
      "Epoch 86: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9466 - loss: 0.2131 - val_accuracy: 0.9444 - val_loss: 0.2123\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9477 - loss: 0.2120\n",
      "Epoch 87: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9477 - loss: 0.2120 - val_accuracy: 0.9444 - val_loss: 0.2114\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9479 - loss: 0.2110\n",
      "Epoch 88: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9479 - loss: 0.2110 - val_accuracy: 0.9436 - val_loss: 0.2120\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9470 - loss: 0.2108\n",
      "Epoch 89: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9470 - loss: 0.2108 - val_accuracy: 0.9444 - val_loss: 0.2095\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9472 - loss: 0.2091\n",
      "Epoch 90: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9472 - loss: 0.2091 - val_accuracy: 0.9453 - val_loss: 0.2078\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9479 - loss: 0.2081\n",
      "Epoch 91: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9479 - loss: 0.2081 - val_accuracy: 0.9462 - val_loss: 0.2064\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9496 - loss: 0.2074\n",
      "Epoch 92: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9496 - loss: 0.2074 - val_accuracy: 0.9470 - val_loss: 0.2051\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9494 - loss: 0.2064\n",
      "Epoch 93: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9494 - loss: 0.2064 - val_accuracy: 0.9470 - val_loss: 0.2039\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9487 - loss: 0.2053\n",
      "Epoch 94: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.9487 - loss: 0.2053 - val_accuracy: 0.9462 - val_loss: 0.2027\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9496 - loss: 0.2041\n",
      "Epoch 95: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9496 - loss: 0.2041 - val_accuracy: 0.9470 - val_loss: 0.2013\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9503 - loss: 0.2031\n",
      "Epoch 96: val_accuracy did not improve from 0.94792\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9503 - loss: 0.2031 - val_accuracy: 0.9470 - val_loss: 0.2000\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9498 - loss: 0.2018\n",
      "Epoch 97: val_accuracy improved from 0.94792 to 0.94965, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.9498 - loss: 0.2018 - val_accuracy: 0.9497 - val_loss: 0.1991\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9503 - loss: 0.2006\n",
      "Epoch 98: val_accuracy improved from 0.94965 to 0.95052, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.9503 - loss: 0.2006 - val_accuracy: 0.9505 - val_loss: 0.1977\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9503 - loss: 0.1994\n",
      "Epoch 99: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9503 - loss: 0.1994 - val_accuracy: 0.9497 - val_loss: 0.1963\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9505 - loss: 0.1980\n",
      "Epoch 100: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9505 - loss: 0.1980 - val_accuracy: 0.9505 - val_loss: 0.1959\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9507 - loss: 0.1969\n",
      "Epoch 101: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9507 - loss: 0.1969 - val_accuracy: 0.9505 - val_loss: 0.1948\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9505 - loss: 0.1956\n",
      "Epoch 102: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9505 - loss: 0.1956 - val_accuracy: 0.9505 - val_loss: 0.1929\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9511 - loss: 0.1942\n",
      "Epoch 103: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - accuracy: 0.9511 - loss: 0.1942 - val_accuracy: 0.9505 - val_loss: 0.1919\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9513 - loss: 0.1931\n",
      "Epoch 104: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9513 - loss: 0.1931 - val_accuracy: 0.9505 - val_loss: 0.1911\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9513 - loss: 0.1918\n",
      "Epoch 105: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9513 - loss: 0.1918 - val_accuracy: 0.9505 - val_loss: 0.1900\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9520 - loss: 0.1905\n",
      "Epoch 106: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9520 - loss: 0.1905 - val_accuracy: 0.9488 - val_loss: 0.1890\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9518 - loss: 0.1893\n",
      "Epoch 107: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9518 - loss: 0.1893 - val_accuracy: 0.9505 - val_loss: 0.1878\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9518 - loss: 0.1879\n",
      "Epoch 108: val_accuracy improved from 0.95052 to 0.95139, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9518 - loss: 0.1879 - val_accuracy: 0.9514 - val_loss: 0.1864\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9516 - loss: 0.1867\n",
      "Epoch 109: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9516 - loss: 0.1867 - val_accuracy: 0.9479 - val_loss: 0.1843\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9531 - loss: 0.1855\n",
      "Epoch 110: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9531 - loss: 0.1855 - val_accuracy: 0.9505 - val_loss: 0.1826\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9522 - loss: 0.1841\n",
      "Epoch 111: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9522 - loss: 0.1841 - val_accuracy: 0.9497 - val_loss: 0.1807\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9529 - loss: 0.1828\n",
      "Epoch 112: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9529 - loss: 0.1828 - val_accuracy: 0.9488 - val_loss: 0.1791\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9535 - loss: 0.1815\n",
      "Epoch 113: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.9535 - loss: 0.1815 - val_accuracy: 0.9505 - val_loss: 0.1780\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9529 - loss: 0.1802\n",
      "Epoch 114: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9529 - loss: 0.1802 - val_accuracy: 0.9497 - val_loss: 0.1762\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9535 - loss: 0.1789\n",
      "Epoch 115: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9535 - loss: 0.1789 - val_accuracy: 0.9505 - val_loss: 0.1753\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9526 - loss: 0.1776\n",
      "Epoch 116: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.9526 - loss: 0.1776 - val_accuracy: 0.9505 - val_loss: 0.1737\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9537 - loss: 0.1762\n",
      "Epoch 117: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9537 - loss: 0.1762 - val_accuracy: 0.9505 - val_loss: 0.1725\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9542 - loss: 0.1750\n",
      "Epoch 118: val_accuracy did not improve from 0.95139\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9542 - loss: 0.1750 - val_accuracy: 0.9505 - val_loss: 0.1715\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9533 - loss: 0.1738\n",
      "Epoch 119: val_accuracy improved from 0.95139 to 0.95226, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.9533 - loss: 0.1738 - val_accuracy: 0.9523 - val_loss: 0.1696\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9546 - loss: 0.1726\n",
      "Epoch 120: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9546 - loss: 0.1726 - val_accuracy: 0.9514 - val_loss: 0.1691\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9537 - loss: 0.1714\n",
      "Epoch 121: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9537 - loss: 0.1714 - val_accuracy: 0.9523 - val_loss: 0.1670\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9548 - loss: 0.1700\n",
      "Epoch 122: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9548 - loss: 0.1700 - val_accuracy: 0.9514 - val_loss: 0.1660\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9550 - loss: 0.1686\n",
      "Epoch 123: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9550 - loss: 0.1686 - val_accuracy: 0.9514 - val_loss: 0.1646\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9550 - loss: 0.1672\n",
      "Epoch 124: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9550 - loss: 0.1672 - val_accuracy: 0.9523 - val_loss: 0.1629\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9553 - loss: 0.1660\n",
      "Epoch 125: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9553 - loss: 0.1660 - val_accuracy: 0.9514 - val_loss: 0.1622\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9550 - loss: 0.1650\n",
      "Epoch 126: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9550 - loss: 0.1650 - val_accuracy: 0.9514 - val_loss: 0.1598\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9568 - loss: 0.1637\n",
      "Epoch 127: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9568 - loss: 0.1637 - val_accuracy: 0.9514 - val_loss: 0.1590\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9550 - loss: 0.1624\n",
      "Epoch 128: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9550 - loss: 0.1624 - val_accuracy: 0.9523 - val_loss: 0.1566\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9574 - loss: 0.1609\n",
      "Epoch 129: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.9574 - loss: 0.1609 - val_accuracy: 0.9523 - val_loss: 0.1552\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9570 - loss: 0.1594\n",
      "Epoch 130: val_accuracy improved from 0.95226 to 0.95312, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9570 - loss: 0.1594 - val_accuracy: 0.9531 - val_loss: 0.1537\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9568 - loss: 0.1582\n",
      "Epoch 131: val_accuracy did not improve from 0.95312\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9568 - loss: 0.1582 - val_accuracy: 0.9531 - val_loss: 0.1520\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9576 - loss: 0.1569\n",
      "Epoch 132: val_accuracy did not improve from 0.95312\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9576 - loss: 0.1569 - val_accuracy: 0.9523 - val_loss: 0.1516\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9563 - loss: 0.1559\n",
      "Epoch 133: val_accuracy did not improve from 0.95312\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9563 - loss: 0.1559 - val_accuracy: 0.9523 - val_loss: 0.1495\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9576 - loss: 0.1550\n",
      "Epoch 134: val_accuracy did not improve from 0.95312\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.9576 - loss: 0.1550 - val_accuracy: 0.9514 - val_loss: 0.1503\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9548 - loss: 0.1543\n",
      "Epoch 135: val_accuracy improved from 0.95312 to 0.95486, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9548 - loss: 0.1543 - val_accuracy: 0.9549 - val_loss: 0.1469\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9587 - loss: 0.1527\n",
      "Epoch 136: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.9587 - loss: 0.1527 - val_accuracy: 0.9531 - val_loss: 0.1466\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9568 - loss: 0.1508\n",
      "Epoch 137: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9568 - loss: 0.1508 - val_accuracy: 0.9531 - val_loss: 0.1445\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9579 - loss: 0.1491\n",
      "Epoch 138: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9579 - loss: 0.1491 - val_accuracy: 0.9531 - val_loss: 0.1430\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9583 - loss: 0.1482\n",
      "Epoch 139: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9583 - loss: 0.1482 - val_accuracy: 0.9531 - val_loss: 0.1433\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9563 - loss: 0.1475\n",
      "Epoch 140: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9563 - loss: 0.1475 - val_accuracy: 0.9549 - val_loss: 0.1403\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9613 - loss: 0.1461\n",
      "Epoch 141: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9613 - loss: 0.1461 - val_accuracy: 0.9531 - val_loss: 0.1401\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9574 - loss: 0.1447\n",
      "Epoch 142: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9574 - loss: 0.1447 - val_accuracy: 0.9540 - val_loss: 0.1370\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9605 - loss: 0.1429\n",
      "Epoch 143: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9605 - loss: 0.1429 - val_accuracy: 0.9523 - val_loss: 0.1359\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9592 - loss: 0.1415\n",
      "Epoch 144: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9592 - loss: 0.1415 - val_accuracy: 0.9523 - val_loss: 0.1353\n",
      "Epoch 145/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9585 - loss: 0.1404\n",
      "Epoch 145: val_accuracy improved from 0.95486 to 0.95573, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.9585 - loss: 0.1404 - val_accuracy: 0.9557 - val_loss: 0.1335\n",
      "Epoch 146/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9622 - loss: 0.1396\n",
      "Epoch 146: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9622 - loss: 0.1396 - val_accuracy: 0.9523 - val_loss: 0.1349\n",
      "Epoch 147/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9589 - loss: 0.1391\n",
      "Epoch 147: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9589 - loss: 0.1391 - val_accuracy: 0.9557 - val_loss: 0.1319\n",
      "Epoch 148/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9624 - loss: 0.1384\n",
      "Epoch 148: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9624 - loss: 0.1384 - val_accuracy: 0.9523 - val_loss: 0.1340\n",
      "Epoch 149/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9579 - loss: 0.1379\n",
      "Epoch 149: val_accuracy improved from 0.95573 to 0.95660, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.9579 - loss: 0.1379 - val_accuracy: 0.9566 - val_loss: 0.1297\n",
      "Epoch 150/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9626 - loss: 0.1359\n",
      "Epoch 150: val_accuracy did not improve from 0.95660\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9626 - loss: 0.1359 - val_accuracy: 0.9531 - val_loss: 0.1293\n",
      "Epoch 151/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9603 - loss: 0.1339\n",
      "Epoch 151: val_accuracy did not improve from 0.95660\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9603 - loss: 0.1339 - val_accuracy: 0.9540 - val_loss: 0.1277\n",
      "Epoch 152/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9616 - loss: 0.1327\n",
      "Epoch 152: val_accuracy did not improve from 0.95660\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9616 - loss: 0.1327 - val_accuracy: 0.9566 - val_loss: 0.1260\n",
      "Epoch 153/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9631 - loss: 0.1322\n",
      "Epoch 153: val_accuracy did not improve from 0.95660\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.9631 - loss: 0.1322 - val_accuracy: 0.9531 - val_loss: 0.1272\n",
      "Epoch 154/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9600 - loss: 0.1318\n",
      "Epoch 154: val_accuracy did not improve from 0.95660\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9600 - loss: 0.1318 - val_accuracy: 0.9557 - val_loss: 0.1239\n",
      "Epoch 155/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9631 - loss: 0.1304\n",
      "Epoch 155: val_accuracy did not improve from 0.95660\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9631 - loss: 0.1304 - val_accuracy: 0.9557 - val_loss: 0.1239\n",
      "Epoch 156/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9624 - loss: 0.1290\n",
      "Epoch 156: val_accuracy improved from 0.95660 to 0.95747, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9624 - loss: 0.1290 - val_accuracy: 0.9575 - val_loss: 0.1220\n",
      "Epoch 157/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9633 - loss: 0.1278\n",
      "Epoch 157: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step - accuracy: 0.9633 - loss: 0.1278 - val_accuracy: 0.9575 - val_loss: 0.1210\n",
      "Epoch 158/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9646 - loss: 0.1270\n",
      "Epoch 158: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9646 - loss: 0.1270 - val_accuracy: 0.9557 - val_loss: 0.1219\n",
      "Epoch 159/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9620 - loss: 0.1266\n",
      "Epoch 159: val_accuracy improved from 0.95747 to 0.95920, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.9620 - loss: 0.1266 - val_accuracy: 0.9592 - val_loss: 0.1194\n",
      "Epoch 160/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9661 - loss: 0.1261\n",
      "Epoch 160: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9661 - loss: 0.1261 - val_accuracy: 0.9557 - val_loss: 0.1209\n",
      "Epoch 161/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9616 - loss: 0.1254\n",
      "Epoch 161: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9616 - loss: 0.1254 - val_accuracy: 0.9583 - val_loss: 0.1176\n",
      "Epoch 162/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9657 - loss: 0.1241\n",
      "Epoch 162: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9657 - loss: 0.1241 - val_accuracy: 0.9566 - val_loss: 0.1179\n",
      "Epoch 163/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9631 - loss: 0.1229\n",
      "Epoch 163: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9631 - loss: 0.1229 - val_accuracy: 0.9557 - val_loss: 0.1158\n",
      "Epoch 164/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9650 - loss: 0.1217\n",
      "Epoch 164: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9650 - loss: 0.1217 - val_accuracy: 0.9575 - val_loss: 0.1148\n",
      "Epoch 165/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9652 - loss: 0.1208\n",
      "Epoch 165: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.9652 - loss: 0.1208 - val_accuracy: 0.9575 - val_loss: 0.1144\n",
      "Epoch 166/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9652 - loss: 0.1202\n",
      "Epoch 166: val_accuracy improved from 0.95920 to 0.96007, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.9652 - loss: 0.1202 - val_accuracy: 0.9601 - val_loss: 0.1128\n",
      "Epoch 167/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9666 - loss: 0.1196\n",
      "Epoch 167: val_accuracy did not improve from 0.96007\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9666 - loss: 0.1196 - val_accuracy: 0.9575 - val_loss: 0.1139\n",
      "Epoch 168/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9637 - loss: 0.1191\n",
      "Epoch 168: val_accuracy improved from 0.96007 to 0.96094, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9637 - loss: 0.1191 - val_accuracy: 0.9609 - val_loss: 0.1113\n",
      "Epoch 169/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9670 - loss: 0.1185\n",
      "Epoch 169: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9670 - loss: 0.1185 - val_accuracy: 0.9566 - val_loss: 0.1126\n",
      "Epoch 170/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9646 - loss: 0.1179\n",
      "Epoch 170: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9646 - loss: 0.1179 - val_accuracy: 0.9609 - val_loss: 0.1095\n",
      "Epoch 171/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9672 - loss: 0.1170\n",
      "Epoch 171: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9672 - loss: 0.1170 - val_accuracy: 0.9575 - val_loss: 0.1107\n",
      "Epoch 172/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9650 - loss: 0.1162\n",
      "Epoch 172: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.9650 - loss: 0.1162 - val_accuracy: 0.9609 - val_loss: 0.1078\n",
      "Epoch 173/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9674 - loss: 0.1152\n",
      "Epoch 173: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9674 - loss: 0.1152 - val_accuracy: 0.9575 - val_loss: 0.1081\n",
      "Epoch 174/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9652 - loss: 0.1143\n",
      "Epoch 174: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.9652 - loss: 0.1143 - val_accuracy: 0.9592 - val_loss: 0.1061\n",
      "Epoch 175/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9666 - loss: 0.1134\n",
      "Epoch 175: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9666 - loss: 0.1134 - val_accuracy: 0.9601 - val_loss: 0.1060\n",
      "Epoch 176/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9663 - loss: 0.1126\n",
      "Epoch 176: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9663 - loss: 0.1126 - val_accuracy: 0.9592 - val_loss: 0.1053\n",
      "Epoch 177/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9666 - loss: 0.1119\n",
      "Epoch 177: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9666 - loss: 0.1119 - val_accuracy: 0.9592 - val_loss: 0.1041\n",
      "Epoch 178/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9672 - loss: 0.1114\n",
      "Epoch 178: val_accuracy did not improve from 0.96094\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - accuracy: 0.9672 - loss: 0.1114 - val_accuracy: 0.9583 - val_loss: 0.1046\n",
      "Epoch 179/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9659 - loss: 0.1110\n",
      "Epoch 179: val_accuracy improved from 0.96094 to 0.96354, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.9659 - loss: 0.1110 - val_accuracy: 0.9635 - val_loss: 0.1027\n",
      "Epoch 180/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9679 - loss: 0.1107\n",
      "Epoch 180: val_accuracy did not improve from 0.96354\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9679 - loss: 0.1107 - val_accuracy: 0.9583 - val_loss: 0.1049\n",
      "Epoch 181/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9659 - loss: 0.1109\n",
      "Epoch 181: val_accuracy improved from 0.96354 to 0.96701, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.9659 - loss: 0.1109 - val_accuracy: 0.9670 - val_loss: 0.1019\n",
      "Epoch 182/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9698 - loss: 0.1112\n",
      "Epoch 182: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9698 - loss: 0.1112 - val_accuracy: 0.9566 - val_loss: 0.1076\n",
      "Epoch 183/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9652 - loss: 0.1127\n",
      "Epoch 183: val_accuracy improved from 0.96701 to 0.96875, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.9652 - loss: 0.1127 - val_accuracy: 0.9688 - val_loss: 0.1015\n",
      "Epoch 184/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9711 - loss: 0.1112\n",
      "Epoch 184: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9711 - loss: 0.1112 - val_accuracy: 0.9583 - val_loss: 0.1039\n",
      "Epoch 185/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9661 - loss: 0.1094\n",
      "Epoch 185: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9661 - loss: 0.1094 - val_accuracy: 0.9644 - val_loss: 0.0989\n",
      "Epoch 186/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9679 - loss: 0.1065\n",
      "Epoch 186: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9679 - loss: 0.1065 - val_accuracy: 0.9644 - val_loss: 0.0983\n",
      "Epoch 187/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9694 - loss: 0.1066\n",
      "Epoch 187: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9694 - loss: 0.1066 - val_accuracy: 0.9575 - val_loss: 0.1030\n",
      "Epoch 188/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9663 - loss: 0.1084\n",
      "Epoch 188: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9663 - loss: 0.1084 - val_accuracy: 0.9688 - val_loss: 0.0975\n",
      "Epoch 189/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9713 - loss: 0.1069\n",
      "Epoch 189: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9713 - loss: 0.1069 - val_accuracy: 0.9627 - val_loss: 0.0979\n",
      "Epoch 190/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9676 - loss: 0.1047\n",
      "Epoch 190: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.9676 - loss: 0.1047 - val_accuracy: 0.9644 - val_loss: 0.0967\n",
      "Epoch 191/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9683 - loss: 0.1038\n",
      "Epoch 191: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9683 - loss: 0.1038 - val_accuracy: 0.9679 - val_loss: 0.0956\n",
      "Epoch 192/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9718 - loss: 0.1046\n",
      "Epoch 192: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9718 - loss: 0.1046 - val_accuracy: 0.9609 - val_loss: 0.0990\n",
      "Epoch 193/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9674 - loss: 0.1050\n",
      "Epoch 193: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9674 - loss: 0.1050 - val_accuracy: 0.9661 - val_loss: 0.0942\n",
      "Epoch 194/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9707 - loss: 0.1028\n",
      "Epoch 194: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9707 - loss: 0.1028 - val_accuracy: 0.9644 - val_loss: 0.0941\n",
      "Epoch 195/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9692 - loss: 0.1017\n",
      "Epoch 195: val_accuracy did not improve from 0.96875\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9692 - loss: 0.1017 - val_accuracy: 0.9609 - val_loss: 0.0959\n",
      "Epoch 196/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9681 - loss: 0.1021\n",
      "Epoch 196: val_accuracy improved from 0.96875 to 0.96962, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.9681 - loss: 0.1021 - val_accuracy: 0.9696 - val_loss: 0.0928\n",
      "Epoch 197/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9715 - loss: 0.1018\n",
      "Epoch 197: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9715 - loss: 0.1018 - val_accuracy: 0.9627 - val_loss: 0.0938\n",
      "Epoch 198/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9681 - loss: 0.1007\n",
      "Epoch 198: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9681 - loss: 0.1007 - val_accuracy: 0.9644 - val_loss: 0.0922\n",
      "Epoch 199/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9694 - loss: 0.0997\n",
      "Epoch 199: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9694 - loss: 0.0997 - val_accuracy: 0.9661 - val_loss: 0.0913\n",
      "Epoch 200/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9709 - loss: 0.0996\n",
      "Epoch 200: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.9709 - loss: 0.0996 - val_accuracy: 0.9627 - val_loss: 0.0932\n",
      "Epoch 201/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9676 - loss: 0.0997\n",
      "Epoch 201: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9676 - loss: 0.0997 - val_accuracy: 0.9688 - val_loss: 0.0903\n",
      "Epoch 202/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9711 - loss: 0.0989\n",
      "Epoch 202: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9711 - loss: 0.0989 - val_accuracy: 0.9653 - val_loss: 0.0906\n",
      "Epoch 203/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9685 - loss: 0.0980\n",
      "Epoch 203: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9685 - loss: 0.0980 - val_accuracy: 0.9661 - val_loss: 0.0902\n",
      "Epoch 204/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9689 - loss: 0.0975\n",
      "Epoch 204: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9689 - loss: 0.0975 - val_accuracy: 0.9679 - val_loss: 0.0889\n",
      "Epoch 205/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9715 - loss: 0.0975\n",
      "Epoch 205: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9715 - loss: 0.0975 - val_accuracy: 0.9627 - val_loss: 0.0904\n",
      "Epoch 206/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9683 - loss: 0.0973\n",
      "Epoch 206: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9683 - loss: 0.0973 - val_accuracy: 0.9688 - val_loss: 0.0882\n",
      "Epoch 207/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9718 - loss: 0.0965\n",
      "Epoch 207: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9718 - loss: 0.0965 - val_accuracy: 0.9653 - val_loss: 0.0884\n",
      "Epoch 208/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9700 - loss: 0.0958\n",
      "Epoch 208: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9700 - loss: 0.0958 - val_accuracy: 0.9661 - val_loss: 0.0876\n",
      "Epoch 209/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9702 - loss: 0.0954\n",
      "Epoch 209: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9702 - loss: 0.0954 - val_accuracy: 0.9688 - val_loss: 0.0868\n",
      "Epoch 210/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9722 - loss: 0.0952\n",
      "Epoch 210: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9722 - loss: 0.0952 - val_accuracy: 0.9653 - val_loss: 0.0882\n",
      "Epoch 211/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9683 - loss: 0.0950\n",
      "Epoch 211: val_accuracy improved from 0.96962 to 0.97135, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.9683 - loss: 0.0950 - val_accuracy: 0.9714 - val_loss: 0.0859\n",
      "Epoch 212/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9722 - loss: 0.0946\n",
      "Epoch 212: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9722 - loss: 0.0946 - val_accuracy: 0.9644 - val_loss: 0.0867\n",
      "Epoch 213/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9689 - loss: 0.0940\n",
      "Epoch 213: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9689 - loss: 0.0940 - val_accuracy: 0.9688 - val_loss: 0.0854\n",
      "Epoch 214/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9713 - loss: 0.0934\n",
      "Epoch 214: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9713 - loss: 0.0934 - val_accuracy: 0.9688 - val_loss: 0.0849\n",
      "Epoch 215/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9713 - loss: 0.0929\n",
      "Epoch 215: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9713 - loss: 0.0929 - val_accuracy: 0.9679 - val_loss: 0.0850\n",
      "Epoch 216/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9700 - loss: 0.0927\n",
      "Epoch 216: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9700 - loss: 0.0927 - val_accuracy: 0.9714 - val_loss: 0.0837\n",
      "Epoch 217/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9726 - loss: 0.0924\n",
      "Epoch 217: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9726 - loss: 0.0924 - val_accuracy: 0.9661 - val_loss: 0.0851\n",
      "Epoch 218/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9698 - loss: 0.0922\n",
      "Epoch 218: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - accuracy: 0.9698 - loss: 0.0922 - val_accuracy: 0.9714 - val_loss: 0.0830\n",
      "Epoch 219/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9726 - loss: 0.0917\n",
      "Epoch 219: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9726 - loss: 0.0917 - val_accuracy: 0.9679 - val_loss: 0.0838\n",
      "Epoch 220/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9709 - loss: 0.0912\n",
      "Epoch 220: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9709 - loss: 0.0912 - val_accuracy: 0.9696 - val_loss: 0.0827\n",
      "Epoch 221/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9724 - loss: 0.0906\n",
      "Epoch 221: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9724 - loss: 0.0906 - val_accuracy: 0.9696 - val_loss: 0.0825\n",
      "Epoch 222/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9715 - loss: 0.0901\n",
      "Epoch 222: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9715 - loss: 0.0901 - val_accuracy: 0.9696 - val_loss: 0.0821\n",
      "Epoch 223/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9718 - loss: 0.0897\n",
      "Epoch 223: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9718 - loss: 0.0897 - val_accuracy: 0.9688 - val_loss: 0.0815\n",
      "Epoch 224/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9726 - loss: 0.0894\n",
      "Epoch 224: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9726 - loss: 0.0894 - val_accuracy: 0.9679 - val_loss: 0.0819\n",
      "Epoch 225/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9720 - loss: 0.0891\n",
      "Epoch 225: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9720 - loss: 0.0891 - val_accuracy: 0.9705 - val_loss: 0.0805\n",
      "Epoch 226/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9735 - loss: 0.0889\n",
      "Epoch 226: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.9735 - loss: 0.0889 - val_accuracy: 0.9688 - val_loss: 0.0817\n",
      "Epoch 227/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9724 - loss: 0.0886\n",
      "Epoch 227: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9724 - loss: 0.0886 - val_accuracy: 0.9714 - val_loss: 0.0802\n",
      "Epoch 228/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9746 - loss: 0.0882\n",
      "Epoch 228: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9746 - loss: 0.0882 - val_accuracy: 0.9688 - val_loss: 0.0807\n",
      "Epoch 229/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9726 - loss: 0.0878\n",
      "Epoch 229: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9726 - loss: 0.0878 - val_accuracy: 0.9714 - val_loss: 0.0790\n",
      "Epoch 230/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9739 - loss: 0.0874\n",
      "Epoch 230: val_accuracy did not improve from 0.97135\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9739 - loss: 0.0874 - val_accuracy: 0.9688 - val_loss: 0.0799\n",
      "Epoch 231/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9731 - loss: 0.0870\n",
      "Epoch 231: val_accuracy improved from 0.97135 to 0.97222, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.9731 - loss: 0.0870 - val_accuracy: 0.9722 - val_loss: 0.0783\n",
      "Epoch 232/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9748 - loss: 0.0867\n",
      "Epoch 232: val_accuracy did not improve from 0.97222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9748 - loss: 0.0867 - val_accuracy: 0.9679 - val_loss: 0.0796\n",
      "Epoch 233/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9726 - loss: 0.0867\n",
      "Epoch 233: val_accuracy did not improve from 0.97222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9726 - loss: 0.0867 - val_accuracy: 0.9722 - val_loss: 0.0776\n",
      "Epoch 234/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9757 - loss: 0.0868\n",
      "Epoch 234: val_accuracy did not improve from 0.97222\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9757 - loss: 0.0868 - val_accuracy: 0.9653 - val_loss: 0.0814\n",
      "Epoch 235/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9707 - loss: 0.0879\n",
      "Epoch 235: val_accuracy improved from 0.97222 to 0.97656, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.9707 - loss: 0.0879 - val_accuracy: 0.9766 - val_loss: 0.0793\n",
      "Epoch 236/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9791 - loss: 0.0889\n",
      "Epoch 236: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9791 - loss: 0.0889 - val_accuracy: 0.9627 - val_loss: 0.0874\n",
      "Epoch 237/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9687 - loss: 0.0927\n",
      "Epoch 237: val_accuracy improved from 0.97656 to 0.97830, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - accuracy: 0.9687 - loss: 0.0927 - val_accuracy: 0.9783 - val_loss: 0.0804\n",
      "Epoch 238/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9794 - loss: 0.0901\n",
      "Epoch 238: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9794 - loss: 0.0901 - val_accuracy: 0.9661 - val_loss: 0.0837\n",
      "Epoch 239/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9694 - loss: 0.0884\n",
      "Epoch 239: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9694 - loss: 0.0884 - val_accuracy: 0.9714 - val_loss: 0.0768\n",
      "Epoch 240/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9763 - loss: 0.0841\n",
      "Epoch 240: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9763 - loss: 0.0841 - val_accuracy: 0.9722 - val_loss: 0.0768\n",
      "Epoch 241/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9770 - loss: 0.0843\n",
      "Epoch 241: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9770 - loss: 0.0843 - val_accuracy: 0.9670 - val_loss: 0.0830\n",
      "Epoch 242/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9698 - loss: 0.0873\n",
      "Epoch 242: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9698 - loss: 0.0873 - val_accuracy: 0.9774 - val_loss: 0.0772\n",
      "Epoch 243/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9794 - loss: 0.0858\n",
      "Epoch 243: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9794 - loss: 0.0858 - val_accuracy: 0.9688 - val_loss: 0.0776\n",
      "Epoch 244/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9728 - loss: 0.0836\n",
      "Epoch 244: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 186ms/step - accuracy: 0.9728 - loss: 0.0836 - val_accuracy: 0.9714 - val_loss: 0.0749\n",
      "Epoch 245/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9750 - loss: 0.0820\n",
      "Epoch 245: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 0.9750 - loss: 0.0820 - val_accuracy: 0.9748 - val_loss: 0.0752\n",
      "Epoch 246/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9778 - loss: 0.0831\n",
      "Epoch 246: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9778 - loss: 0.0831 - val_accuracy: 0.9670 - val_loss: 0.0801\n",
      "Epoch 247/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9700 - loss: 0.0851\n",
      "Epoch 247: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9700 - loss: 0.0851 - val_accuracy: 0.9774 - val_loss: 0.0750\n",
      "Epoch 248/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9794 - loss: 0.0834\n",
      "Epoch 248: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9794 - loss: 0.0834 - val_accuracy: 0.9688 - val_loss: 0.0752\n",
      "Epoch 249/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9737 - loss: 0.0813\n",
      "Epoch 249: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9737 - loss: 0.0813 - val_accuracy: 0.9705 - val_loss: 0.0746\n",
      "Epoch 250/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9752 - loss: 0.0806\n",
      "Epoch 250: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9752 - loss: 0.0806 - val_accuracy: 0.9740 - val_loss: 0.0743\n",
      "Epoch 251/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9794 - loss: 0.0814\n",
      "Epoch 251: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9794 - loss: 0.0814 - val_accuracy: 0.9670 - val_loss: 0.0775\n",
      "Epoch 252/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9715 - loss: 0.0823\n",
      "Epoch 252: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9715 - loss: 0.0823 - val_accuracy: 0.9722 - val_loss: 0.0734\n",
      "Epoch 253/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9781 - loss: 0.0805\n",
      "Epoch 253: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9781 - loss: 0.0805 - val_accuracy: 0.9705 - val_loss: 0.0735\n",
      "Epoch 254/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9755 - loss: 0.0793\n",
      "Epoch 254: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9755 - loss: 0.0793 - val_accuracy: 0.9688 - val_loss: 0.0735\n",
      "Epoch 255/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9748 - loss: 0.0791\n",
      "Epoch 255: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9748 - loss: 0.0791 - val_accuracy: 0.9722 - val_loss: 0.0724\n",
      "Epoch 256/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9785 - loss: 0.0797\n",
      "Epoch 256: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9785 - loss: 0.0797 - val_accuracy: 0.9679 - val_loss: 0.0750\n",
      "Epoch 257/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9731 - loss: 0.0799\n",
      "Epoch 257: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9731 - loss: 0.0799 - val_accuracy: 0.9740 - val_loss: 0.0724\n",
      "Epoch 258/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9789 - loss: 0.0789\n",
      "Epoch 258: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9789 - loss: 0.0789 - val_accuracy: 0.9705 - val_loss: 0.0725\n",
      "Epoch 259/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9750 - loss: 0.0779\n",
      "Epoch 259: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - accuracy: 0.9750 - loss: 0.0779 - val_accuracy: 0.9705 - val_loss: 0.0716\n",
      "Epoch 260/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9761 - loss: 0.0775\n",
      "Epoch 260: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9761 - loss: 0.0775 - val_accuracy: 0.9722 - val_loss: 0.0714\n",
      "Epoch 261/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9783 - loss: 0.0775\n",
      "Epoch 261: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9783 - loss: 0.0775 - val_accuracy: 0.9688 - val_loss: 0.0733\n",
      "Epoch 262/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9755 - loss: 0.0778\n",
      "Epoch 262: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9755 - loss: 0.0778 - val_accuracy: 0.9740 - val_loss: 0.0712\n",
      "Epoch 263/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9802 - loss: 0.0776\n",
      "Epoch 263: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9802 - loss: 0.0776 - val_accuracy: 0.9688 - val_loss: 0.0725\n",
      "Epoch 264/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9750 - loss: 0.0774\n",
      "Epoch 264: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9750 - loss: 0.0774 - val_accuracy: 0.9748 - val_loss: 0.0707\n",
      "Epoch 265/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9800 - loss: 0.0765\n",
      "Epoch 265: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9800 - loss: 0.0765 - val_accuracy: 0.9705 - val_loss: 0.0714\n",
      "Epoch 266/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9763 - loss: 0.0760\n",
      "Epoch 266: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9763 - loss: 0.0760 - val_accuracy: 0.9705 - val_loss: 0.0704\n",
      "Epoch 267/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9763 - loss: 0.0755\n",
      "Epoch 267: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9763 - loss: 0.0755 - val_accuracy: 0.9714 - val_loss: 0.0698\n",
      "Epoch 268/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9781 - loss: 0.0754\n",
      "Epoch 268: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9781 - loss: 0.0754 - val_accuracy: 0.9705 - val_loss: 0.0708\n",
      "Epoch 269/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9755 - loss: 0.0753\n",
      "Epoch 269: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.9755 - loss: 0.0753 - val_accuracy: 0.9757 - val_loss: 0.0698\n",
      "Epoch 270/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9805 - loss: 0.0754\n",
      "Epoch 270: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.9805 - loss: 0.0754 - val_accuracy: 0.9688 - val_loss: 0.0717\n",
      "Epoch 271/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9757 - loss: 0.0757\n",
      "Epoch 271: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9757 - loss: 0.0757 - val_accuracy: 0.9774 - val_loss: 0.0700\n",
      "Epoch 272/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9815 - loss: 0.0760\n",
      "Epoch 272: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9815 - loss: 0.0760 - val_accuracy: 0.9688 - val_loss: 0.0734\n",
      "Epoch 273/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9737 - loss: 0.0766\n",
      "Epoch 273: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9737 - loss: 0.0766 - val_accuracy: 0.9774 - val_loss: 0.0702\n",
      "Epoch 274/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9815 - loss: 0.0759\n",
      "Epoch 274: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9815 - loss: 0.0759 - val_accuracy: 0.9688 - val_loss: 0.0723\n",
      "Epoch 275/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9744 - loss: 0.0752\n",
      "Epoch 275: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9744 - loss: 0.0752 - val_accuracy: 0.9757 - val_loss: 0.0691\n",
      "Epoch 276/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9809 - loss: 0.0736\n",
      "Epoch 276: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9809 - loss: 0.0736 - val_accuracy: 0.9714 - val_loss: 0.0689\n",
      "Epoch 277/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9770 - loss: 0.0729\n",
      "Epoch 277: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9770 - loss: 0.0729 - val_accuracy: 0.9705 - val_loss: 0.0691\n",
      "Epoch 278/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9763 - loss: 0.0729\n",
      "Epoch 278: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9763 - loss: 0.0729 - val_accuracy: 0.9766 - val_loss: 0.0684\n",
      "Epoch 279/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9818 - loss: 0.0733\n",
      "Epoch 279: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9818 - loss: 0.0733 - val_accuracy: 0.9696 - val_loss: 0.0709\n",
      "Epoch 280/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9755 - loss: 0.0738\n",
      "Epoch 280: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9755 - loss: 0.0738 - val_accuracy: 0.9774 - val_loss: 0.0688\n",
      "Epoch 281/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9815 - loss: 0.0737\n",
      "Epoch 281: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9815 - loss: 0.0737 - val_accuracy: 0.9688 - val_loss: 0.0709\n",
      "Epoch 282/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9750 - loss: 0.0737\n",
      "Epoch 282: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9750 - loss: 0.0737 - val_accuracy: 0.9774 - val_loss: 0.0679\n",
      "Epoch 283/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9813 - loss: 0.0725\n",
      "Epoch 283: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9813 - loss: 0.0725 - val_accuracy: 0.9714 - val_loss: 0.0686\n",
      "Epoch 284/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9768 - loss: 0.0716\n",
      "Epoch 284: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9768 - loss: 0.0716 - val_accuracy: 0.9748 - val_loss: 0.0671\n",
      "Epoch 285/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9798 - loss: 0.0709\n",
      "Epoch 285: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9798 - loss: 0.0709 - val_accuracy: 0.9731 - val_loss: 0.0672\n",
      "Epoch 286/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9783 - loss: 0.0705\n",
      "Epoch 286: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9783 - loss: 0.0705 - val_accuracy: 0.9714 - val_loss: 0.0676\n",
      "Epoch 287/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9776 - loss: 0.0704\n",
      "Epoch 287: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9776 - loss: 0.0704 - val_accuracy: 0.9757 - val_loss: 0.0667\n",
      "Epoch 288/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9811 - loss: 0.0704\n",
      "Epoch 288: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9811 - loss: 0.0704 - val_accuracy: 0.9696 - val_loss: 0.0681\n",
      "Epoch 289/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9763 - loss: 0.0706\n",
      "Epoch 289: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9763 - loss: 0.0706 - val_accuracy: 0.9783 - val_loss: 0.0666\n",
      "Epoch 290/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9818 - loss: 0.0704\n",
      "Epoch 290: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9818 - loss: 0.0704 - val_accuracy: 0.9705 - val_loss: 0.0684\n",
      "Epoch 291/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9770 - loss: 0.0703\n",
      "Epoch 291: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9770 - loss: 0.0703 - val_accuracy: 0.9774 - val_loss: 0.0665\n",
      "Epoch 292/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9822 - loss: 0.0701\n",
      "Epoch 292: val_accuracy did not improve from 0.97830\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9822 - loss: 0.0701 - val_accuracy: 0.9696 - val_loss: 0.0684\n",
      "Epoch 293/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9768 - loss: 0.0701\n",
      "Epoch 293: val_accuracy improved from 0.97830 to 0.98003, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.9768 - loss: 0.0701 - val_accuracy: 0.9800 - val_loss: 0.0666\n",
      "Epoch 294/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9824 - loss: 0.0698\n",
      "Epoch 294: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9824 - loss: 0.0698 - val_accuracy: 0.9714 - val_loss: 0.0685\n",
      "Epoch 295/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9770 - loss: 0.0698\n",
      "Epoch 295: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9770 - loss: 0.0698 - val_accuracy: 0.9783 - val_loss: 0.0662\n",
      "Epoch 296/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9822 - loss: 0.0693\n",
      "Epoch 296: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9822 - loss: 0.0693 - val_accuracy: 0.9714 - val_loss: 0.0677\n",
      "Epoch 297/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9765 - loss: 0.0688\n",
      "Epoch 297: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9765 - loss: 0.0688 - val_accuracy: 0.9757 - val_loss: 0.0658\n",
      "Epoch 298/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9813 - loss: 0.0680\n",
      "Epoch 298: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9813 - loss: 0.0680 - val_accuracy: 0.9722 - val_loss: 0.0661\n",
      "Epoch 299/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9781 - loss: 0.0674\n",
      "Epoch 299: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9781 - loss: 0.0674 - val_accuracy: 0.9757 - val_loss: 0.0650\n",
      "Epoch 300/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9807 - loss: 0.0670\n",
      "Epoch 300: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9807 - loss: 0.0670 - val_accuracy: 0.9748 - val_loss: 0.0651\n",
      "Epoch 301/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9798 - loss: 0.0666\n",
      "Epoch 301: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9798 - loss: 0.0666 - val_accuracy: 0.9748 - val_loss: 0.0651\n",
      "Epoch 302/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9800 - loss: 0.0664\n",
      "Epoch 302: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9800 - loss: 0.0664 - val_accuracy: 0.9748 - val_loss: 0.0645\n",
      "Epoch 303/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9802 - loss: 0.0661\n",
      "Epoch 303: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9802 - loss: 0.0661 - val_accuracy: 0.9740 - val_loss: 0.0650\n",
      "Epoch 304/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9794 - loss: 0.0659\n",
      "Epoch 304: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9794 - loss: 0.0659 - val_accuracy: 0.9766 - val_loss: 0.0647\n",
      "Epoch 305/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9815 - loss: 0.0659\n",
      "Epoch 305: val_accuracy did not improve from 0.98003\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9815 - loss: 0.0659 - val_accuracy: 0.9714 - val_loss: 0.0660\n",
      "Epoch 306/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9776 - loss: 0.0660\n",
      "Epoch 306: val_accuracy improved from 0.98003 to 0.98090, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.9776 - loss: 0.0660 - val_accuracy: 0.9809 - val_loss: 0.0652\n",
      "Epoch 307/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9831 - loss: 0.0669\n",
      "Epoch 307: val_accuracy did not improve from 0.98090\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9831 - loss: 0.0669 - val_accuracy: 0.9696 - val_loss: 0.0716\n",
      "Epoch 308/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9746 - loss: 0.0697\n",
      "Epoch 308: val_accuracy improved from 0.98090 to 0.98264, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.9746 - loss: 0.0697 - val_accuracy: 0.9826 - val_loss: 0.0708\n",
      "Epoch 309/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9841 - loss: 0.0728\n",
      "Epoch 309: val_accuracy did not improve from 0.98264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9841 - loss: 0.0728 - val_accuracy: 0.9627 - val_loss: 0.0882\n",
      "Epoch 310/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9696 - loss: 0.0833\n",
      "Epoch 310: val_accuracy did not improve from 0.98264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9696 - loss: 0.0833 - val_accuracy: 0.9800 - val_loss: 0.0711\n",
      "Epoch 311/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9815 - loss: 0.0737\n",
      "Epoch 311: val_accuracy did not improve from 0.98264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9815 - loss: 0.0737 - val_accuracy: 0.9722 - val_loss: 0.0684\n",
      "Epoch 312/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9770 - loss: 0.0661\n",
      "Epoch 312: val_accuracy did not improve from 0.98264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9770 - loss: 0.0661 - val_accuracy: 0.9722 - val_loss: 0.0675\n",
      "Epoch 313/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9785 - loss: 0.0653\n",
      "Epoch 313: val_accuracy did not improve from 0.98264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9785 - loss: 0.0653 - val_accuracy: 0.9826 - val_loss: 0.0692\n",
      "Epoch 314/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9835 - loss: 0.0707\n",
      "Epoch 314: val_accuracy did not improve from 0.98264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9835 - loss: 0.0707 - val_accuracy: 0.9653 - val_loss: 0.0790\n",
      "Epoch 315/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9724 - loss: 0.0754\n",
      "Epoch 315: val_accuracy did not improve from 0.98264\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9724 - loss: 0.0754 - val_accuracy: 0.9792 - val_loss: 0.0637\n",
      "Epoch 316/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9818 - loss: 0.0643\n",
      "Epoch 316: val_accuracy improved from 0.98264 to 0.98438, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.9818 - loss: 0.0643 - val_accuracy: 0.9844 - val_loss: 0.0670\n",
      "Epoch 317/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9850 - loss: 0.0678\n",
      "Epoch 317: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9850 - loss: 0.0678 - val_accuracy: 0.9653 - val_loss: 0.0829\n",
      "Epoch 318/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9685 - loss: 0.0787\n",
      "Epoch 318: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9685 - loss: 0.0787 - val_accuracy: 0.9835 - val_loss: 0.0658\n",
      "Epoch 319/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9837 - loss: 0.0674\n",
      "Epoch 319: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9837 - loss: 0.0674 - val_accuracy: 0.9792 - val_loss: 0.0638\n",
      "Epoch 320/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9813 - loss: 0.0647\n",
      "Epoch 320: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9813 - loss: 0.0647 - val_accuracy: 0.9688 - val_loss: 0.0738\n",
      "Epoch 321/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9731 - loss: 0.0696\n",
      "Epoch 321: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9731 - loss: 0.0696 - val_accuracy: 0.9809 - val_loss: 0.0672\n",
      "Epoch 322/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9846 - loss: 0.0658\n",
      "Epoch 322: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9846 - loss: 0.0658 - val_accuracy: 0.9748 - val_loss: 0.0638\n",
      "Epoch 323/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9813 - loss: 0.0620\n",
      "Epoch 323: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9813 - loss: 0.0620 - val_accuracy: 0.9740 - val_loss: 0.0662\n",
      "Epoch 324/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9774 - loss: 0.0641\n",
      "Epoch 324: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9774 - loss: 0.0641 - val_accuracy: 0.9800 - val_loss: 0.0636\n",
      "Epoch 325/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9837 - loss: 0.0631\n",
      "Epoch 325: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9837 - loss: 0.0631 - val_accuracy: 0.9757 - val_loss: 0.0643\n",
      "Epoch 326/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9805 - loss: 0.0613\n",
      "Epoch 326: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9805 - loss: 0.0613 - val_accuracy: 0.9757 - val_loss: 0.0658\n",
      "Epoch 327/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9794 - loss: 0.0618\n",
      "Epoch 327: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9794 - loss: 0.0618 - val_accuracy: 0.9809 - val_loss: 0.0640\n",
      "Epoch 328/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9841 - loss: 0.0620\n",
      "Epoch 328: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9841 - loss: 0.0620 - val_accuracy: 0.9722 - val_loss: 0.0664\n",
      "Epoch 329/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9776 - loss: 0.0627\n",
      "Epoch 329: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9776 - loss: 0.0627 - val_accuracy: 0.9809 - val_loss: 0.0636\n",
      "Epoch 330/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9837 - loss: 0.0606\n",
      "Epoch 330: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9837 - loss: 0.0606 - val_accuracy: 0.9783 - val_loss: 0.0647\n",
      "Epoch 331/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9826 - loss: 0.0602\n",
      "Epoch 331: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9826 - loss: 0.0602 - val_accuracy: 0.9722 - val_loss: 0.0659\n",
      "Epoch 332/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9798 - loss: 0.0604\n",
      "Epoch 332: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9798 - loss: 0.0604 - val_accuracy: 0.9800 - val_loss: 0.0638\n",
      "Epoch 333/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9841 - loss: 0.0605\n",
      "Epoch 333: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9841 - loss: 0.0605 - val_accuracy: 0.9714 - val_loss: 0.0651\n",
      "Epoch 334/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9796 - loss: 0.0601\n",
      "Epoch 334: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.9796 - loss: 0.0601 - val_accuracy: 0.9766 - val_loss: 0.0633\n",
      "Epoch 335/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9828 - loss: 0.0588\n",
      "Epoch 335: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9828 - loss: 0.0588 - val_accuracy: 0.9792 - val_loss: 0.0634\n",
      "Epoch 336/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9835 - loss: 0.0588\n",
      "Epoch 336: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9835 - loss: 0.0588 - val_accuracy: 0.9722 - val_loss: 0.0641\n",
      "Epoch 337/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9802 - loss: 0.0588\n",
      "Epoch 337: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9802 - loss: 0.0588 - val_accuracy: 0.9800 - val_loss: 0.0630\n",
      "Epoch 338/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9839 - loss: 0.0589\n",
      "Epoch 338: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9839 - loss: 0.0589 - val_accuracy: 0.9722 - val_loss: 0.0643\n",
      "Epoch 339/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9802 - loss: 0.0582\n",
      "Epoch 339: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9802 - loss: 0.0582 - val_accuracy: 0.9792 - val_loss: 0.0640\n",
      "Epoch 340/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9831 - loss: 0.0577\n",
      "Epoch 340: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9831 - loss: 0.0577 - val_accuracy: 0.9783 - val_loss: 0.0638\n",
      "Epoch 341/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9828 - loss: 0.0574\n",
      "Epoch 341: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9828 - loss: 0.0574 - val_accuracy: 0.9748 - val_loss: 0.0638\n",
      "Epoch 342/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9798 - loss: 0.0573\n",
      "Epoch 342: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9798 - loss: 0.0573 - val_accuracy: 0.9800 - val_loss: 0.0626\n",
      "Epoch 343/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9844 - loss: 0.0573\n",
      "Epoch 343: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9844 - loss: 0.0573 - val_accuracy: 0.9757 - val_loss: 0.0639\n",
      "Epoch 344/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9802 - loss: 0.0570\n",
      "Epoch 344: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9802 - loss: 0.0570 - val_accuracy: 0.9800 - val_loss: 0.0629\n",
      "Epoch 345/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9837 - loss: 0.0566\n",
      "Epoch 345: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9837 - loss: 0.0566 - val_accuracy: 0.9766 - val_loss: 0.0633\n",
      "Epoch 346/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9813 - loss: 0.0560\n",
      "Epoch 346: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9813 - loss: 0.0560 - val_accuracy: 0.9757 - val_loss: 0.0632\n",
      "Epoch 347/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9818 - loss: 0.0559\n",
      "Epoch 347: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9818 - loss: 0.0559 - val_accuracy: 0.9792 - val_loss: 0.0627\n",
      "Epoch 348/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9835 - loss: 0.0558\n",
      "Epoch 348: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9835 - loss: 0.0558 - val_accuracy: 0.9757 - val_loss: 0.0642\n",
      "Epoch 349/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9802 - loss: 0.0559\n",
      "Epoch 349: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9802 - loss: 0.0559 - val_accuracy: 0.9809 - val_loss: 0.0626\n",
      "Epoch 350/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9848 - loss: 0.0560\n",
      "Epoch 350: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9848 - loss: 0.0560 - val_accuracy: 0.9748 - val_loss: 0.0640\n",
      "Epoch 351/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9798 - loss: 0.0559\n",
      "Epoch 351: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9798 - loss: 0.0559 - val_accuracy: 0.9800 - val_loss: 0.0620\n",
      "Epoch 352/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9846 - loss: 0.0553\n",
      "Epoch 352: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9846 - loss: 0.0553 - val_accuracy: 0.9766 - val_loss: 0.0630\n",
      "Epoch 353/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9811 - loss: 0.0548\n",
      "Epoch 353: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9811 - loss: 0.0548 - val_accuracy: 0.9792 - val_loss: 0.0620\n",
      "Epoch 354/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9839 - loss: 0.0543\n",
      "Epoch 354: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9839 - loss: 0.0543 - val_accuracy: 0.9774 - val_loss: 0.0621\n",
      "Epoch 355/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9826 - loss: 0.0539\n",
      "Epoch 355: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9826 - loss: 0.0539 - val_accuracy: 0.9774 - val_loss: 0.0621\n",
      "Epoch 356/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9820 - loss: 0.0538\n",
      "Epoch 356: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9820 - loss: 0.0538 - val_accuracy: 0.9800 - val_loss: 0.0617\n",
      "Epoch 357/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9839 - loss: 0.0537\n",
      "Epoch 357: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9839 - loss: 0.0537 - val_accuracy: 0.9766 - val_loss: 0.0627\n",
      "Epoch 358/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9818 - loss: 0.0536\n",
      "Epoch 358: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9818 - loss: 0.0536 - val_accuracy: 0.9800 - val_loss: 0.0615\n",
      "Epoch 359/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9850 - loss: 0.0536\n",
      "Epoch 359: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9850 - loss: 0.0536 - val_accuracy: 0.9766 - val_loss: 0.0630\n",
      "Epoch 360/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9805 - loss: 0.0536\n",
      "Epoch 360: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9805 - loss: 0.0536 - val_accuracy: 0.9800 - val_loss: 0.0616\n",
      "Epoch 361/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9859 - loss: 0.0536\n",
      "Epoch 361: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9859 - loss: 0.0536 - val_accuracy: 0.9757 - val_loss: 0.0642\n",
      "Epoch 362/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9796 - loss: 0.0540\n",
      "Epoch 362: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9796 - loss: 0.0540 - val_accuracy: 0.9809 - val_loss: 0.0619\n",
      "Epoch 363/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9861 - loss: 0.0541\n",
      "Epoch 363: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9861 - loss: 0.0541 - val_accuracy: 0.9731 - val_loss: 0.0655\n",
      "Epoch 364/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9787 - loss: 0.0547\n",
      "Epoch 364: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9787 - loss: 0.0547 - val_accuracy: 0.9809 - val_loss: 0.0623\n",
      "Epoch 365/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9863 - loss: 0.0544\n",
      "Epoch 365: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9863 - loss: 0.0544 - val_accuracy: 0.9740 - val_loss: 0.0662\n",
      "Epoch 366/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9787 - loss: 0.0547\n",
      "Epoch 366: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9787 - loss: 0.0547 - val_accuracy: 0.9809 - val_loss: 0.0618\n",
      "Epoch 367/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9863 - loss: 0.0536\n",
      "Epoch 367: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9863 - loss: 0.0536 - val_accuracy: 0.9748 - val_loss: 0.0642\n",
      "Epoch 368/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9800 - loss: 0.0531\n",
      "Epoch 368: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9800 - loss: 0.0531 - val_accuracy: 0.9800 - val_loss: 0.0609\n",
      "Epoch 369/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9861 - loss: 0.0519\n",
      "Epoch 369: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9861 - loss: 0.0519 - val_accuracy: 0.9783 - val_loss: 0.0619\n",
      "Epoch 370/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9815 - loss: 0.0513\n",
      "Epoch 370: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9815 - loss: 0.0513 - val_accuracy: 0.9800 - val_loss: 0.0606\n",
      "Epoch 371/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9848 - loss: 0.0508\n",
      "Epoch 371: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9848 - loss: 0.0508 - val_accuracy: 0.9783 - val_loss: 0.0606\n",
      "Epoch 372/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9846 - loss: 0.0505\n",
      "Epoch 372: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9846 - loss: 0.0505 - val_accuracy: 0.9774 - val_loss: 0.0609\n",
      "Epoch 373/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9835 - loss: 0.0503\n",
      "Epoch 373: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9835 - loss: 0.0503 - val_accuracy: 0.9800 - val_loss: 0.0604\n",
      "Epoch 374/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9854 - loss: 0.0503\n",
      "Epoch 374: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9854 - loss: 0.0503 - val_accuracy: 0.9774 - val_loss: 0.0617\n",
      "Epoch 375/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9820 - loss: 0.0504\n",
      "Epoch 375: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.9820 - loss: 0.0504 - val_accuracy: 0.9800 - val_loss: 0.0603\n",
      "Epoch 376/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9868 - loss: 0.0506\n",
      "Epoch 376: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9868 - loss: 0.0506 - val_accuracy: 0.9766 - val_loss: 0.0635\n",
      "Epoch 377/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9794 - loss: 0.0514\n",
      "Epoch 377: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9794 - loss: 0.0514 - val_accuracy: 0.9809 - val_loss: 0.0612\n",
      "Epoch 378/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9870 - loss: 0.0521\n",
      "Epoch 378: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9870 - loss: 0.0521 - val_accuracy: 0.9731 - val_loss: 0.0673\n",
      "Epoch 379/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9770 - loss: 0.0544\n",
      "Epoch 379: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9770 - loss: 0.0544 - val_accuracy: 0.9809 - val_loss: 0.0633\n",
      "Epoch 380/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9863 - loss: 0.0551\n",
      "Epoch 380: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9863 - loss: 0.0551 - val_accuracy: 0.9688 - val_loss: 0.0720\n",
      "Epoch 381/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9748 - loss: 0.0582\n",
      "Epoch 381: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9748 - loss: 0.0582 - val_accuracy: 0.9809 - val_loss: 0.0631\n",
      "Epoch 382/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9865 - loss: 0.0547\n",
      "Epoch 382: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9865 - loss: 0.0547 - val_accuracy: 0.9740 - val_loss: 0.0657\n",
      "Epoch 383/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9781 - loss: 0.0525\n",
      "Epoch 383: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9781 - loss: 0.0525 - val_accuracy: 0.9800 - val_loss: 0.0597\n",
      "Epoch 384/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9863 - loss: 0.0493\n",
      "Epoch 384: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9863 - loss: 0.0493 - val_accuracy: 0.9809 - val_loss: 0.0596\n",
      "Epoch 385/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9854 - loss: 0.0484\n",
      "Epoch 385: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9854 - loss: 0.0484 - val_accuracy: 0.9766 - val_loss: 0.0622\n",
      "Epoch 386/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9807 - loss: 0.0496\n",
      "Epoch 386: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9807 - loss: 0.0496 - val_accuracy: 0.9818 - val_loss: 0.0608\n",
      "Epoch 387/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9874 - loss: 0.0510\n",
      "Epoch 387: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9874 - loss: 0.0510 - val_accuracy: 0.9740 - val_loss: 0.0663\n",
      "Epoch 388/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9772 - loss: 0.0528\n",
      "Epoch 388: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9772 - loss: 0.0528 - val_accuracy: 0.9800 - val_loss: 0.0609\n",
      "Epoch 389/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9870 - loss: 0.0513\n",
      "Epoch 389: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9870 - loss: 0.0513 - val_accuracy: 0.9766 - val_loss: 0.0633\n",
      "Epoch 390/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9802 - loss: 0.0500\n",
      "Epoch 390: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9802 - loss: 0.0500 - val_accuracy: 0.9800 - val_loss: 0.0591\n",
      "Epoch 391/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9861 - loss: 0.0480\n",
      "Epoch 391: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9861 - loss: 0.0480 - val_accuracy: 0.9800 - val_loss: 0.0592\n",
      "Epoch 392/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9850 - loss: 0.0472\n",
      "Epoch 392: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9850 - loss: 0.0472 - val_accuracy: 0.9783 - val_loss: 0.0603\n",
      "Epoch 393/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9822 - loss: 0.0475\n",
      "Epoch 393: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9822 - loss: 0.0475 - val_accuracy: 0.9809 - val_loss: 0.0591\n",
      "Epoch 394/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9865 - loss: 0.0483\n",
      "Epoch 394: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9865 - loss: 0.0483 - val_accuracy: 0.9766 - val_loss: 0.0630\n",
      "Epoch 395/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9800 - loss: 0.0493\n",
      "Epoch 395: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9800 - loss: 0.0493 - val_accuracy: 0.9800 - val_loss: 0.0596\n",
      "Epoch 396/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9865 - loss: 0.0490\n",
      "Epoch 396: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9865 - loss: 0.0490 - val_accuracy: 0.9766 - val_loss: 0.0627\n",
      "Epoch 397/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9805 - loss: 0.0488\n",
      "Epoch 397: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9805 - loss: 0.0488 - val_accuracy: 0.9809 - val_loss: 0.0588\n",
      "Epoch 398/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9870 - loss: 0.0475\n",
      "Epoch 398: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9870 - loss: 0.0475 - val_accuracy: 0.9783 - val_loss: 0.0599\n",
      "Epoch 399/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9833 - loss: 0.0466\n",
      "Epoch 399: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9833 - loss: 0.0466 - val_accuracy: 0.9800 - val_loss: 0.0585\n",
      "Epoch 400/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9857 - loss: 0.0460\n",
      "Epoch 400: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9857 - loss: 0.0460 - val_accuracy: 0.9800 - val_loss: 0.0582\n",
      "Epoch 401/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9857 - loss: 0.0459\n",
      "Epoch 401: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9857 - loss: 0.0459 - val_accuracy: 0.9783 - val_loss: 0.0599\n",
      "Epoch 402/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9828 - loss: 0.0462\n",
      "Epoch 402: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9828 - loss: 0.0462 - val_accuracy: 0.9809 - val_loss: 0.0582\n",
      "Epoch 403/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9870 - loss: 0.0465\n",
      "Epoch 403: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9870 - loss: 0.0465 - val_accuracy: 0.9783 - val_loss: 0.0607\n",
      "Epoch 404/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9820 - loss: 0.0466\n",
      "Epoch 404: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9820 - loss: 0.0466 - val_accuracy: 0.9800 - val_loss: 0.0582\n",
      "Epoch 405/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9870 - loss: 0.0463\n",
      "Epoch 405: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9870 - loss: 0.0463 - val_accuracy: 0.9783 - val_loss: 0.0602\n",
      "Epoch 406/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9824 - loss: 0.0460\n",
      "Epoch 406: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9824 - loss: 0.0460 - val_accuracy: 0.9809 - val_loss: 0.0580\n",
      "Epoch 407/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9863 - loss: 0.0455\n",
      "Epoch 407: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9863 - loss: 0.0455 - val_accuracy: 0.9800 - val_loss: 0.0588\n",
      "Epoch 408/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9844 - loss: 0.0450\n",
      "Epoch 408: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9844 - loss: 0.0450 - val_accuracy: 0.9800 - val_loss: 0.0581\n",
      "Epoch 409/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9850 - loss: 0.0447\n",
      "Epoch 409: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9850 - loss: 0.0447 - val_accuracy: 0.9809 - val_loss: 0.0578\n",
      "Epoch 410/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9857 - loss: 0.0446\n",
      "Epoch 410: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9857 - loss: 0.0446 - val_accuracy: 0.9783 - val_loss: 0.0589\n",
      "Epoch 411/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9837 - loss: 0.0448\n",
      "Epoch 411: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9837 - loss: 0.0448 - val_accuracy: 0.9800 - val_loss: 0.0577\n",
      "Epoch 412/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9868 - loss: 0.0450\n",
      "Epoch 412: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9868 - loss: 0.0450 - val_accuracy: 0.9774 - val_loss: 0.0602\n",
      "Epoch 413/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9820 - loss: 0.0454\n",
      "Epoch 413: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9820 - loss: 0.0454 - val_accuracy: 0.9800 - val_loss: 0.0581\n",
      "Epoch 414/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9870 - loss: 0.0455\n",
      "Epoch 414: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9870 - loss: 0.0455 - val_accuracy: 0.9774 - val_loss: 0.0610\n",
      "Epoch 415/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9822 - loss: 0.0457\n",
      "Epoch 415: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9822 - loss: 0.0457 - val_accuracy: 0.9800 - val_loss: 0.0581\n",
      "Epoch 416/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9870 - loss: 0.0452\n",
      "Epoch 416: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9870 - loss: 0.0452 - val_accuracy: 0.9774 - val_loss: 0.0598\n",
      "Epoch 417/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9826 - loss: 0.0447\n",
      "Epoch 417: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9826 - loss: 0.0447 - val_accuracy: 0.9800 - val_loss: 0.0573\n",
      "Epoch 418/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9861 - loss: 0.0440\n",
      "Epoch 418: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9861 - loss: 0.0440 - val_accuracy: 0.9809 - val_loss: 0.0581\n",
      "Epoch 419/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9854 - loss: 0.0436\n",
      "Epoch 419: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9854 - loss: 0.0436 - val_accuracy: 0.9800 - val_loss: 0.0576\n",
      "Epoch 420/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9854 - loss: 0.0433\n",
      "Epoch 420: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9854 - loss: 0.0433 - val_accuracy: 0.9800 - val_loss: 0.0575\n",
      "Epoch 421/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9859 - loss: 0.0433\n",
      "Epoch 421: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9859 - loss: 0.0433 - val_accuracy: 0.9800 - val_loss: 0.0582\n",
      "Epoch 422/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9857 - loss: 0.0432\n",
      "Epoch 422: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9857 - loss: 0.0432 - val_accuracy: 0.9800 - val_loss: 0.0575\n",
      "Epoch 423/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9863 - loss: 0.0432\n",
      "Epoch 423: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9863 - loss: 0.0432 - val_accuracy: 0.9800 - val_loss: 0.0587\n",
      "Epoch 424/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9848 - loss: 0.0431\n",
      "Epoch 424: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9848 - loss: 0.0431 - val_accuracy: 0.9792 - val_loss: 0.0574\n",
      "Epoch 425/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9865 - loss: 0.0430\n",
      "Epoch 425: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9865 - loss: 0.0430 - val_accuracy: 0.9800 - val_loss: 0.0583\n",
      "Epoch 426/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9852 - loss: 0.0429\n",
      "Epoch 426: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.9852 - loss: 0.0429 - val_accuracy: 0.9792 - val_loss: 0.0572\n",
      "Epoch 427/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9865 - loss: 0.0427\n",
      "Epoch 427: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9865 - loss: 0.0427 - val_accuracy: 0.9800 - val_loss: 0.0582\n",
      "Epoch 428/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9857 - loss: 0.0426\n",
      "Epoch 428: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9857 - loss: 0.0426 - val_accuracy: 0.9792 - val_loss: 0.0574\n",
      "Epoch 429/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9865 - loss: 0.0424\n",
      "Epoch 429: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9865 - loss: 0.0424 - val_accuracy: 0.9800 - val_loss: 0.0584\n",
      "Epoch 430/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9854 - loss: 0.0424\n",
      "Epoch 430: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9854 - loss: 0.0424 - val_accuracy: 0.9792 - val_loss: 0.0574\n",
      "Epoch 431/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9868 - loss: 0.0422\n",
      "Epoch 431: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9868 - loss: 0.0422 - val_accuracy: 0.9800 - val_loss: 0.0581\n",
      "Epoch 432/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9857 - loss: 0.0421\n",
      "Epoch 432: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9857 - loss: 0.0421 - val_accuracy: 0.9792 - val_loss: 0.0572\n",
      "Epoch 433/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9863 - loss: 0.0419\n",
      "Epoch 433: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9863 - loss: 0.0419 - val_accuracy: 0.9809 - val_loss: 0.0578\n",
      "Epoch 434/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9859 - loss: 0.0418\n",
      "Epoch 434: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9859 - loss: 0.0418 - val_accuracy: 0.9792 - val_loss: 0.0574\n",
      "Epoch 435/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9861 - loss: 0.0417\n",
      "Epoch 435: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9861 - loss: 0.0417 - val_accuracy: 0.9809 - val_loss: 0.0581\n",
      "Epoch 436/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9861 - loss: 0.0416\n",
      "Epoch 436: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9861 - loss: 0.0416 - val_accuracy: 0.9792 - val_loss: 0.0575\n",
      "Epoch 437/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9863 - loss: 0.0415\n",
      "Epoch 437: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9863 - loss: 0.0415 - val_accuracy: 0.9800 - val_loss: 0.0580\n",
      "Epoch 438/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9857 - loss: 0.0414\n",
      "Epoch 438: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9857 - loss: 0.0414 - val_accuracy: 0.9792 - val_loss: 0.0573\n",
      "Epoch 439/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9863 - loss: 0.0413\n",
      "Epoch 439: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9863 - loss: 0.0413 - val_accuracy: 0.9792 - val_loss: 0.0581\n",
      "Epoch 440/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9857 - loss: 0.0413\n",
      "Epoch 440: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9857 - loss: 0.0413 - val_accuracy: 0.9792 - val_loss: 0.0572\n",
      "Epoch 441/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9872 - loss: 0.0413\n",
      "Epoch 441: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9872 - loss: 0.0413 - val_accuracy: 0.9800 - val_loss: 0.0590\n",
      "Epoch 442/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9852 - loss: 0.0415\n",
      "Epoch 442: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9852 - loss: 0.0415 - val_accuracy: 0.9792 - val_loss: 0.0576\n",
      "Epoch 443/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9872 - loss: 0.0418\n",
      "Epoch 443: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9872 - loss: 0.0418 - val_accuracy: 0.9774 - val_loss: 0.0607\n",
      "Epoch 444/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9828 - loss: 0.0424\n",
      "Epoch 444: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9828 - loss: 0.0424 - val_accuracy: 0.9792 - val_loss: 0.0582\n",
      "Epoch 445/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9878 - loss: 0.0431\n",
      "Epoch 445: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9878 - loss: 0.0431 - val_accuracy: 0.9748 - val_loss: 0.0642\n",
      "Epoch 446/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9809 - loss: 0.0450\n",
      "Epoch 446: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9809 - loss: 0.0450 - val_accuracy: 0.9800 - val_loss: 0.0603\n",
      "Epoch 447/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9887 - loss: 0.0462\n",
      "Epoch 447: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9887 - loss: 0.0462 - val_accuracy: 0.9731 - val_loss: 0.0692\n",
      "Epoch 448/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9772 - loss: 0.0491\n",
      "Epoch 448: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9772 - loss: 0.0491 - val_accuracy: 0.9792 - val_loss: 0.0613\n",
      "Epoch 449/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9889 - loss: 0.0476\n",
      "Epoch 449: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.9889 - loss: 0.0476 - val_accuracy: 0.9740 - val_loss: 0.0665\n",
      "Epoch 450/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9794 - loss: 0.0468\n",
      "Epoch 450: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9794 - loss: 0.0468 - val_accuracy: 0.9809 - val_loss: 0.0579\n",
      "Epoch 451/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9878 - loss: 0.0427\n",
      "Epoch 451: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9878 - loss: 0.0427 - val_accuracy: 0.9809 - val_loss: 0.0576\n",
      "Epoch 452/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9859 - loss: 0.0403\n",
      "Epoch 452: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9859 - loss: 0.0403 - val_accuracy: 0.9766 - val_loss: 0.0605\n",
      "Epoch 453/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9828 - loss: 0.0418\n",
      "Epoch 453: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9828 - loss: 0.0418 - val_accuracy: 0.9800 - val_loss: 0.0590\n",
      "Epoch 454/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9889 - loss: 0.0443\n",
      "Epoch 454: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9889 - loss: 0.0443 - val_accuracy: 0.9748 - val_loss: 0.0664\n",
      "Epoch 455/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9785 - loss: 0.0467\n",
      "Epoch 455: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9785 - loss: 0.0467 - val_accuracy: 0.9783 - val_loss: 0.0593\n",
      "Epoch 456/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9881 - loss: 0.0440\n",
      "Epoch 456: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9881 - loss: 0.0440 - val_accuracy: 0.9774 - val_loss: 0.0610\n",
      "Epoch 457/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9822 - loss: 0.0418\n",
      "Epoch 457: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9822 - loss: 0.0418 - val_accuracy: 0.9792 - val_loss: 0.0573\n",
      "Epoch 458/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9870 - loss: 0.0398\n",
      "Epoch 458: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9870 - loss: 0.0398 - val_accuracy: 0.9792 - val_loss: 0.0573\n",
      "Epoch 459/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9874 - loss: 0.0398\n",
      "Epoch 459: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9874 - loss: 0.0398 - val_accuracy: 0.9774 - val_loss: 0.0605\n",
      "Epoch 460/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9833 - loss: 0.0412\n",
      "Epoch 460: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9833 - loss: 0.0412 - val_accuracy: 0.9792 - val_loss: 0.0581\n",
      "Epoch 461/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9885 - loss: 0.0420\n",
      "Epoch 461: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.9885 - loss: 0.0420 - val_accuracy: 0.9774 - val_loss: 0.0624\n",
      "Epoch 462/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9822 - loss: 0.0425\n",
      "Epoch 462: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9822 - loss: 0.0425 - val_accuracy: 0.9800 - val_loss: 0.0579\n",
      "Epoch 463/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9883 - loss: 0.0412\n",
      "Epoch 463: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9883 - loss: 0.0412 - val_accuracy: 0.9783 - val_loss: 0.0596\n",
      "Epoch 464/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9839 - loss: 0.0399\n",
      "Epoch 464: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9839 - loss: 0.0399 - val_accuracy: 0.9800 - val_loss: 0.0574\n",
      "Epoch 465/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9874 - loss: 0.0390\n",
      "Epoch 465: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9874 - loss: 0.0390 - val_accuracy: 0.9792 - val_loss: 0.0573\n",
      "Epoch 466/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9876 - loss: 0.0390\n",
      "Epoch 466: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9876 - loss: 0.0390 - val_accuracy: 0.9774 - val_loss: 0.0601\n",
      "Epoch 467/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9844 - loss: 0.0400\n",
      "Epoch 467: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9844 - loss: 0.0400 - val_accuracy: 0.9800 - val_loss: 0.0578\n",
      "Epoch 468/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9878 - loss: 0.0405\n",
      "Epoch 468: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9878 - loss: 0.0405 - val_accuracy: 0.9774 - val_loss: 0.0617\n",
      "Epoch 469/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9824 - loss: 0.0409\n",
      "Epoch 469: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9824 - loss: 0.0409 - val_accuracy: 0.9792 - val_loss: 0.0579\n",
      "Epoch 470/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9881 - loss: 0.0402\n",
      "Epoch 470: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9881 - loss: 0.0402 - val_accuracy: 0.9783 - val_loss: 0.0601\n",
      "Epoch 471/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9844 - loss: 0.0395\n",
      "Epoch 471: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9844 - loss: 0.0395 - val_accuracy: 0.9792 - val_loss: 0.0573\n",
      "Epoch 472/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9878 - loss: 0.0385\n",
      "Epoch 472: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9878 - loss: 0.0385 - val_accuracy: 0.9792 - val_loss: 0.0577\n",
      "Epoch 473/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9868 - loss: 0.0382\n",
      "Epoch 473: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9868 - loss: 0.0382 - val_accuracy: 0.9783 - val_loss: 0.0587\n",
      "Epoch 474/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9859 - loss: 0.0384\n",
      "Epoch 474: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9859 - loss: 0.0384 - val_accuracy: 0.9800 - val_loss: 0.0574\n",
      "Epoch 475/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9876 - loss: 0.0388\n",
      "Epoch 475: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9876 - loss: 0.0388 - val_accuracy: 0.9783 - val_loss: 0.0602\n",
      "Epoch 476/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9844 - loss: 0.0391\n",
      "Epoch 476: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9844 - loss: 0.0391 - val_accuracy: 0.9792 - val_loss: 0.0576\n",
      "Epoch 477/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9881 - loss: 0.0391\n",
      "Epoch 477: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9881 - loss: 0.0391 - val_accuracy: 0.9792 - val_loss: 0.0600\n",
      "Epoch 478/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9850 - loss: 0.0388\n",
      "Epoch 478: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9850 - loss: 0.0388 - val_accuracy: 0.9792 - val_loss: 0.0574\n",
      "Epoch 479/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9881 - loss: 0.0382\n",
      "Epoch 479: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9881 - loss: 0.0382 - val_accuracy: 0.9800 - val_loss: 0.0583\n",
      "Epoch 480/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9863 - loss: 0.0378\n",
      "Epoch 480: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9863 - loss: 0.0378 - val_accuracy: 0.9792 - val_loss: 0.0580\n",
      "Epoch 481/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9870 - loss: 0.0376\n",
      "Epoch 481: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9870 - loss: 0.0376 - val_accuracy: 0.9792 - val_loss: 0.0576\n",
      "Epoch 482/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9885 - loss: 0.0377\n",
      "Epoch 482: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9885 - loss: 0.0377 - val_accuracy: 0.9792 - val_loss: 0.0591\n",
      "Epoch 483/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9859 - loss: 0.0379\n",
      "Epoch 483: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9859 - loss: 0.0379 - val_accuracy: 0.9800 - val_loss: 0.0576\n",
      "Epoch 484/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9881 - loss: 0.0379\n",
      "Epoch 484: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9881 - loss: 0.0379 - val_accuracy: 0.9792 - val_loss: 0.0595\n",
      "Epoch 485/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9854 - loss: 0.0378\n",
      "Epoch 485: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9854 - loss: 0.0378 - val_accuracy: 0.9800 - val_loss: 0.0577\n",
      "Epoch 486/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9881 - loss: 0.0376\n",
      "Epoch 486: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9881 - loss: 0.0376 - val_accuracy: 0.9792 - val_loss: 0.0588\n",
      "Epoch 487/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9865 - loss: 0.0373\n",
      "Epoch 487: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9865 - loss: 0.0373 - val_accuracy: 0.9792 - val_loss: 0.0579\n",
      "Epoch 488/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9876 - loss: 0.0371\n",
      "Epoch 488: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9876 - loss: 0.0371 - val_accuracy: 0.9792 - val_loss: 0.0580\n",
      "Epoch 489/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9874 - loss: 0.0370\n",
      "Epoch 489: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9874 - loss: 0.0370 - val_accuracy: 0.9800 - val_loss: 0.0587\n",
      "Epoch 490/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9868 - loss: 0.0370\n",
      "Epoch 490: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9868 - loss: 0.0370 - val_accuracy: 0.9800 - val_loss: 0.0579\n",
      "Epoch 491/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.9885 - loss: 0.0371\n",
      "Epoch 491: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9885 - loss: 0.0371 - val_accuracy: 0.9792 - val_loss: 0.0595\n",
      "Epoch 492/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9861 - loss: 0.0372\n",
      "Epoch 492: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9861 - loss: 0.0372 - val_accuracy: 0.9800 - val_loss: 0.0578\n",
      "Epoch 493/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9881 - loss: 0.0373\n",
      "Epoch 493: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.9881 - loss: 0.0373 - val_accuracy: 0.9792 - val_loss: 0.0599\n",
      "Epoch 494/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.9854 - loss: 0.0374\n",
      "Epoch 494: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9854 - loss: 0.0374 - val_accuracy: 0.9800 - val_loss: 0.0579\n",
      "Epoch 495/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9883 - loss: 0.0373\n",
      "Epoch 495: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.9883 - loss: 0.0373 - val_accuracy: 0.9792 - val_loss: 0.0600\n",
      "Epoch 496/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9859 - loss: 0.0372\n",
      "Epoch 496: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9859 - loss: 0.0372 - val_accuracy: 0.9800 - val_loss: 0.0580\n",
      "Epoch 497/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9883 - loss: 0.0369\n",
      "Epoch 497: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9883 - loss: 0.0369 - val_accuracy: 0.9792 - val_loss: 0.0595\n",
      "Epoch 498/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.9870 - loss: 0.0366\n",
      "Epoch 498: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9870 - loss: 0.0366 - val_accuracy: 0.9800 - val_loss: 0.0583\n",
      "Epoch 499/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9881 - loss: 0.0364\n",
      "Epoch 499: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9881 - loss: 0.0364 - val_accuracy: 0.9792 - val_loss: 0.0587\n",
      "Epoch 500/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9872 - loss: 0.0362\n",
      "Epoch 500: val_accuracy did not improve from 0.98438\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9872 - loss: 0.0362 - val_accuracy: 0.9792 - val_loss: 0.0587\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "num_classes = 3\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=f'MLP.keras', \n",
    "        verbose=1, \n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True, \n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=8192, verbose=1, validation_split=0.2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4329cbf6-b303-465d-b0b0-1d318d3fb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"MLP.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e136186b-e082-40d5-97d0-f00d41c89b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 08:06:12.259531: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-29 08:06:12.314750: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 08:06:12.451123: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 408 bytes spill stores, 444 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.1281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 08:06:15.018705: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2024-11-29 08:06:15.070408: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-29 08:06:15.115942: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 392 bytes spill stores, 368 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.9718 - loss: 0.1264\n",
      "Test results - Loss: 0.11357888579368591 - Accuracy: 97.76%\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "367e1c58-f9a1-4963-9f7d-c60abbe42580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_test_pred = to_categorical(np.argmax(model.predict(X_test), axis=1), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e75d39d-2bd0-428a-bd6f-7855d15e6932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.94610778, 0.98699764, 0.8       ])\n",
      "specificity=array([0.99257991, 0.91189427, 0.99462076])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7701eefc2c60>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"hypothyroid\", \"negative\", \"hyperthyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21033cf1-491b-4cec-add6-bef52f928f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
