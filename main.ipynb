{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cea473-23b3-4806-8a99-0c1e59dc8dea",
   "metadata": {},
   "source": [
    "# Thyroid Disease Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c385569-6843-4602-bccd-8efdb9bf0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "df = pd.read_csv(\"thyroidDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14924645-c9c9-49bc-b268-80d1f13ed085",
   "metadata": {},
   "source": [
    "## Data feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93fd52-25c5-4241-9086-cefdf7707859",
   "metadata": {},
   "source": [
    "### Initital columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb966f50-3ed5-4c90-b4fd-362f1287db2b",
   "metadata": {},
   "source": [
    "These are all the columns present in the dataset:\n",
    "\n",
    "1. age - age of the patient (int)\n",
    "2. sex - sex patient identifies (str)\n",
    "3. on_thyroxine - whether patient is on thyroxine (bool)\n",
    "4. query on thyroxine - *whether patient is on thyroxine (bool)\n",
    "5. on antithyroid meds - whether patient is on antithyroid meds (bool)\n",
    "6. sick - whether patient is sick (bool)\n",
    "7. pregnant - whether patient is pregnant (bool)\n",
    "8. thyroid_surgery - whether patient has undergone thyroid surgery (bool)\n",
    "9. I131_treatment - whether patient is undergoing I131 treatment (bool)\n",
    "10. query_hypothyroid - whether patient believes they have hypothyroid (bool)\n",
    "11. query_hyperthyroid - whether patient believes they have hyperthyroid (bool)\n",
    "12. lithium - whether patient * lithium (bool)\n",
    "13. goitre - whether patient has goitre (bool)\n",
    "14. tumor - whether patient has tumor (bool)\n",
    "15. hypopituitary - whether patient * hyperpituitary gland (float)\n",
    "16. psych - whether patient * psych (bool)\n",
    "17. TSH_measured - whether TSH was measured in the blood (bool)\n",
    "18. TSH - TSH level in blood from lab work (float)\n",
    "19. T3_measured - whether T3 was measured in the blood (bool)\n",
    "20. T3 - T3 level in blood from lab work (float)\n",
    "21. TT4_measured - whether TT4 was measured in the blood (bool)\n",
    "22. TT4 - TT4 level in blood from lab work (float)\n",
    "23. T4U_measured - whether T4U was measured in the blood (bool)\n",
    "24. T4U - T4U level in blood from lab work (float)\n",
    "25. FTI_measured - whether FTI was measured in the blood (bool)\n",
    "26. FTI - FTI level in blood from lab work (float)\n",
    "27. TBG_measured - whether TBG was measured in the blood (bool)\n",
    "28. TBG - TBG level in blood from lab work (float)\n",
    "29. referral_source - (str)\n",
    "30. target - hyperthyroidism medical diagnosis (str)\n",
    "31. patient_id - unique id of the patient (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5bbd0-7678-4447-872d-18d8a0ac7f1e",
   "metadata": {},
   "source": [
    "### Targets from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13899f3a-f706-4023-aaea-5a787d2cc06c",
   "metadata": {},
   "source": [
    "This are the initial target defined in df:\n",
    "1. hyperthyroid conditions:\n",
    "- A   hyperthyroid\n",
    "- B   T3 toxic\n",
    "- C   toxic goitre\n",
    "- D   secondary toxic\n",
    "  \n",
    "2. hypothyroid conditions:\n",
    "- E   hypothyroid\n",
    "- F   primary hypothyroid\n",
    "- G   compensated hypothyroid\n",
    "- H   secondary hypothyroid\n",
    "\n",
    "3. binding protein:\n",
    "- I   increased binding protein\n",
    "- J   decreased binding protein\n",
    "\n",
    "4. general health:\n",
    "- K   concurrent non-thyroidal illness\n",
    "\n",
    "5. replacement therapy:\n",
    "- L   consistent with replacement therapy\n",
    "- M   underreplaced\n",
    "- N   overreplaced\n",
    "\n",
    "6. antithyroid treatment:\n",
    "- O   antithyroid drugs\n",
    "- P   I131 treatment\n",
    "- Q   surgery\n",
    "\n",
    "7. miscellaneous:\n",
    "- R   discordant assay results\n",
    "- S   elevated TBG\n",
    "- T   elevated thyroid hormones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb3fb6-96c4-47f2-acbf-4e498ed355d6",
   "metadata": {},
   "source": [
    "Since there is little data we are focusing on distinguishing patients with <b>hyperthyroid</b>, <b>hypothyroid</b> condtions or <b>none of these</b>.\n",
    "\n",
    "There needs to be conversion to new <b>targets</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb2bb81-0b12-4693-835e-621d2d7320c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_targets = [\"-\"]\n",
    "hyperthyroid_targets = [\"A\", \"B\", \"C\", \"D\"]\n",
    "hypothyroid_targets = [\"E\", \"F\", \"G\", \"H\"]\n",
    "\n",
    "def convert_to_new_target(old_target):\n",
    "    if any([x in old_target for x in negative_targets]):\n",
    "        return \"negative\"\n",
    "    if any([x in old_target for x in hyperthyroid_targets]):\n",
    "        return \"hyperthyroid\"\n",
    "    if any([x in old_target for x in hypothyroid_targets]):\n",
    "        return \"hypothyroid\"\n",
    "    return None\n",
    "\n",
    "df[\"target\"] = df[\"target\"].map(convert_to_new_target)\n",
    "df.dropna(subset = ['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2541212b-7851-4b5f-a7c5-c65753d72824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "negative        6771\n",
       "hypothyroid      667\n",
       "hyperthyroid     241\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caad193-bfc0-4474-a8d0-0461d1405b38",
   "metadata": {},
   "source": [
    "### Removing redundant columns\n",
    "Columns that have artifial values and information about measurement that is repesented by empty value in associated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd29c10-ed57-4a5d-8769-9e551f98c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_columns = [\n",
    "    'TSH_measured',\n",
    "    'T3_measured',\n",
    "    'TT4_measured',\n",
    "    'T4U_measured',\n",
    "    'FTI_measured',\n",
    "    'TBG_measured',\n",
    "    'referral_source',\n",
    "    'patient_id',\n",
    "]\n",
    "df.drop(rem_columns, axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9134cf6a-0053-4e4b-b0e8-be01cc9d616d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'on_thyroxine', 'query_on_thyroxine',\n",
       "       'on_antithyroid_meds', 'sick', 'pregnant', 'thyroid_surgery',\n",
       "       'I131_treatment', 'query_hypothyroid', 'query_hyperthyroid', 'lithium',\n",
       "       'goitre', 'tumor', 'hypopituitary', 'psych', 'TSH', 'T3', 'TT4', 'T4U',\n",
       "       'FTI', 'TBG', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20fd66-48a9-4206-8bba-fae71d29380a",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4cfde7-00b7-41fd-bc67-14f6fff47fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>7679.0</td>\n",
       "      <td>77.640839</td>\n",
       "      <td>1293.909497</td>\n",
       "      <td>1.000</td>\n",
       "      <td>37.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>65526.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>6955.0</td>\n",
       "      <td>5.500684</td>\n",
       "      <td>25.978304</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>530.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>5470.0</td>\n",
       "      <td>2.010773</td>\n",
       "      <td>0.818738</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>7325.0</td>\n",
       "      <td>105.497565</td>\n",
       "      <td>33.125317</td>\n",
       "      <td>2.000</td>\n",
       "      <td>87.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>6998.0</td>\n",
       "      <td>0.967297</td>\n",
       "      <td>0.164388</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>7005.0</td>\n",
       "      <td>110.941312</td>\n",
       "      <td>37.167537</td>\n",
       "      <td>1.400</td>\n",
       "      <td>93.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>839.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>259.0</td>\n",
       "      <td>22.955019</td>\n",
       "      <td>6.088392</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        mean          std    min    25%     50%     75%       max\n",
       "age  7679.0   77.640839  1293.909497  1.000  37.00   55.00   67.00  65526.00\n",
       "TSH  6955.0    5.500684    25.978304  0.005   0.55    1.40    2.70    530.00\n",
       "T3   5470.0    2.010773     0.818738  0.050   1.60    1.90    2.30     18.00\n",
       "TT4  7325.0  105.497565    33.125317  2.000  87.00  103.00  121.00    430.00\n",
       "T4U  6998.0    0.967297     0.164388  0.190   0.87    0.96    1.06      2.12\n",
       "FTI  7005.0  110.941312    37.167537  1.400  93.00  108.00  125.00    839.00\n",
       "TBG   259.0   22.955019     6.088392  0.100  20.00   23.00   27.00     45.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc6868-0ae2-422a-8df8-c8e64c32be33",
   "metadata": {},
   "source": [
    "## Fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2922f-73df-4a42-a125-d2e63721715a",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc96b4-814a-41fc-b129-437192c2e8b0",
   "metadata": {},
   "source": [
    "Without deep analysis we can see there is a problem with age column since max is 65526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f863b88-f898-4225-afd3-c65a093af62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>455</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5710</th>\n",
       "      <td>65511</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>65512</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8105</th>\n",
       "      <td>65526</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    target\n",
       "7355     97  negative\n",
       "790      97  negative\n",
       "7356     97  negative\n",
       "2976    455  negative\n",
       "5710  65511  negative\n",
       "6392  65512  negative\n",
       "8105  65526  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"age\")[-7:][[\"age\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99ea855-c2c3-43a7-8a25-b647f452f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['age'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fb72ec-c6ca-4ae1-88c0-501baf11c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>7675.0</td>\n",
       "      <td>52.013029</td>\n",
       "      <td>18.654684</td>\n",
       "      <td>1.000</td>\n",
       "      <td>37.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>97.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>6951.0</td>\n",
       "      <td>5.503416</td>\n",
       "      <td>25.985525</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>530.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>5467.0</td>\n",
       "      <td>2.010633</td>\n",
       "      <td>0.818893</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>7321.0</td>\n",
       "      <td>105.490324</td>\n",
       "      <td>33.132392</td>\n",
       "      <td>2.000</td>\n",
       "      <td>87.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>6994.0</td>\n",
       "      <td>0.967268</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>7001.0</td>\n",
       "      <td>110.937565</td>\n",
       "      <td>37.176408</td>\n",
       "      <td>1.400</td>\n",
       "      <td>93.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>839.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>259.0</td>\n",
       "      <td>22.955019</td>\n",
       "      <td>6.088392</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        mean        std    min    25%     50%     75%     max\n",
       "age  7675.0   52.013029  18.654684  1.000  37.00   55.00   67.00   97.00\n",
       "TSH  6951.0    5.503416  25.985525  0.005   0.55    1.40    2.70  530.00\n",
       "T3   5467.0    2.010633   0.818893  0.050   1.60    1.90    2.30   18.00\n",
       "TT4  7321.0  105.490324  33.132392  2.000  87.00  103.00  121.00  430.00\n",
       "T4U  6994.0    0.967268   0.164410  0.190   0.87    0.96    1.06    2.12\n",
       "FTI  7001.0  110.937565  37.176408  1.400  93.00  108.00  125.00  839.00\n",
       "TBG   259.0   22.955019   6.088392  0.100  20.00   23.00   27.00   45.00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19658207-fff5-4a0b-a2f7-eccf41fb20df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>7421</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>5006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on_thyroxine</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>6825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on_antithyroid_meds</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sick</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnant</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I131_treatment</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_hyperthyroid</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lithium</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goitre</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tumor</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypopituitary</th>\n",
       "      <td>7675</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>7675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psych</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>7675</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>6767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count unique       top  freq\n",
       "sex                  7421      2         F  5006\n",
       "on_thyroxine         7675      2         f  6825\n",
       "query_on_thyroxine   7675      2         f  7552\n",
       "on_antithyroid_meds  7675      2         f  7583\n",
       "sick                 7675      2         f  7388\n",
       "pregnant             7675      2         f  7635\n",
       "thyroid_surgery      7675      2         f  7569\n",
       "I131_treatment       7675      2         f  7534\n",
       "query_hypothyroid    7675      2         f  7155\n",
       "query_hyperthyroid   7675      2         f  7115\n",
       "lithium              7675      2         f  7589\n",
       "goitre               7675      2         f  7601\n",
       "tumor                7675      2         f  7472\n",
       "hypopituitary        7675      1         f  7675\n",
       "psych                7675      2         f  7294\n",
       "target               7675      3  negative  6767"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a74c71-fcda-4aff-8dc9-9be304cf39c8",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d218ad32-06c9-41dc-a255-5437732395c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>3.309446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>9.433225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>28.768730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>4.612378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>8.872964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>8.781759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>96.625407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing Values %\n",
       "sex          3.309446\n",
       "TSH          9.433225\n",
       "T3          28.768730\n",
       "TT4          4.612378\n",
       "T4U          8.872964\n",
       "FTI          8.781759\n",
       "TBG         96.625407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847d42b-7e10-4038-81dc-1ff49bc04e53",
   "metadata": {},
   "source": [
    "Drop TBG since almost whole column is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef1622c-4a43-4409-89dd-0f76f739b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"TBG\"], axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6206922d-4bc7-4af4-960e-6771ab272313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>on_antithyroid_meds</th>\n",
       "      <th>sick</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <th>I131_treatment</th>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <th>...</th>\n",
       "      <th>goitre</th>\n",
       "      <th>tumor</th>\n",
       "      <th>hypopituitary</th>\n",
       "      <th>psych</th>\n",
       "      <th>TSH</th>\n",
       "      <th>T3</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9166</th>\n",
       "      <td>70</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>119.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9167</th>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>77.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9168</th>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>99.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9170</th>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>88.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9171</th>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>65.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7675 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age sex on_thyroxine query_on_thyroxine on_antithyroid_meds sick  \\\n",
       "0      29   F            f                  f                   f    f   \n",
       "1      29   F            f                  f                   f    f   \n",
       "2      41   F            f                  f                   f    f   \n",
       "3      36   F            f                  f                   f    f   \n",
       "5      60   F            f                  f                   f    f   \n",
       "...   ...  ..          ...                ...                 ...  ...   \n",
       "9166   70   F            f                  f                   f    f   \n",
       "9167   56   M            f                  f                   f    f   \n",
       "9168   22   M            f                  f                   f    f   \n",
       "9170   47   F            f                  f                   f    f   \n",
       "9171   31   M            f                  f                   f    f   \n",
       "\n",
       "     pregnant thyroid_surgery I131_treatment query_hypothyroid  ... goitre  \\\n",
       "0           f               f              f                 t  ...      f   \n",
       "1           f               f              f                 f  ...      f   \n",
       "2           f               f              f                 f  ...      f   \n",
       "3           f               f              f                 f  ...      f   \n",
       "5           f               f              f                 f  ...      f   \n",
       "...       ...             ...            ...               ...  ...    ...   \n",
       "9166        f               f              f                 f  ...      f   \n",
       "9167        f               f              f                 f  ...      f   \n",
       "9168        f               f              f                 f  ...      f   \n",
       "9170        f               f              f                 f  ...      f   \n",
       "9171        f               f              f                 t  ...      f   \n",
       "\n",
       "     tumor hypopituitary psych  TSH   T3    TT4   T4U    FTI    target  \n",
       "0        f             f     f  0.3  NaN    NaN   NaN    NaN  negative  \n",
       "1        f             f     f  1.6  1.9  128.0   NaN    NaN  negative  \n",
       "2        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "3        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "5        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "...    ...           ...   ...  ...  ...    ...   ...    ...       ...  \n",
       "9166     f             f     f  NaN  NaN   88.0  0.74  119.0  negative  \n",
       "9167     f             f     f  NaN  NaN   64.0  0.83   77.0  negative  \n",
       "9168     f             f     f  NaN  NaN   91.0  0.92   99.0  negative  \n",
       "9170     f             f     f  NaN  NaN   75.0  0.85   88.0  negative  \n",
       "9171     f             f     f  NaN  NaN   66.0  1.02   65.0  negative  \n",
       "\n",
       "[7675 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4cb293-178a-44da-a0d7-deb4454fcb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('f', 0, inplace=True)\n",
    "df.replace('t', 1, inplace=True)\n",
    "df.replace('M', 0, inplace=True)\n",
    "df.replace('F', 1, inplace=True)\n",
    "\n",
    "target_map = {\n",
    "    \"negative\": 0,\n",
    "    \"hyperthyroid\": 1,\n",
    "    \"hypothyroid\": 2,\n",
    "}\n",
    "\n",
    "df[\"target\"] = df[\"target\"].map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ab1b3c-c3bd-47fc-b926-5f5150054674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>3.309446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>9.433225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>28.768730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>4.612378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>8.872964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>8.781759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing Values %\n",
       "sex          3.309446\n",
       "TSH          9.433225\n",
       "T3          28.768730\n",
       "TT4          4.612378\n",
       "T4U          8.872964\n",
       "FTI          8.781759"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f162555-3d09-40a3-a55e-ec64ff80b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "\n",
    "seed(1337)\n",
    "\n",
    "def get_impute_value(df, col, target):\n",
    "    match col:\n",
    "        case \"age\" | \"TSH\" | \"T3\" | \"TT4\" | \"T4U\" | \"FTI\" | \"target\":\n",
    "            median_for_target = df.loc[df['target'] == target][col].median()\n",
    "            return median_for_target\n",
    "        case _:\n",
    "            prob = df.loc[df['target'] == target][col].value_counts(normalize=True)\n",
    "            return 1 if random() < prob[1] else 0\n",
    "\n",
    "def imputed_df(df):\n",
    "    df = df.copy()\n",
    "    null_cols = df.columns[df.isnull().any()]\n",
    "    for null_col in null_cols:\n",
    "        print(f\"Imputing column: {null_col}\")\n",
    "        df[null_col] = df.apply(\n",
    "            lambda row: get_impute_value(df, null_col, row[\"target\"]) if pd.isnull(row[null_col]) else row[null_col],\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e295ad91-a7ea-4534-a063-7990484a1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing column: sex\n",
      "Imputing column: TSH\n",
      "Imputing column: T3\n",
      "Imputing column: TT4\n",
      "Imputing column: T4U\n",
      "Imputing column: FTI\n"
     ]
    }
   ],
   "source": [
    "df = imputed_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1698dbab-43e0-4913-9918-f945b900a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Values %]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93f2a5-ab65-4b94-a54a-bbe92c0256da",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f47a658-a265-45d2-acbc-94388dd4da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1).copy()\n",
    "y = df['target'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be826f-4d8a-4bdb-bd68-50da18acbbb6",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "404616de-5fee-4f08-ad06-f20a77ae4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a0c6ef-42e4-483d-b4fa-3d5a440cc6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9869723814486712"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "430edea8-1b14-4a31-aff8-313c8a55de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.99349882, 0.83333333, 0.9760479 ])\n",
      "specificity=array([0.93832599, 0.99462076, 0.99942922])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7594a1d4f9b0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"negative\", \"hyperthyroid\", \"hypothyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "220fb68f-2eed-4c7b-b04e-05cb84cce3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.986966343478184)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test , y_test_pred,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9465832-9628-415e-a0a2-c86efba0c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"decisiontree.bin\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d03c4e-0124-494e-b108-4e02d91f81ab",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7b9a3f-3fff-42da-9f17-2582e13031da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier(5)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9e9df1f-33c0-4e3b-a16d-382a37af465b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948931735278791"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75e375ef-61ea-49b7-a336-769422f27550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.99349882, 0.75      , 0.56886228])\n",
      "specificity=array([0.61674009, 0.99623453, 0.99771689])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7594a31fd700>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"negative\", \"hyperthyroid\", \"hypothyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac9b1bd-65da-4d7a-9d23-cc90d08acbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9440210008223214)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test , y_test_pred,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd860e01-556f-4e48-bd3a-cc9c856cd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"knn.bin\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781d8ba-bf09-416a-9e5a-522b39bc470b",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f40de43-e8d0-4f5d-8632-26ca594d917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5eb77b8-69bb-4ace-a95e-d434d25ef311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 01:02:25.334946: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-29 01:02:25.345309: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732838545.357271  562292 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732838545.360815  562292 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 01:02:25.374408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/szymon/hackathon2025/repo/venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1732838547.957807  562292 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6258 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732838549.084847  562426 service.cc:148] XLA service 0x7d8d4c008cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732838549.084874  562426 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-11-29 01:02:29.101734: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732838549.185255  562426 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-29 01:02:29.770444: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 104 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.814126: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.840054: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.888334: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 300 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.929197: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 44 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.943212: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.994772: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188_0', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.176390: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 824 bytes spill stores, 544 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.233230: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_312', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.260615: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.356037: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_195', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.431749: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.447590: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 284 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.646611: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352_0', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.653714: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.931460: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 276 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.020537: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.230165: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 200 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.354748: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.518648: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 2060 bytes spill stores, 1928 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.585083: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350_0', 736 bytes spill stores, 664 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.767370: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.188325: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.271433: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.278288: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.289724: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.356172: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 1412 bytes spill stores, 1300 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.365949: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.398318: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.411269: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352_0', 664 bytes spill stores, 664 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.414151: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.527604: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 928 bytes spill stores, 932 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.609170: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 4480 bytes spill stores, 4476 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0799 - loss: 6.6570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732838554.033496  562426 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-29 01:02:34.760038: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:34.864461: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 272 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:34.910714: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:34.972888: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.032849: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.066584: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.137740: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49_0', 784 bytes spill stores, 736 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.271617: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56_0', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.389964: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 248 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.410293: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.597168: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.640705: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 280 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.665293: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 1304 bytes spill stores, 1444 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88021, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.0799 - loss: 6.6570 - val_accuracy: 0.8802 - val_loss: 2.0003\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8842 - loss: 2.1206\n",
      "Epoch 2: val_accuracy improved from 0.88021 to 0.89670, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8842 - loss: 2.1206 - val_accuracy: 0.8967 - val_loss: 1.5516\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8983 - loss: 1.6150\n",
      "Epoch 3: val_accuracy improved from 0.89670 to 0.90712, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8983 - loss: 1.6150 - val_accuracy: 0.9071 - val_loss: 0.8478\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9029 - loss: 0.8710\n",
      "Epoch 4: val_accuracy did not improve from 0.90712\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9029 - loss: 0.8710 - val_accuracy: 0.0599 - val_loss: 3.0437\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0606 - loss: 3.0357\n",
      "Epoch 5: val_accuracy improved from 0.90712 to 0.90885, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.0606 - loss: 3.0357 - val_accuracy: 0.9089 - val_loss: 0.8125\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9118 - loss: 0.8669\n",
      "Epoch 6: val_accuracy improved from 0.90885 to 0.90972, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9118 - loss: 0.8669 - val_accuracy: 0.9097 - val_loss: 1.4291\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9094 - loss: 1.5379\n",
      "Epoch 7: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9094 - loss: 1.5379 - val_accuracy: 0.9097 - val_loss: 1.7824\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9114 - loss: 1.9324\n",
      "Epoch 8: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9114 - loss: 1.9324 - val_accuracy: 0.9089 - val_loss: 1.9151\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9129 - loss: 2.0953\n",
      "Epoch 9: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9129 - loss: 2.0953 - val_accuracy: 0.9080 - val_loss: 1.9481\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9075 - loss: 2.1417\n",
      "Epoch 10: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9075 - loss: 2.1417 - val_accuracy: 0.8290 - val_loss: 2.1420\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8388 - loss: 2.3296\n",
      "Epoch 11: val_accuracy improved from 0.90972 to 0.91840, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8388 - loss: 2.3296 - val_accuracy: 0.9184 - val_loss: 1.9604\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9192 - loss: 2.1612\n",
      "Epoch 12: val_accuracy did not improve from 0.91840\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9192 - loss: 2.1612 - val_accuracy: 0.9123 - val_loss: 1.9924\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9138 - loss: 2.1881\n",
      "Epoch 13: val_accuracy did not improve from 0.91840\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9138 - loss: 2.1881 - val_accuracy: 0.9141 - val_loss: 1.9102\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9155 - loss: 2.0979\n",
      "Epoch 14: val_accuracy did not improve from 0.91840\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9155 - loss: 2.0979 - val_accuracy: 0.9184 - val_loss: 1.7443\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9201 - loss: 1.9211\n",
      "Epoch 15: val_accuracy improved from 0.91840 to 0.92622, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9201 - loss: 1.9211 - val_accuracy: 0.9262 - val_loss: 1.6204\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9266 - loss: 1.7792\n",
      "Epoch 16: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9266 - loss: 1.7792 - val_accuracy: 0.9253 - val_loss: 1.4952\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9294 - loss: 1.6350\n",
      "Epoch 17: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9294 - loss: 1.6350 - val_accuracy: 0.9184 - val_loss: 1.2585\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9216 - loss: 1.3796\n",
      "Epoch 18: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9216 - loss: 1.3796 - val_accuracy: 0.9167 - val_loss: 1.0969\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9185 - loss: 1.1934\n",
      "Epoch 19: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9185 - loss: 1.1934 - val_accuracy: 0.9184 - val_loss: 0.8727\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9192 - loss: 0.9372\n",
      "Epoch 20: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9192 - loss: 0.9372 - val_accuracy: 0.9201 - val_loss: 0.5785\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9231 - loss: 0.6066\n",
      "Epoch 21: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9231 - loss: 0.6066 - val_accuracy: 0.9106 - val_loss: 0.3867\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9185 - loss: 0.3765\n",
      "Epoch 22: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9185 - loss: 0.3765 - val_accuracy: 0.7240 - val_loss: 0.9220\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7246 - loss: 0.9137\n",
      "Epoch 23: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7246 - loss: 0.9137 - val_accuracy: 0.7179 - val_loss: 0.8697\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7168 - loss: 0.8594\n",
      "Epoch 24: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7168 - loss: 0.8594 - val_accuracy: 0.8724 - val_loss: 0.4560\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8725 - loss: 0.4389\n",
      "Epoch 25: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8725 - loss: 0.4389 - val_accuracy: 0.9097 - val_loss: 0.4757\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9177 - loss: 0.4760\n",
      "Epoch 26: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9177 - loss: 0.4760 - val_accuracy: 0.9201 - val_loss: 0.5548\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9207 - loss: 0.5725\n",
      "Epoch 27: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9207 - loss: 0.5725 - val_accuracy: 0.9227 - val_loss: 0.5810\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9242 - loss: 0.6104\n",
      "Epoch 28: val_accuracy improved from 0.92622 to 0.92795, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9242 - loss: 0.6104 - val_accuracy: 0.9280 - val_loss: 0.5596\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9314 - loss: 0.5957\n",
      "Epoch 29: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9314 - loss: 0.5957 - val_accuracy: 0.9201 - val_loss: 0.6040\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9181 - loss: 0.6358\n",
      "Epoch 30: val_accuracy improved from 0.92795 to 0.93490, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9181 - loss: 0.6358 - val_accuracy: 0.9349 - val_loss: 0.5363\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9351 - loss: 0.5654\n",
      "Epoch 31: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9351 - loss: 0.5654 - val_accuracy: 0.9332 - val_loss: 0.4406\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9331 - loss: 0.4628\n",
      "Epoch 32: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9331 - loss: 0.4628 - val_accuracy: 0.9245 - val_loss: 0.3827\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9268 - loss: 0.3929\n",
      "Epoch 33: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9268 - loss: 0.3929 - val_accuracy: 0.9106 - val_loss: 0.3242\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9207 - loss: 0.3225\n",
      "Epoch 34: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9207 - loss: 0.3225 - val_accuracy: 0.8602 - val_loss: 0.3724\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8664 - loss: 0.3623\n",
      "Epoch 35: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8664 - loss: 0.3623 - val_accuracy: 0.7995 - val_loss: 0.4850\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8019 - loss: 0.4755\n",
      "Epoch 36: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8019 - loss: 0.4755 - val_accuracy: 0.8602 - val_loss: 0.3706\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8608 - loss: 0.3630\n",
      "Epoch 37: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8608 - loss: 0.3630 - val_accuracy: 0.9089 - val_loss: 0.2929\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9181 - loss: 0.2930\n",
      "Epoch 38: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9181 - loss: 0.2930 - val_accuracy: 0.9219 - val_loss: 0.3101\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9285 - loss: 0.3191\n",
      "Epoch 39: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9285 - loss: 0.3191 - val_accuracy: 0.9314 - val_loss: 0.3308\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9348 - loss: 0.3454\n",
      "Epoch 40: val_accuracy improved from 0.93490 to 0.94878, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9348 - loss: 0.3454 - val_accuracy: 0.9488 - val_loss: 0.3498\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9429 - loss: 0.3659\n",
      "Epoch 41: val_accuracy improved from 0.94878 to 0.95052, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9429 - loss: 0.3659 - val_accuracy: 0.9505 - val_loss: 0.3430\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9461 - loss: 0.3578\n",
      "Epoch 42: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9461 - loss: 0.3578 - val_accuracy: 0.9401 - val_loss: 0.2999\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9398 - loss: 0.3123\n",
      "Epoch 43: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9398 - loss: 0.3123 - val_accuracy: 0.9245 - val_loss: 0.2687\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9329 - loss: 0.2772\n",
      "Epoch 44: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9329 - loss: 0.2772 - val_accuracy: 0.9089 - val_loss: 0.2685\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9168 - loss: 0.2719\n",
      "Epoch 45: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9168 - loss: 0.2719 - val_accuracy: 0.8828 - val_loss: 0.3117\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8810 - loss: 0.3118\n",
      "Epoch 46: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8810 - loss: 0.3118 - val_accuracy: 0.8854 - val_loss: 0.3069\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8842 - loss: 0.3078\n",
      "Epoch 47: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8842 - loss: 0.3078 - val_accuracy: 0.9115 - val_loss: 0.2607\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9194 - loss: 0.2658\n",
      "Epoch 48: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9194 - loss: 0.2658 - val_accuracy: 0.9280 - val_loss: 0.2514\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9348 - loss: 0.2615\n",
      "Epoch 49: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9348 - loss: 0.2615 - val_accuracy: 0.9401 - val_loss: 0.2600\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9416 - loss: 0.2716\n",
      "Epoch 50: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9416 - loss: 0.2716 - val_accuracy: 0.9505 - val_loss: 0.2738\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9457 - loss: 0.2843\n",
      "Epoch 51: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9457 - loss: 0.2843 - val_accuracy: 0.9479 - val_loss: 0.2670\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9446 - loss: 0.2757\n",
      "Epoch 52: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9446 - loss: 0.2757 - val_accuracy: 0.9418 - val_loss: 0.2439\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9416 - loss: 0.2508\n",
      "Epoch 53: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9416 - loss: 0.2508 - val_accuracy: 0.9332 - val_loss: 0.2400\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9370 - loss: 0.2446\n",
      "Epoch 54: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9370 - loss: 0.2446 - val_accuracy: 0.9201 - val_loss: 0.2550\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9231 - loss: 0.2577\n",
      "Epoch 55: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9231 - loss: 0.2577 - val_accuracy: 0.9158 - val_loss: 0.2607\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9201 - loss: 0.2631\n",
      "Epoch 56: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9201 - loss: 0.2631 - val_accuracy: 0.9236 - val_loss: 0.2446\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9325 - loss: 0.2481\n",
      "Epoch 57: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9325 - loss: 0.2481 - val_accuracy: 0.9358 - val_loss: 0.2332\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9390 - loss: 0.2381\n",
      "Epoch 58: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9390 - loss: 0.2381 - val_accuracy: 0.9462 - val_loss: 0.2353\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9446 - loss: 0.2407\n",
      "Epoch 59: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9446 - loss: 0.2407 - val_accuracy: 0.9488 - val_loss: 0.2443\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9479 - loss: 0.2492\n",
      "Epoch 60: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9479 - loss: 0.2492 - val_accuracy: 0.9479 - val_loss: 0.2380\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2427\n",
      "Epoch 61: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9470 - loss: 0.2427 - val_accuracy: 0.9427 - val_loss: 0.2286\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9424 - loss: 0.2328\n",
      "Epoch 62: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9424 - loss: 0.2328 - val_accuracy: 0.9392 - val_loss: 0.2286\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9396 - loss: 0.2323\n",
      "Epoch 63: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9396 - loss: 0.2323 - val_accuracy: 0.9349 - val_loss: 0.2348\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9372 - loss: 0.2377\n",
      "Epoch 64: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9372 - loss: 0.2377 - val_accuracy: 0.9366 - val_loss: 0.2333\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9387 - loss: 0.2362\n",
      "Epoch 65: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9387 - loss: 0.2362 - val_accuracy: 0.9410 - val_loss: 0.2244\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9411 - loss: 0.2283\n",
      "Epoch 66: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9411 - loss: 0.2283 - val_accuracy: 0.9453 - val_loss: 0.2220\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9446 - loss: 0.2266\n",
      "Epoch 67: val_accuracy improved from 0.95052 to 0.95139, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9446 - loss: 0.2266 - val_accuracy: 0.9514 - val_loss: 0.2255\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9481 - loss: 0.2302\n",
      "Epoch 68: val_accuracy improved from 0.95139 to 0.95226, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9481 - loss: 0.2302 - val_accuracy: 0.9523 - val_loss: 0.2251\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9492 - loss: 0.2296\n",
      "Epoch 69: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9492 - loss: 0.2296 - val_accuracy: 0.9488 - val_loss: 0.2195\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9468 - loss: 0.2234\n",
      "Epoch 70: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9468 - loss: 0.2234 - val_accuracy: 0.9444 - val_loss: 0.2184\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9427 - loss: 0.2217\n",
      "Epoch 71: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9427 - loss: 0.2217 - val_accuracy: 0.9444 - val_loss: 0.2212\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9424 - loss: 0.2240\n",
      "Epoch 72: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9424 - loss: 0.2240 - val_accuracy: 0.9444 - val_loss: 0.2201\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9422 - loss: 0.2232\n",
      "Epoch 73: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9422 - loss: 0.2232 - val_accuracy: 0.9436 - val_loss: 0.2150\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9429 - loss: 0.2187\n",
      "Epoch 74: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9429 - loss: 0.2187 - val_accuracy: 0.9479 - val_loss: 0.2135\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2176\n",
      "Epoch 75: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9470 - loss: 0.2176 - val_accuracy: 0.9514 - val_loss: 0.2148\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9496 - loss: 0.2186\n",
      "Epoch 76: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9496 - loss: 0.2186 - val_accuracy: 0.9523 - val_loss: 0.2139\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9505 - loss: 0.2173\n",
      "Epoch 77: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9505 - loss: 0.2173 - val_accuracy: 0.9479 - val_loss: 0.2107\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2138\n",
      "Epoch 78: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9470 - loss: 0.2138 - val_accuracy: 0.9453 - val_loss: 0.2104\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9431 - loss: 0.2134\n",
      "Epoch 79: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9431 - loss: 0.2134 - val_accuracy: 0.9436 - val_loss: 0.2112\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9437 - loss: 0.2136\n",
      "Epoch 80: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9437 - loss: 0.2136 - val_accuracy: 0.9444 - val_loss: 0.2097\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9433 - loss: 0.2114\n",
      "Epoch 81: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9433 - loss: 0.2114 - val_accuracy: 0.9479 - val_loss: 0.2076\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9468 - loss: 0.2091\n",
      "Epoch 82: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9468 - loss: 0.2091 - val_accuracy: 0.9523 - val_loss: 0.2079\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9507 - loss: 0.2091\n",
      "Epoch 83: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9507 - loss: 0.2091 - val_accuracy: 0.9523 - val_loss: 0.2073\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9505 - loss: 0.2084\n",
      "Epoch 84: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9505 - loss: 0.2084 - val_accuracy: 0.9505 - val_loss: 0.2052\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9487 - loss: 0.2062\n",
      "Epoch 85: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9487 - loss: 0.2062 - val_accuracy: 0.9470 - val_loss: 0.2044\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9464 - loss: 0.2049\n",
      "Epoch 86: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9464 - loss: 0.2049 - val_accuracy: 0.9462 - val_loss: 0.2047\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9459 - loss: 0.2047\n",
      "Epoch 87: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9459 - loss: 0.2047 - val_accuracy: 0.9470 - val_loss: 0.2032\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2031\n",
      "Epoch 88: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9470 - loss: 0.2031 - val_accuracy: 0.9488 - val_loss: 0.2012\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9479 - loss: 0.2014\n",
      "Epoch 89: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9479 - loss: 0.2014 - val_accuracy: 0.9523 - val_loss: 0.2006\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9503 - loss: 0.2006\n",
      "Epoch 90: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9503 - loss: 0.2006 - val_accuracy: 0.9523 - val_loss: 0.2001\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9507 - loss: 0.1997\n",
      "Epoch 91: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9507 - loss: 0.1997 - val_accuracy: 0.9497 - val_loss: 0.1990\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9496 - loss: 0.1979\n",
      "Epoch 92: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9496 - loss: 0.1979 - val_accuracy: 0.9488 - val_loss: 0.1980\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9479 - loss: 0.1969\n",
      "Epoch 93: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9479 - loss: 0.1969 - val_accuracy: 0.9497 - val_loss: 0.1965\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.1958\n",
      "Epoch 94: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9470 - loss: 0.1958 - val_accuracy: 0.9505 - val_loss: 0.1948\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9487 - loss: 0.1943\n",
      "Epoch 95: val_accuracy improved from 0.95226 to 0.95312, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9487 - loss: 0.1943 - val_accuracy: 0.9531 - val_loss: 0.1939\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9496 - loss: 0.1932\n",
      "Epoch 96: val_accuracy improved from 0.95312 to 0.95486, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9496 - loss: 0.1932 - val_accuracy: 0.9549 - val_loss: 0.1931\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9509 - loss: 0.1920\n",
      "Epoch 97: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9509 - loss: 0.1920 - val_accuracy: 0.9540 - val_loss: 0.1921\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9509 - loss: 0.1905\n",
      "Epoch 98: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9509 - loss: 0.1905 - val_accuracy: 0.9505 - val_loss: 0.1915\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9503 - loss: 0.1891\n",
      "Epoch 99: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9503 - loss: 0.1891 - val_accuracy: 0.9514 - val_loss: 0.1912\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9500 - loss: 0.1879\n",
      "Epoch 100: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9500 - loss: 0.1879 - val_accuracy: 0.9523 - val_loss: 0.1897\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9507 - loss: 0.1862\n",
      "Epoch 101: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9507 - loss: 0.1862 - val_accuracy: 0.9531 - val_loss: 0.1884\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9507 - loss: 0.1849\n",
      "Epoch 102: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9507 - loss: 0.1849 - val_accuracy: 0.9540 - val_loss: 0.1876\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9509 - loss: 0.1835\n",
      "Epoch 103: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9509 - loss: 0.1835 - val_accuracy: 0.9540 - val_loss: 0.1867\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9513 - loss: 0.1820\n",
      "Epoch 104: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9513 - loss: 0.1820 - val_accuracy: 0.9514 - val_loss: 0.1857\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9513 - loss: 0.1806\n",
      "Epoch 105: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9513 - loss: 0.1806 - val_accuracy: 0.9523 - val_loss: 0.1845\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9511 - loss: 0.1792\n",
      "Epoch 106: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9511 - loss: 0.1792 - val_accuracy: 0.9540 - val_loss: 0.1834\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9520 - loss: 0.1776\n",
      "Epoch 107: val_accuracy improved from 0.95486 to 0.95573, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9520 - loss: 0.1776 - val_accuracy: 0.9557 - val_loss: 0.1823\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9524 - loss: 0.1763\n",
      "Epoch 108: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9524 - loss: 0.1763 - val_accuracy: 0.9557 - val_loss: 0.1813\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9520 - loss: 0.1748\n",
      "Epoch 109: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9520 - loss: 0.1748 - val_accuracy: 0.9540 - val_loss: 0.1802\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9516 - loss: 0.1733\n",
      "Epoch 110: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9516 - loss: 0.1733 - val_accuracy: 0.9540 - val_loss: 0.1795\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9516 - loss: 0.1718\n",
      "Epoch 111: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9516 - loss: 0.1718 - val_accuracy: 0.9549 - val_loss: 0.1788\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9524 - loss: 0.1704\n",
      "Epoch 112: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9524 - loss: 0.1704 - val_accuracy: 0.9557 - val_loss: 0.1785\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9535 - loss: 0.1689\n",
      "Epoch 113: val_accuracy improved from 0.95573 to 0.95747, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9535 - loss: 0.1689 - val_accuracy: 0.9575 - val_loss: 0.1774\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9537 - loss: 0.1675\n",
      "Epoch 114: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9537 - loss: 0.1675 - val_accuracy: 0.9540 - val_loss: 0.1767\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9529 - loss: 0.1659\n",
      "Epoch 115: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9529 - loss: 0.1659 - val_accuracy: 0.9531 - val_loss: 0.1755\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9529 - loss: 0.1645\n",
      "Epoch 116: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9529 - loss: 0.1645 - val_accuracy: 0.9557 - val_loss: 0.1743\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9540 - loss: 0.1630\n",
      "Epoch 117: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9540 - loss: 0.1630 - val_accuracy: 0.9557 - val_loss: 0.1734\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9553 - loss: 0.1615\n",
      "Epoch 118: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9553 - loss: 0.1615 - val_accuracy: 0.9557 - val_loss: 0.1726\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9553 - loss: 0.1600\n",
      "Epoch 119: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9553 - loss: 0.1600 - val_accuracy: 0.9566 - val_loss: 0.1717\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9559 - loss: 0.1584\n",
      "Epoch 120: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9559 - loss: 0.1584 - val_accuracy: 0.9557 - val_loss: 0.1711\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9559 - loss: 0.1571\n",
      "Epoch 121: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9559 - loss: 0.1571 - val_accuracy: 0.9575 - val_loss: 0.1697\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9561 - loss: 0.1557\n",
      "Epoch 122: val_accuracy improved from 0.95747 to 0.95833, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9561 - loss: 0.1557 - val_accuracy: 0.9583 - val_loss: 0.1695\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9563 - loss: 0.1546\n",
      "Epoch 123: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9563 - loss: 0.1546 - val_accuracy: 0.9557 - val_loss: 0.1677\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9553 - loss: 0.1539\n",
      "Epoch 124: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9553 - loss: 0.1539 - val_accuracy: 0.9575 - val_loss: 0.1697\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9589 - loss: 0.1533\n",
      "Epoch 125: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9589 - loss: 0.1533 - val_accuracy: 0.9557 - val_loss: 0.1657\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9555 - loss: 0.1509\n",
      "Epoch 126: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9555 - loss: 0.1509 - val_accuracy: 0.9575 - val_loss: 0.1649\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9563 - loss: 0.1491\n",
      "Epoch 127: val_accuracy improved from 0.95833 to 0.95920, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9563 - loss: 0.1491 - val_accuracy: 0.9592 - val_loss: 0.1672\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9600 - loss: 0.1490\n",
      "Epoch 128: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9600 - loss: 0.1490 - val_accuracy: 0.9575 - val_loss: 0.1637\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9559 - loss: 0.1476\n",
      "Epoch 129: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9559 - loss: 0.1476 - val_accuracy: 0.9592 - val_loss: 0.1621\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9589 - loss: 0.1452\n",
      "Epoch 130: val_accuracy improved from 0.95920 to 0.96267, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9589 - loss: 0.1452 - val_accuracy: 0.9627 - val_loss: 0.1632\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9605 - loss: 0.1449\n",
      "Epoch 131: val_accuracy did not improve from 0.96267\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9605 - loss: 0.1449 - val_accuracy: 0.9575 - val_loss: 0.1597\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9568 - loss: 0.1435\n",
      "Epoch 132: val_accuracy did not improve from 0.96267\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9568 - loss: 0.1435 - val_accuracy: 0.9583 - val_loss: 0.1582\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9585 - loss: 0.1414\n",
      "Epoch 133: val_accuracy improved from 0.96267 to 0.96354, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9585 - loss: 0.1414 - val_accuracy: 0.9635 - val_loss: 0.1595\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9609 - loss: 0.1412\n",
      "Epoch 134: val_accuracy did not improve from 0.96354\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9609 - loss: 0.1412 - val_accuracy: 0.9592 - val_loss: 0.1565\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9583 - loss: 0.1395\n",
      "Epoch 135: val_accuracy did not improve from 0.96354\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9583 - loss: 0.1395 - val_accuracy: 0.9601 - val_loss: 0.1560\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9596 - loss: 0.1378\n",
      "Epoch 136: val_accuracy improved from 0.96354 to 0.96528, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9596 - loss: 0.1378 - val_accuracy: 0.9653 - val_loss: 0.1573\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9633 - loss: 0.1375\n",
      "Epoch 137: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9633 - loss: 0.1375 - val_accuracy: 0.9592 - val_loss: 0.1542\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9585 - loss: 0.1360\n",
      "Epoch 138: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9585 - loss: 0.1360 - val_accuracy: 0.9618 - val_loss: 0.1535\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9598 - loss: 0.1343\n",
      "Epoch 139: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9598 - loss: 0.1343 - val_accuracy: 0.9627 - val_loss: 0.1550\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9629 - loss: 0.1342\n",
      "Epoch 140: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9629 - loss: 0.1342 - val_accuracy: 0.9609 - val_loss: 0.1518\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9598 - loss: 0.1325\n",
      "Epoch 141: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9598 - loss: 0.1325 - val_accuracy: 0.9644 - val_loss: 0.1511\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9613 - loss: 0.1308\n",
      "Epoch 142: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9613 - loss: 0.1308 - val_accuracy: 0.9653 - val_loss: 0.1522\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9639 - loss: 0.1305\n",
      "Epoch 143: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9639 - loss: 0.1305 - val_accuracy: 0.9601 - val_loss: 0.1488\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9609 - loss: 0.1294\n",
      "Epoch 144: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9609 - loss: 0.1294 - val_accuracy: 0.9627 - val_loss: 0.1484\n",
      "Epoch 145/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9629 - loss: 0.1274\n",
      "Epoch 145: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9629 - loss: 0.1274 - val_accuracy: 0.9644 - val_loss: 0.1491\n",
      "Epoch 146/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9648 - loss: 0.1269\n",
      "Epoch 146: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9648 - loss: 0.1269 - val_accuracy: 0.9618 - val_loss: 0.1469\n",
      "Epoch 147/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9622 - loss: 0.1259\n",
      "Epoch 147: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9622 - loss: 0.1259 - val_accuracy: 0.9635 - val_loss: 0.1470\n",
      "Epoch 148/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9646 - loss: 0.1242\n",
      "Epoch 148: val_accuracy improved from 0.96528 to 0.96615, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9646 - loss: 0.1242 - val_accuracy: 0.9661 - val_loss: 0.1476\n",
      "Epoch 149/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9657 - loss: 0.1235\n",
      "Epoch 149: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9657 - loss: 0.1235 - val_accuracy: 0.9627 - val_loss: 0.1456\n",
      "Epoch 150/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9642 - loss: 0.1229\n",
      "Epoch 150: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9642 - loss: 0.1229 - val_accuracy: 0.9644 - val_loss: 0.1458\n",
      "Epoch 151/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9666 - loss: 0.1213\n",
      "Epoch 151: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9666 - loss: 0.1213 - val_accuracy: 0.9644 - val_loss: 0.1450\n",
      "Epoch 152/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9668 - loss: 0.1203\n",
      "Epoch 152: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9668 - loss: 0.1203 - val_accuracy: 0.9627 - val_loss: 0.1437\n",
      "Epoch 153/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9646 - loss: 0.1197\n",
      "Epoch 153: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9646 - loss: 0.1197 - val_accuracy: 0.9644 - val_loss: 0.1445\n",
      "Epoch 154/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9666 - loss: 0.1186\n",
      "Epoch 154: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9666 - loss: 0.1186 - val_accuracy: 0.9644 - val_loss: 0.1429\n",
      "Epoch 155/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9672 - loss: 0.1173\n",
      "Epoch 155: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9672 - loss: 0.1173 - val_accuracy: 0.9644 - val_loss: 0.1426\n",
      "Epoch 156/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9668 - loss: 0.1164\n",
      "Epoch 156: val_accuracy improved from 0.96615 to 0.96701, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9668 - loss: 0.1164 - val_accuracy: 0.9670 - val_loss: 0.1433\n",
      "Epoch 157/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9672 - loss: 0.1157\n",
      "Epoch 157: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9672 - loss: 0.1157 - val_accuracy: 0.9635 - val_loss: 0.1415\n",
      "Epoch 158/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9674 - loss: 0.1147\n",
      "Epoch 158: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9674 - loss: 0.1147 - val_accuracy: 0.9635 - val_loss: 0.1418\n",
      "Epoch 159/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9674 - loss: 0.1137\n",
      "Epoch 159: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9674 - loss: 0.1137 - val_accuracy: 0.9635 - val_loss: 0.1411\n",
      "Epoch 160/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9683 - loss: 0.1127\n",
      "Epoch 160: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9683 - loss: 0.1127 - val_accuracy: 0.9627 - val_loss: 0.1405\n",
      "Epoch 161/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9683 - loss: 0.1119\n",
      "Epoch 161: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9683 - loss: 0.1119 - val_accuracy: 0.9661 - val_loss: 0.1410\n",
      "Epoch 162/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9685 - loss: 0.1110\n",
      "Epoch 162: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9685 - loss: 0.1110 - val_accuracy: 0.9627 - val_loss: 0.1402\n",
      "Epoch 163/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9681 - loss: 0.1101\n",
      "Epoch 163: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9681 - loss: 0.1101 - val_accuracy: 0.9627 - val_loss: 0.1401\n",
      "Epoch 164/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9687 - loss: 0.1092\n",
      "Epoch 164: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9687 - loss: 0.1092 - val_accuracy: 0.9635 - val_loss: 0.1395\n",
      "Epoch 165/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9694 - loss: 0.1083\n",
      "Epoch 165: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9694 - loss: 0.1083 - val_accuracy: 0.9644 - val_loss: 0.1388\n",
      "Epoch 166/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9689 - loss: 0.1075\n",
      "Epoch 166: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9689 - loss: 0.1075 - val_accuracy: 0.9653 - val_loss: 0.1391\n",
      "Epoch 167/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9700 - loss: 0.1067\n",
      "Epoch 167: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9700 - loss: 0.1067 - val_accuracy: 0.9653 - val_loss: 0.1380\n",
      "Epoch 168/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9692 - loss: 0.1059\n",
      "Epoch 168: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9692 - loss: 0.1059 - val_accuracy: 0.9661 - val_loss: 0.1386\n",
      "Epoch 169/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9707 - loss: 0.1051\n",
      "Epoch 169: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9707 - loss: 0.1051 - val_accuracy: 0.9644 - val_loss: 0.1372\n",
      "Epoch 170/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9698 - loss: 0.1043\n",
      "Epoch 170: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9698 - loss: 0.1043 - val_accuracy: 0.9670 - val_loss: 0.1372\n",
      "Epoch 171/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9705 - loss: 0.1035\n",
      "Epoch 171: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9705 - loss: 0.1035 - val_accuracy: 0.9661 - val_loss: 0.1365\n",
      "Epoch 172/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9705 - loss: 0.1027\n",
      "Epoch 172: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9705 - loss: 0.1027 - val_accuracy: 0.9670 - val_loss: 0.1362\n",
      "Epoch 173/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9713 - loss: 0.1019\n",
      "Epoch 173: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9713 - loss: 0.1019 - val_accuracy: 0.9670 - val_loss: 0.1357\n",
      "Epoch 174/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9713 - loss: 0.1012\n",
      "Epoch 174: val_accuracy improved from 0.96701 to 0.96788, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9713 - loss: 0.1012 - val_accuracy: 0.9679 - val_loss: 0.1351\n",
      "Epoch 175/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9711 - loss: 0.1005\n",
      "Epoch 175: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9711 - loss: 0.1005 - val_accuracy: 0.9679 - val_loss: 0.1350\n",
      "Epoch 176/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9711 - loss: 0.0998\n",
      "Epoch 176: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9711 - loss: 0.0998 - val_accuracy: 0.9670 - val_loss: 0.1347\n",
      "Epoch 177/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9715 - loss: 0.0991\n",
      "Epoch 177: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9715 - loss: 0.0991 - val_accuracy: 0.9679 - val_loss: 0.1353\n",
      "Epoch 178/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9715 - loss: 0.0986\n",
      "Epoch 178: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9715 - loss: 0.0986 - val_accuracy: 0.9644 - val_loss: 0.1340\n",
      "Epoch 179/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9698 - loss: 0.0992\n",
      "Epoch 179: val_accuracy improved from 0.96788 to 0.96962, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9698 - loss: 0.0992 - val_accuracy: 0.9696 - val_loss: 0.1420\n",
      "Epoch 180/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9744 - loss: 0.1027\n",
      "Epoch 180: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9744 - loss: 0.1027 - val_accuracy: 0.9635 - val_loss: 0.1408\n",
      "Epoch 181/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9607 - loss: 0.1093\n",
      "Epoch 181: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9607 - loss: 0.1093 - val_accuracy: 0.9688 - val_loss: 0.1385\n",
      "Epoch 182/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9748 - loss: 0.0998\n",
      "Epoch 182: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9748 - loss: 0.0998 - val_accuracy: 0.9688 - val_loss: 0.1339\n",
      "Epoch 183/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9733 - loss: 0.0964\n",
      "Epoch 183: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9733 - loss: 0.0964 - val_accuracy: 0.9644 - val_loss: 0.1342\n",
      "Epoch 184/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9659 - loss: 0.1010\n",
      "Epoch 184: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9659 - loss: 0.1010 - val_accuracy: 0.9688 - val_loss: 0.1336\n",
      "Epoch 185/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9744 - loss: 0.0951\n",
      "Epoch 185: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9744 - loss: 0.0951 - val_accuracy: 0.9696 - val_loss: 0.1353\n",
      "Epoch 186/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9757 - loss: 0.0949\n",
      "Epoch 186: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9757 - loss: 0.0949 - val_accuracy: 0.9653 - val_loss: 0.1351\n",
      "Epoch 187/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9679 - loss: 0.0983\n",
      "Epoch 187: val_accuracy improved from 0.96962 to 0.97049, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9679 - loss: 0.0983 - val_accuracy: 0.9705 - val_loss: 0.1331\n",
      "Epoch 188/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9752 - loss: 0.0930\n",
      "Epoch 188: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9752 - loss: 0.0930 - val_accuracy: 0.9705 - val_loss: 0.1336\n",
      "Epoch 189/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9759 - loss: 0.0932\n",
      "Epoch 189: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9759 - loss: 0.0932 - val_accuracy: 0.9653 - val_loss: 0.1318\n",
      "Epoch 190/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9705 - loss: 0.0952\n",
      "Epoch 190: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9705 - loss: 0.0952 - val_accuracy: 0.9661 - val_loss: 0.1294\n",
      "Epoch 191/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9733 - loss: 0.0909\n",
      "Epoch 191: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9733 - loss: 0.0909 - val_accuracy: 0.9705 - val_loss: 0.1326\n",
      "Epoch 192/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9750 - loss: 0.0932\n",
      "Epoch 192: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9750 - loss: 0.0932 - val_accuracy: 0.9661 - val_loss: 0.1304\n",
      "Epoch 193/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9705 - loss: 0.0933\n",
      "Epoch 193: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9705 - loss: 0.0933 - val_accuracy: 0.9670 - val_loss: 0.1293\n",
      "Epoch 194/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9726 - loss: 0.0889\n",
      "Epoch 194: val_accuracy improved from 0.97049 to 0.97396, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9726 - loss: 0.0889 - val_accuracy: 0.9740 - val_loss: 0.1348\n",
      "Epoch 195/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9765 - loss: 0.0924\n",
      "Epoch 195: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9765 - loss: 0.0924 - val_accuracy: 0.9670 - val_loss: 0.1301\n",
      "Epoch 196/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9700 - loss: 0.0917\n",
      "Epoch 196: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9700 - loss: 0.0917 - val_accuracy: 0.9688 - val_loss: 0.1279\n",
      "Epoch 197/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9739 - loss: 0.0874\n",
      "Epoch 197: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9739 - loss: 0.0874 - val_accuracy: 0.9731 - val_loss: 0.1335\n",
      "Epoch 198/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9770 - loss: 0.0913\n",
      "Epoch 198: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9770 - loss: 0.0913 - val_accuracy: 0.9670 - val_loss: 0.1272\n",
      "Epoch 199/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9705 - loss: 0.0898\n",
      "Epoch 199: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9705 - loss: 0.0898 - val_accuracy: 0.9661 - val_loss: 0.1252\n",
      "Epoch 200/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9728 - loss: 0.0861\n",
      "Epoch 200: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9728 - loss: 0.0861 - val_accuracy: 0.9714 - val_loss: 0.1320\n",
      "Epoch 201/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9770 - loss: 0.0904\n",
      "Epoch 201: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9770 - loss: 0.0904 - val_accuracy: 0.9679 - val_loss: 0.1248\n",
      "Epoch 202/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9724 - loss: 0.0869\n",
      "Epoch 202: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9724 - loss: 0.0869 - val_accuracy: 0.9679 - val_loss: 0.1239\n",
      "Epoch 203/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9733 - loss: 0.0856\n",
      "Epoch 203: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9733 - loss: 0.0856 - val_accuracy: 0.9722 - val_loss: 0.1305\n",
      "Epoch 204/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9781 - loss: 0.0884\n",
      "Epoch 204: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9781 - loss: 0.0884 - val_accuracy: 0.9679 - val_loss: 0.1241\n",
      "Epoch 205/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9744 - loss: 0.0843\n",
      "Epoch 205: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9744 - loss: 0.0843 - val_accuracy: 0.9679 - val_loss: 0.1243\n",
      "Epoch 206/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9737 - loss: 0.0845\n",
      "Epoch 206: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9737 - loss: 0.0845 - val_accuracy: 0.9722 - val_loss: 0.1283\n",
      "Epoch 207/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9783 - loss: 0.0852\n",
      "Epoch 207: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9783 - loss: 0.0852 - val_accuracy: 0.9688 - val_loss: 0.1239\n",
      "Epoch 208/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9757 - loss: 0.0822\n",
      "Epoch 208: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9757 - loss: 0.0822 - val_accuracy: 0.9679 - val_loss: 0.1243\n",
      "Epoch 209/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9728 - loss: 0.0835\n",
      "Epoch 209: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9728 - loss: 0.0835 - val_accuracy: 0.9722 - val_loss: 0.1257\n",
      "Epoch 210/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9776 - loss: 0.0824\n",
      "Epoch 210: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9776 - loss: 0.0824 - val_accuracy: 0.9705 - val_loss: 0.1239\n",
      "Epoch 211/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9765 - loss: 0.0811\n",
      "Epoch 211: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9765 - loss: 0.0811 - val_accuracy: 0.9679 - val_loss: 0.1232\n",
      "Epoch 212/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9742 - loss: 0.0820\n",
      "Epoch 212: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9742 - loss: 0.0820 - val_accuracy: 0.9731 - val_loss: 0.1237\n",
      "Epoch 213/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9772 - loss: 0.0804\n",
      "Epoch 213: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9772 - loss: 0.0804 - val_accuracy: 0.9740 - val_loss: 0.1237\n",
      "Epoch 214/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9785 - loss: 0.0804\n",
      "Epoch 214: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9785 - loss: 0.0804 - val_accuracy: 0.9688 - val_loss: 0.1218\n",
      "Epoch 215/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9742 - loss: 0.0805\n",
      "Epoch 215: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9742 - loss: 0.0805 - val_accuracy: 0.9679 - val_loss: 0.1221\n",
      "Epoch 216/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9765 - loss: 0.0789\n",
      "Epoch 216: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9765 - loss: 0.0789 - val_accuracy: 0.9740 - val_loss: 0.1238\n",
      "Epoch 217/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9794 - loss: 0.0795\n",
      "Epoch 217: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9794 - loss: 0.0795 - val_accuracy: 0.9688 - val_loss: 0.1215\n",
      "Epoch 218/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9750 - loss: 0.0790\n",
      "Epoch 218: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9750 - loss: 0.0790 - val_accuracy: 0.9688 - val_loss: 0.1212\n",
      "Epoch 219/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9765 - loss: 0.0778\n",
      "Epoch 219: val_accuracy improved from 0.97396 to 0.97483, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9765 - loss: 0.0778 - val_accuracy: 0.9748 - val_loss: 0.1229\n",
      "Epoch 220/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9794 - loss: 0.0782\n",
      "Epoch 220: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9794 - loss: 0.0782 - val_accuracy: 0.9688 - val_loss: 0.1209\n",
      "Epoch 221/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9761 - loss: 0.0774\n",
      "Epoch 221: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9761 - loss: 0.0774 - val_accuracy: 0.9688 - val_loss: 0.1206\n",
      "Epoch 222/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9768 - loss: 0.0768\n",
      "Epoch 222: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9768 - loss: 0.0768 - val_accuracy: 0.9740 - val_loss: 0.1222\n",
      "Epoch 223/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 0.0771\n",
      "Epoch 223: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9800 - loss: 0.0771 - val_accuracy: 0.9679 - val_loss: 0.1204\n",
      "Epoch 224/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9765 - loss: 0.0761\n",
      "Epoch 224: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9765 - loss: 0.0761 - val_accuracy: 0.9688 - val_loss: 0.1203\n",
      "Epoch 225/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9765 - loss: 0.0758\n",
      "Epoch 225: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9765 - loss: 0.0758 - val_accuracy: 0.9731 - val_loss: 0.1219\n",
      "Epoch 226/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9805 - loss: 0.0760\n",
      "Epoch 226: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9805 - loss: 0.0760 - val_accuracy: 0.9705 - val_loss: 0.1200\n",
      "Epoch 227/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9768 - loss: 0.0750\n",
      "Epoch 227: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9768 - loss: 0.0750 - val_accuracy: 0.9705 - val_loss: 0.1200\n",
      "Epoch 228/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9768 - loss: 0.0747\n",
      "Epoch 228: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9768 - loss: 0.0747 - val_accuracy: 0.9748 - val_loss: 0.1213\n",
      "Epoch 229/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9802 - loss: 0.0748\n",
      "Epoch 229: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9802 - loss: 0.0748 - val_accuracy: 0.9696 - val_loss: 0.1197\n",
      "Epoch 230/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9774 - loss: 0.0740\n",
      "Epoch 230: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9774 - loss: 0.0740 - val_accuracy: 0.9705 - val_loss: 0.1197\n",
      "Epoch 231/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9776 - loss: 0.0736\n",
      "Epoch 231: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9776 - loss: 0.0736 - val_accuracy: 0.9740 - val_loss: 0.1204\n",
      "Epoch 232/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9798 - loss: 0.0735\n",
      "Epoch 232: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9798 - loss: 0.0735 - val_accuracy: 0.9705 - val_loss: 0.1191\n",
      "Epoch 233/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9776 - loss: 0.0729\n",
      "Epoch 233: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9776 - loss: 0.0729 - val_accuracy: 0.9705 - val_loss: 0.1190\n",
      "Epoch 234/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9783 - loss: 0.0726\n",
      "Epoch 234: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9783 - loss: 0.0726 - val_accuracy: 0.9740 - val_loss: 0.1199\n",
      "Epoch 235/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9802 - loss: 0.0725\n",
      "Epoch 235: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9802 - loss: 0.0725 - val_accuracy: 0.9705 - val_loss: 0.1189\n",
      "Epoch 236/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9781 - loss: 0.0722\n",
      "Epoch 236: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9781 - loss: 0.0722 - val_accuracy: 0.9714 - val_loss: 0.1191\n",
      "Epoch 237/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9789 - loss: 0.0716\n",
      "Epoch 237: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9789 - loss: 0.0716 - val_accuracy: 0.9740 - val_loss: 0.1192\n",
      "Epoch 238/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9802 - loss: 0.0714\n",
      "Epoch 238: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9802 - loss: 0.0714 - val_accuracy: 0.9705 - val_loss: 0.1183\n",
      "Epoch 239/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9781 - loss: 0.0712\n",
      "Epoch 239: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9781 - loss: 0.0712 - val_accuracy: 0.9722 - val_loss: 0.1187\n",
      "Epoch 240/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 0.0707\n",
      "Epoch 240: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9800 - loss: 0.0707 - val_accuracy: 0.9714 - val_loss: 0.1184\n",
      "Epoch 241/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9796 - loss: 0.0704\n",
      "Epoch 241: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9796 - loss: 0.0704 - val_accuracy: 0.9696 - val_loss: 0.1183\n",
      "Epoch 242/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9787 - loss: 0.0702\n",
      "Epoch 242: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9787 - loss: 0.0702 - val_accuracy: 0.9731 - val_loss: 0.1188\n",
      "Epoch 243/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9807 - loss: 0.0699\n",
      "Epoch 243: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9807 - loss: 0.0699 - val_accuracy: 0.9714 - val_loss: 0.1183\n",
      "Epoch 244/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 0.0695\n",
      "Epoch 244: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9800 - loss: 0.0695 - val_accuracy: 0.9722 - val_loss: 0.1181\n",
      "Epoch 245/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9789 - loss: 0.0692\n",
      "Epoch 245: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9789 - loss: 0.0692 - val_accuracy: 0.9731 - val_loss: 0.1185\n",
      "Epoch 246/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9811 - loss: 0.0690\n",
      "Epoch 246: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9811 - loss: 0.0690 - val_accuracy: 0.9714 - val_loss: 0.1177\n",
      "Epoch 247/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0686\n",
      "Epoch 247: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9796 - loss: 0.0686 - val_accuracy: 0.9714 - val_loss: 0.1179\n",
      "Epoch 248/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9805 - loss: 0.0682\n",
      "Epoch 248: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9805 - loss: 0.0682 - val_accuracy: 0.9714 - val_loss: 0.1181\n",
      "Epoch 249/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9813 - loss: 0.0680\n",
      "Epoch 249: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9813 - loss: 0.0680 - val_accuracy: 0.9722 - val_loss: 0.1173\n",
      "Epoch 250/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0677\n",
      "Epoch 250: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9796 - loss: 0.0677 - val_accuracy: 0.9722 - val_loss: 0.1178\n",
      "Epoch 251/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9811 - loss: 0.0674\n",
      "Epoch 251: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9811 - loss: 0.0674 - val_accuracy: 0.9722 - val_loss: 0.1173\n",
      "Epoch 252/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9809 - loss: 0.0670\n",
      "Epoch 252: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9809 - loss: 0.0670 - val_accuracy: 0.9714 - val_loss: 0.1173\n",
      "Epoch 253/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9802 - loss: 0.0668\n",
      "Epoch 253: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9802 - loss: 0.0668 - val_accuracy: 0.9714 - val_loss: 0.1177\n",
      "Epoch 254/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9818 - loss: 0.0666\n",
      "Epoch 254: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9818 - loss: 0.0666 - val_accuracy: 0.9714 - val_loss: 0.1169\n",
      "Epoch 255/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9805 - loss: 0.0663\n",
      "Epoch 255: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9805 - loss: 0.0663 - val_accuracy: 0.9722 - val_loss: 0.1170\n",
      "Epoch 256/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9813 - loss: 0.0660\n",
      "Epoch 256: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9813 - loss: 0.0660 - val_accuracy: 0.9714 - val_loss: 0.1172\n",
      "Epoch 257/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9815 - loss: 0.0657\n",
      "Epoch 257: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9815 - loss: 0.0657 - val_accuracy: 0.9714 - val_loss: 0.1166\n",
      "Epoch 258/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9807 - loss: 0.0655\n",
      "Epoch 258: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9807 - loss: 0.0655 - val_accuracy: 0.9714 - val_loss: 0.1169\n",
      "Epoch 259/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0652\n",
      "Epoch 259: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9820 - loss: 0.0652 - val_accuracy: 0.9714 - val_loss: 0.1166\n",
      "Epoch 260/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9813 - loss: 0.0649\n",
      "Epoch 260: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9813 - loss: 0.0649 - val_accuracy: 0.9722 - val_loss: 0.1164\n",
      "Epoch 261/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9818 - loss: 0.0647\n",
      "Epoch 261: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9818 - loss: 0.0647 - val_accuracy: 0.9722 - val_loss: 0.1166\n",
      "Epoch 262/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9813 - loss: 0.0645\n",
      "Epoch 262: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9813 - loss: 0.0645 - val_accuracy: 0.9722 - val_loss: 0.1168\n",
      "Epoch 263/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9818 - loss: 0.0642\n",
      "Epoch 263: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9818 - loss: 0.0642 - val_accuracy: 0.9722 - val_loss: 0.1167\n",
      "Epoch 264/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9815 - loss: 0.0639\n",
      "Epoch 264: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9815 - loss: 0.0639 - val_accuracy: 0.9722 - val_loss: 0.1168\n",
      "Epoch 265/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9815 - loss: 0.0637\n",
      "Epoch 265: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9815 - loss: 0.0637 - val_accuracy: 0.9714 - val_loss: 0.1167\n",
      "Epoch 266/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.0634\n",
      "Epoch 266: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9818 - loss: 0.0634 - val_accuracy: 0.9714 - val_loss: 0.1164\n",
      "Epoch 267/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9820 - loss: 0.0632\n",
      "Epoch 267: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9820 - loss: 0.0632 - val_accuracy: 0.9722 - val_loss: 0.1166\n",
      "Epoch 268/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0629\n",
      "Epoch 268: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9820 - loss: 0.0629 - val_accuracy: 0.9722 - val_loss: 0.1168\n",
      "Epoch 269/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0627\n",
      "Epoch 269: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9820 - loss: 0.0627 - val_accuracy: 0.9705 - val_loss: 0.1165\n",
      "Epoch 270/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9815 - loss: 0.0624\n",
      "Epoch 270: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9815 - loss: 0.0624 - val_accuracy: 0.9722 - val_loss: 0.1165\n",
      "Epoch 271/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9822 - loss: 0.0622\n",
      "Epoch 271: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9822 - loss: 0.0622 - val_accuracy: 0.9722 - val_loss: 0.1162\n",
      "Epoch 272/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.0620\n",
      "Epoch 272: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9818 - loss: 0.0620 - val_accuracy: 0.9722 - val_loss: 0.1163\n",
      "Epoch 273/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0617\n",
      "Epoch 273: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9824 - loss: 0.0617 - val_accuracy: 0.9722 - val_loss: 0.1160\n",
      "Epoch 274/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9824 - loss: 0.0614\n",
      "Epoch 274: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9824 - loss: 0.0614 - val_accuracy: 0.9731 - val_loss: 0.1162\n",
      "Epoch 275/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0612\n",
      "Epoch 275: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9820 - loss: 0.0612 - val_accuracy: 0.9722 - val_loss: 0.1158\n",
      "Epoch 276/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0610\n",
      "Epoch 276: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9826 - loss: 0.0610 - val_accuracy: 0.9722 - val_loss: 0.1158\n",
      "Epoch 277/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0607\n",
      "Epoch 277: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0607 - val_accuracy: 0.9722 - val_loss: 0.1157\n",
      "Epoch 278/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9822 - loss: 0.0605\n",
      "Epoch 278: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9822 - loss: 0.0605 - val_accuracy: 0.9722 - val_loss: 0.1156\n",
      "Epoch 279/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0603\n",
      "Epoch 279: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0603 - val_accuracy: 0.9731 - val_loss: 0.1153\n",
      "Epoch 280/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9818 - loss: 0.0600\n",
      "Epoch 280: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9818 - loss: 0.0600 - val_accuracy: 0.9722 - val_loss: 0.1157\n",
      "Epoch 281/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0598\n",
      "Epoch 281: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0598 - val_accuracy: 0.9722 - val_loss: 0.1155\n",
      "Epoch 282/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9824 - loss: 0.0596\n",
      "Epoch 282: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9824 - loss: 0.0596 - val_accuracy: 0.9722 - val_loss: 0.1154\n",
      "Epoch 283/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9826 - loss: 0.0594\n",
      "Epoch 283: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9826 - loss: 0.0594 - val_accuracy: 0.9722 - val_loss: 0.1158\n",
      "Epoch 284/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0592\n",
      "Epoch 284: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9824 - loss: 0.0592 - val_accuracy: 0.9722 - val_loss: 0.1154\n",
      "Epoch 285/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9815 - loss: 0.0591\n",
      "Epoch 285: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9815 - loss: 0.0591 - val_accuracy: 0.9740 - val_loss: 0.1160\n",
      "Epoch 286/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9837 - loss: 0.0590\n",
      "Epoch 286: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9837 - loss: 0.0590 - val_accuracy: 0.9722 - val_loss: 0.1154\n",
      "Epoch 287/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9811 - loss: 0.0589\n",
      "Epoch 287: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9811 - loss: 0.0589 - val_accuracy: 0.9722 - val_loss: 0.1157\n",
      "Epoch 288/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9837 - loss: 0.0586\n",
      "Epoch 288: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9837 - loss: 0.0586 - val_accuracy: 0.9722 - val_loss: 0.1149\n",
      "Epoch 289/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9813 - loss: 0.0583\n",
      "Epoch 289: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9813 - loss: 0.0583 - val_accuracy: 0.9731 - val_loss: 0.1156\n",
      "Epoch 290/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9835 - loss: 0.0581\n",
      "Epoch 290: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9835 - loss: 0.0581 - val_accuracy: 0.9731 - val_loss: 0.1146\n",
      "Epoch 291/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9820 - loss: 0.0577\n",
      "Epoch 291: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9820 - loss: 0.0577 - val_accuracy: 0.9731 - val_loss: 0.1150\n",
      "Epoch 292/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9831 - loss: 0.0574\n",
      "Epoch 292: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9831 - loss: 0.0574 - val_accuracy: 0.9722 - val_loss: 0.1148\n",
      "Epoch 293/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0571\n",
      "Epoch 293: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9826 - loss: 0.0571 - val_accuracy: 0.9722 - val_loss: 0.1145\n",
      "Epoch 294/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9824 - loss: 0.0569\n",
      "Epoch 294: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9824 - loss: 0.0569 - val_accuracy: 0.9731 - val_loss: 0.1147\n",
      "Epoch 295/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0567\n",
      "Epoch 295: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9826 - loss: 0.0567 - val_accuracy: 0.9722 - val_loss: 0.1145\n",
      "Epoch 296/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0565\n",
      "Epoch 296: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9826 - loss: 0.0565 - val_accuracy: 0.9722 - val_loss: 0.1147\n",
      "Epoch 297/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9833 - loss: 0.0563\n",
      "Epoch 297: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9833 - loss: 0.0563 - val_accuracy: 0.9722 - val_loss: 0.1148\n",
      "Epoch 298/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9824 - loss: 0.0560\n",
      "Epoch 298: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0560 - val_accuracy: 0.9731 - val_loss: 0.1146\n",
      "Epoch 299/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9828 - loss: 0.0558\n",
      "Epoch 299: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9828 - loss: 0.0558 - val_accuracy: 0.9731 - val_loss: 0.1145\n",
      "Epoch 300/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9822 - loss: 0.0556\n",
      "Epoch 300: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9822 - loss: 0.0556 - val_accuracy: 0.9740 - val_loss: 0.1147\n",
      "Epoch 301/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9835 - loss: 0.0554\n",
      "Epoch 301: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9835 - loss: 0.0554 - val_accuracy: 0.9705 - val_loss: 0.1142\n",
      "Epoch 302/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0554\n",
      "Epoch 302: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9824 - loss: 0.0554 - val_accuracy: 0.9748 - val_loss: 0.1150\n",
      "Epoch 303/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9848 - loss: 0.0555\n",
      "Epoch 303: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9848 - loss: 0.0555 - val_accuracy: 0.9722 - val_loss: 0.1147\n",
      "Epoch 304/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9805 - loss: 0.0559\n",
      "Epoch 304: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9805 - loss: 0.0559 - val_accuracy: 0.9740 - val_loss: 0.1160\n",
      "Epoch 305/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9861 - loss: 0.0566\n",
      "Epoch 305: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9861 - loss: 0.0566 - val_accuracy: 0.9696 - val_loss: 0.1169\n",
      "Epoch 306/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9781 - loss: 0.0587\n",
      "Epoch 306: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9781 - loss: 0.0587 - val_accuracy: 0.9748 - val_loss: 0.1195\n",
      "Epoch 307/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0591\n",
      "Epoch 307: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9861 - loss: 0.0591 - val_accuracy: 0.9705 - val_loss: 0.1170\n",
      "Epoch 308/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9768 - loss: 0.0593\n",
      "Epoch 308: val_accuracy improved from 0.97483 to 0.97569, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9768 - loss: 0.0593 - val_accuracy: 0.9757 - val_loss: 0.1149\n",
      "Epoch 309/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9859 - loss: 0.0555\n",
      "Epoch 309: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9859 - loss: 0.0555 - val_accuracy: 0.9722 - val_loss: 0.1130\n",
      "Epoch 310/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9837 - loss: 0.0535\n",
      "Epoch 310: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9837 - loss: 0.0535 - val_accuracy: 0.9722 - val_loss: 0.1126\n",
      "Epoch 311/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9822 - loss: 0.0541\n",
      "Epoch 311: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9822 - loss: 0.0541 - val_accuracy: 0.9757 - val_loss: 0.1158\n",
      "Epoch 312/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9861 - loss: 0.0557\n",
      "Epoch 312: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9861 - loss: 0.0557 - val_accuracy: 0.9722 - val_loss: 0.1164\n",
      "Epoch 313/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9785 - loss: 0.0570\n",
      "Epoch 313: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9785 - loss: 0.0570 - val_accuracy: 0.9740 - val_loss: 0.1151\n",
      "Epoch 314/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9863 - loss: 0.0544\n",
      "Epoch 314: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9863 - loss: 0.0544 - val_accuracy: 0.9722 - val_loss: 0.1119\n",
      "Epoch 315/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9835 - loss: 0.0527\n",
      "Epoch 315: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9835 - loss: 0.0527 - val_accuracy: 0.9748 - val_loss: 0.1127\n",
      "Epoch 316/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9833 - loss: 0.0525\n",
      "Epoch 316: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9833 - loss: 0.0525 - val_accuracy: 0.9748 - val_loss: 0.1137\n",
      "Epoch 317/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9868 - loss: 0.0532\n",
      "Epoch 317: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9868 - loss: 0.0532 - val_accuracy: 0.9731 - val_loss: 0.1127\n",
      "Epoch 318/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9807 - loss: 0.0538\n",
      "Epoch 318: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9807 - loss: 0.0538 - val_accuracy: 0.9740 - val_loss: 0.1135\n",
      "Epoch 319/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9868 - loss: 0.0527\n",
      "Epoch 319: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9868 - loss: 0.0527 - val_accuracy: 0.9748 - val_loss: 0.1119\n",
      "Epoch 320/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9839 - loss: 0.0516\n",
      "Epoch 320: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9839 - loss: 0.0516 - val_accuracy: 0.9714 - val_loss: 0.1106\n",
      "Epoch 321/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9841 - loss: 0.0511\n",
      "Epoch 321: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9841 - loss: 0.0511 - val_accuracy: 0.9740 - val_loss: 0.1119\n",
      "Epoch 322/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9868 - loss: 0.0512\n",
      "Epoch 322: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9868 - loss: 0.0512 - val_accuracy: 0.9722 - val_loss: 0.1128\n",
      "Epoch 323/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9820 - loss: 0.0516\n",
      "Epoch 323: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9820 - loss: 0.0516 - val_accuracy: 0.9731 - val_loss: 0.1125\n",
      "Epoch 324/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0518\n",
      "Epoch 324: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9865 - loss: 0.0518 - val_accuracy: 0.9722 - val_loss: 0.1129\n",
      "Epoch 325/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.0517\n",
      "Epoch 325: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9818 - loss: 0.0517 - val_accuracy: 0.9757 - val_loss: 0.1128\n",
      "Epoch 326/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9868 - loss: 0.0513\n",
      "Epoch 326: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9868 - loss: 0.0513 - val_accuracy: 0.9731 - val_loss: 0.1115\n",
      "Epoch 327/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9833 - loss: 0.0506\n",
      "Epoch 327: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9833 - loss: 0.0506 - val_accuracy: 0.9722 - val_loss: 0.1115\n",
      "Epoch 328/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9863 - loss: 0.0500\n",
      "Epoch 328: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9863 - loss: 0.0500 - val_accuracy: 0.9731 - val_loss: 0.1119\n",
      "Epoch 329/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9848 - loss: 0.0495\n",
      "Epoch 329: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9848 - loss: 0.0495 - val_accuracy: 0.9731 - val_loss: 0.1113\n",
      "Epoch 330/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9854 - loss: 0.0493\n",
      "Epoch 330: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9854 - loss: 0.0493 - val_accuracy: 0.9722 - val_loss: 0.1113\n",
      "Epoch 331/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0493\n",
      "Epoch 331: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9865 - loss: 0.0493 - val_accuracy: 0.9731 - val_loss: 0.1120\n",
      "Epoch 332/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9837 - loss: 0.0496\n",
      "Epoch 332: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9837 - loss: 0.0496 - val_accuracy: 0.9748 - val_loss: 0.1120\n",
      "Epoch 333/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - loss: 0.0499\n",
      "Epoch 333: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9876 - loss: 0.0499 - val_accuracy: 0.9714 - val_loss: 0.1120\n",
      "Epoch 334/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9822 - loss: 0.0503\n",
      "Epoch 334: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9822 - loss: 0.0503 - val_accuracy: 0.9757 - val_loss: 0.1124\n",
      "Epoch 335/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9874 - loss: 0.0503\n",
      "Epoch 335: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9874 - loss: 0.0503 - val_accuracy: 0.9722 - val_loss: 0.1120\n",
      "Epoch 336/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9824 - loss: 0.0501\n",
      "Epoch 336: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9824 - loss: 0.0501 - val_accuracy: 0.9748 - val_loss: 0.1107\n",
      "Epoch 337/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9878 - loss: 0.0491\n",
      "Epoch 337: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9878 - loss: 0.0491 - val_accuracy: 0.9731 - val_loss: 0.1106\n",
      "Epoch 338/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9850 - loss: 0.0482\n",
      "Epoch 338: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9850 - loss: 0.0482 - val_accuracy: 0.9740 - val_loss: 0.1106\n",
      "Epoch 339/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9868 - loss: 0.0476\n",
      "Epoch 339: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9868 - loss: 0.0476 - val_accuracy: 0.9714 - val_loss: 0.1105\n",
      "Epoch 340/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9878 - loss: 0.0476\n",
      "Epoch 340: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9878 - loss: 0.0476 - val_accuracy: 0.9731 - val_loss: 0.1114\n",
      "Epoch 341/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9839 - loss: 0.0479\n",
      "Epoch 341: val_accuracy improved from 0.97569 to 0.97656, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9839 - loss: 0.0479 - val_accuracy: 0.9766 - val_loss: 0.1123\n",
      "Epoch 342/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9878 - loss: 0.0484\n",
      "Epoch 342: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9878 - loss: 0.0484 - val_accuracy: 0.9714 - val_loss: 0.1119\n",
      "Epoch 343/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9828 - loss: 0.0488\n",
      "Epoch 343: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9828 - loss: 0.0488 - val_accuracy: 0.9748 - val_loss: 0.1117\n",
      "Epoch 344/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9878 - loss: 0.0484\n",
      "Epoch 344: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9878 - loss: 0.0484 - val_accuracy: 0.9722 - val_loss: 0.1118\n",
      "Epoch 345/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9839 - loss: 0.0481\n",
      "Epoch 345: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9839 - loss: 0.0481 - val_accuracy: 0.9731 - val_loss: 0.1103\n",
      "Epoch 346/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0471\n",
      "Epoch 346: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9883 - loss: 0.0471 - val_accuracy: 0.9740 - val_loss: 0.1099\n",
      "Epoch 347/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9868 - loss: 0.0464\n",
      "Epoch 347: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9868 - loss: 0.0464 - val_accuracy: 0.9740 - val_loss: 0.1105\n",
      "Epoch 348/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9863 - loss: 0.0462\n",
      "Epoch 348: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9863 - loss: 0.0462 - val_accuracy: 0.9722 - val_loss: 0.1106\n",
      "Epoch 349/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0464\n",
      "Epoch 349: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9883 - loss: 0.0464 - val_accuracy: 0.9731 - val_loss: 0.1110\n",
      "Epoch 350/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9839 - loss: 0.0467\n",
      "Epoch 350: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9839 - loss: 0.0467 - val_accuracy: 0.9757 - val_loss: 0.1110\n",
      "Epoch 351/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0466\n",
      "Epoch 351: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9883 - loss: 0.0466 - val_accuracy: 0.9731 - val_loss: 0.1108\n",
      "Epoch 352/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9850 - loss: 0.0462\n",
      "Epoch 352: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9850 - loss: 0.0462 - val_accuracy: 0.9714 - val_loss: 0.1100\n",
      "Epoch 353/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9881 - loss: 0.0456\n",
      "Epoch 353: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9881 - loss: 0.0456 - val_accuracy: 0.9740 - val_loss: 0.1102\n",
      "Epoch 354/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0452\n",
      "Epoch 354: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9865 - loss: 0.0452 - val_accuracy: 0.9740 - val_loss: 0.1104\n",
      "Epoch 355/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0451\n",
      "Epoch 355: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9865 - loss: 0.0451 - val_accuracy: 0.9722 - val_loss: 0.1100\n",
      "Epoch 356/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9885 - loss: 0.0451\n",
      "Epoch 356: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9885 - loss: 0.0451 - val_accuracy: 0.9731 - val_loss: 0.1104\n",
      "Epoch 357/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9861 - loss: 0.0450\n",
      "Epoch 357: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9861 - loss: 0.0450 - val_accuracy: 0.9722 - val_loss: 0.1106\n",
      "Epoch 358/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9881 - loss: 0.0449\n",
      "Epoch 358: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9881 - loss: 0.0449 - val_accuracy: 0.9731 - val_loss: 0.1104\n",
      "Epoch 359/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9861 - loss: 0.0446\n",
      "Epoch 359: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9861 - loss: 0.0446 - val_accuracy: 0.9714 - val_loss: 0.1105\n",
      "Epoch 360/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9874 - loss: 0.0444\n",
      "Epoch 360: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9874 - loss: 0.0444 - val_accuracy: 0.9731 - val_loss: 0.1105\n",
      "Epoch 361/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9872 - loss: 0.0442\n",
      "Epoch 361: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9872 - loss: 0.0442 - val_accuracy: 0.9722 - val_loss: 0.1101\n",
      "Epoch 362/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9872 - loss: 0.0440\n",
      "Epoch 362: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9872 - loss: 0.0440 - val_accuracy: 0.9722 - val_loss: 0.1104\n",
      "Epoch 363/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9881 - loss: 0.0440\n",
      "Epoch 363: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9881 - loss: 0.0440 - val_accuracy: 0.9740 - val_loss: 0.1105\n",
      "Epoch 364/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0438\n",
      "Epoch 364: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9865 - loss: 0.0438 - val_accuracy: 0.9731 - val_loss: 0.1102\n",
      "Epoch 365/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9885 - loss: 0.0437\n",
      "Epoch 365: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9885 - loss: 0.0437 - val_accuracy: 0.9740 - val_loss: 0.1102\n",
      "Epoch 366/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0436\n",
      "Epoch 366: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9865 - loss: 0.0436 - val_accuracy: 0.9731 - val_loss: 0.1102\n",
      "Epoch 367/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9883 - loss: 0.0435\n",
      "Epoch 367: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9883 - loss: 0.0435 - val_accuracy: 0.9740 - val_loss: 0.1104\n",
      "Epoch 368/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0433\n",
      "Epoch 368: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9865 - loss: 0.0433 - val_accuracy: 0.9740 - val_loss: 0.1106\n",
      "Epoch 369/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9883 - loss: 0.0432\n",
      "Epoch 369: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9883 - loss: 0.0432 - val_accuracy: 0.9731 - val_loss: 0.1109\n",
      "Epoch 370/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0432\n",
      "Epoch 370: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9861 - loss: 0.0432 - val_accuracy: 0.9722 - val_loss: 0.1107\n",
      "Epoch 371/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9891 - loss: 0.0430\n",
      "Epoch 371: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9891 - loss: 0.0430 - val_accuracy: 0.9740 - val_loss: 0.1110\n",
      "Epoch 372/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9863 - loss: 0.0428\n",
      "Epoch 372: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9863 - loss: 0.0428 - val_accuracy: 0.9731 - val_loss: 0.1109\n",
      "Epoch 373/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0427\n",
      "Epoch 373: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9889 - loss: 0.0427 - val_accuracy: 0.9740 - val_loss: 0.1112\n",
      "Epoch 374/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9868 - loss: 0.0425\n",
      "Epoch 374: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9868 - loss: 0.0425 - val_accuracy: 0.9740 - val_loss: 0.1113\n",
      "Epoch 375/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9891 - loss: 0.0424\n",
      "Epoch 375: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9891 - loss: 0.0424 - val_accuracy: 0.9740 - val_loss: 0.1114\n",
      "Epoch 376/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9868 - loss: 0.0422\n",
      "Epoch 376: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9868 - loss: 0.0422 - val_accuracy: 0.9748 - val_loss: 0.1111\n",
      "Epoch 377/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0421\n",
      "Epoch 377: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9894 - loss: 0.0421 - val_accuracy: 0.9731 - val_loss: 0.1117\n",
      "Epoch 378/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9861 - loss: 0.0421\n",
      "Epoch 378: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9861 - loss: 0.0421 - val_accuracy: 0.9748 - val_loss: 0.1115\n",
      "Epoch 379/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0420\n",
      "Epoch 379: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9896 - loss: 0.0420 - val_accuracy: 0.9731 - val_loss: 0.1120\n",
      "Epoch 380/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0420\n",
      "Epoch 380: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9865 - loss: 0.0420 - val_accuracy: 0.9757 - val_loss: 0.1120\n",
      "Epoch 381/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9889 - loss: 0.0420\n",
      "Epoch 381: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9889 - loss: 0.0420 - val_accuracy: 0.9714 - val_loss: 0.1118\n",
      "Epoch 382/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0420\n",
      "Epoch 382: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9861 - loss: 0.0420 - val_accuracy: 0.9757 - val_loss: 0.1118\n",
      "Epoch 383/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9891 - loss: 0.0423\n",
      "Epoch 383: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9891 - loss: 0.0423 - val_accuracy: 0.9696 - val_loss: 0.1129\n",
      "Epoch 384/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9850 - loss: 0.0429\n",
      "Epoch 384: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9850 - loss: 0.0429 - val_accuracy: 0.9748 - val_loss: 0.1131\n",
      "Epoch 385/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9887 - loss: 0.0440\n",
      "Epoch 385: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9887 - loss: 0.0440 - val_accuracy: 0.9696 - val_loss: 0.1158\n",
      "Epoch 386/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9815 - loss: 0.0457\n",
      "Epoch 386: val_accuracy improved from 0.97656 to 0.97743, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9815 - loss: 0.0457 - val_accuracy: 0.9774 - val_loss: 0.1156\n",
      "Epoch 387/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9887 - loss: 0.0458\n",
      "Epoch 387: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9887 - loss: 0.0458 - val_accuracy: 0.9688 - val_loss: 0.1140\n",
      "Epoch 388/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9822 - loss: 0.0444\n",
      "Epoch 388: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9822 - loss: 0.0444 - val_accuracy: 0.9766 - val_loss: 0.1102\n",
      "Epoch 389/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0412\n",
      "Epoch 389: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9896 - loss: 0.0412 - val_accuracy: 0.9731 - val_loss: 0.1100\n",
      "Epoch 390/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9891 - loss: 0.0403\n",
      "Epoch 390: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9891 - loss: 0.0403 - val_accuracy: 0.9696 - val_loss: 0.1119\n",
      "Epoch 391/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9852 - loss: 0.0418\n",
      "Epoch 391: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9852 - loss: 0.0418 - val_accuracy: 0.9757 - val_loss: 0.1136\n",
      "Epoch 392/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0436\n",
      "Epoch 392: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9889 - loss: 0.0436 - val_accuracy: 0.9688 - val_loss: 0.1155\n",
      "Epoch 393/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0444\n",
      "Epoch 393: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9824 - loss: 0.0444 - val_accuracy: 0.9766 - val_loss: 0.1129\n",
      "Epoch 394/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0424\n",
      "Epoch 394: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9889 - loss: 0.0424 - val_accuracy: 0.9722 - val_loss: 0.1112\n",
      "Epoch 395/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9863 - loss: 0.0402\n",
      "Epoch 395: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9863 - loss: 0.0402 - val_accuracy: 0.9722 - val_loss: 0.1106\n",
      "Epoch 396/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0395\n",
      "Epoch 396: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9883 - loss: 0.0395 - val_accuracy: 0.9757 - val_loss: 0.1122\n",
      "Epoch 397/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9889 - loss: 0.0408\n",
      "Epoch 397: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0408 - val_accuracy: 0.9679 - val_loss: 0.1144\n",
      "Epoch 398/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9833 - loss: 0.0424\n",
      "Epoch 398: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9833 - loss: 0.0424 - val_accuracy: 0.9766 - val_loss: 0.1138\n",
      "Epoch 399/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9889 - loss: 0.0416\n",
      "Epoch 399: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0416 - val_accuracy: 0.9722 - val_loss: 0.1120\n",
      "Epoch 400/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9870 - loss: 0.0397\n",
      "Epoch 400: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9870 - loss: 0.0397 - val_accuracy: 0.9722 - val_loss: 0.1105\n",
      "Epoch 401/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9889 - loss: 0.0389\n",
      "Epoch 401: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0389 - val_accuracy: 0.9766 - val_loss: 0.1114\n",
      "Epoch 402/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9894 - loss: 0.0401\n",
      "Epoch 402: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9894 - loss: 0.0401 - val_accuracy: 0.9688 - val_loss: 0.1137\n",
      "Epoch 403/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9846 - loss: 0.0411\n",
      "Epoch 403: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9846 - loss: 0.0411 - val_accuracy: 0.9766 - val_loss: 0.1126\n",
      "Epoch 404/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9889 - loss: 0.0401\n",
      "Epoch 404: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9889 - loss: 0.0401 - val_accuracy: 0.9714 - val_loss: 0.1120\n",
      "Epoch 405/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9878 - loss: 0.0387\n",
      "Epoch 405: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9878 - loss: 0.0387 - val_accuracy: 0.9722 - val_loss: 0.1119\n",
      "Epoch 406/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0385\n",
      "Epoch 406: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9883 - loss: 0.0385 - val_accuracy: 0.9774 - val_loss: 0.1122\n",
      "Epoch 407/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0394\n",
      "Epoch 407: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9898 - loss: 0.0394 - val_accuracy: 0.9705 - val_loss: 0.1130\n",
      "Epoch 408/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9859 - loss: 0.0397\n",
      "Epoch 408: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9859 - loss: 0.0397 - val_accuracy: 0.9757 - val_loss: 0.1114\n",
      "Epoch 409/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0387\n",
      "Epoch 409: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0387 - val_accuracy: 0.9722 - val_loss: 0.1112\n",
      "Epoch 410/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0379\n",
      "Epoch 410: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9889 - loss: 0.0379 - val_accuracy: 0.9722 - val_loss: 0.1115\n",
      "Epoch 411/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - loss: 0.0380\n",
      "Epoch 411: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9876 - loss: 0.0380 - val_accuracy: 0.9757 - val_loss: 0.1119\n",
      "Epoch 412/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0387\n",
      "Epoch 412: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9898 - loss: 0.0387 - val_accuracy: 0.9714 - val_loss: 0.1134\n",
      "Epoch 413/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9865 - loss: 0.0388\n",
      "Epoch 413: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9865 - loss: 0.0388 - val_accuracy: 0.9766 - val_loss: 0.1124\n",
      "Epoch 414/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0380\n",
      "Epoch 414: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0380 - val_accuracy: 0.9722 - val_loss: 0.1117\n",
      "Epoch 415/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0374\n",
      "Epoch 415: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9894 - loss: 0.0374 - val_accuracy: 0.9722 - val_loss: 0.1118\n",
      "Epoch 416/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9878 - loss: 0.0375\n",
      "Epoch 416: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9878 - loss: 0.0375 - val_accuracy: 0.9748 - val_loss: 0.1117\n",
      "Epoch 417/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0381\n",
      "Epoch 417: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9900 - loss: 0.0381 - val_accuracy: 0.9714 - val_loss: 0.1123\n",
      "Epoch 418/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - loss: 0.0381\n",
      "Epoch 418: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9876 - loss: 0.0381 - val_accuracy: 0.9740 - val_loss: 0.1116\n",
      "Epoch 419/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9904 - loss: 0.0375\n",
      "Epoch 419: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0375 - val_accuracy: 0.9722 - val_loss: 0.1115\n",
      "Epoch 420/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9896 - loss: 0.0368\n",
      "Epoch 420: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9896 - loss: 0.0368 - val_accuracy: 0.9722 - val_loss: 0.1116\n",
      "Epoch 421/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9894 - loss: 0.0369\n",
      "Epoch 421: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9894 - loss: 0.0369 - val_accuracy: 0.9757 - val_loss: 0.1116\n",
      "Epoch 422/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0373\n",
      "Epoch 422: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9900 - loss: 0.0373 - val_accuracy: 0.9731 - val_loss: 0.1125\n",
      "Epoch 423/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9874 - loss: 0.0374\n",
      "Epoch 423: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9874 - loss: 0.0374 - val_accuracy: 0.9748 - val_loss: 0.1111\n",
      "Epoch 424/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0369\n",
      "Epoch 424: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0369 - val_accuracy: 0.9722 - val_loss: 0.1113\n",
      "Epoch 425/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9896 - loss: 0.0364\n",
      "Epoch 425: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9896 - loss: 0.0364 - val_accuracy: 0.9722 - val_loss: 0.1119\n",
      "Epoch 426/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9896 - loss: 0.0363\n",
      "Epoch 426: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9896 - loss: 0.0363 - val_accuracy: 0.9748 - val_loss: 0.1123\n",
      "Epoch 427/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9904 - loss: 0.0365\n",
      "Epoch 427: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9904 - loss: 0.0365 - val_accuracy: 0.9714 - val_loss: 0.1130\n",
      "Epoch 428/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0366\n",
      "Epoch 428: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9883 - loss: 0.0366 - val_accuracy: 0.9748 - val_loss: 0.1126\n",
      "Epoch 429/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9902 - loss: 0.0364\n",
      "Epoch 429: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9902 - loss: 0.0364 - val_accuracy: 0.9722 - val_loss: 0.1123\n",
      "Epoch 430/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0360\n",
      "Epoch 430: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9894 - loss: 0.0360 - val_accuracy: 0.9722 - val_loss: 0.1118\n",
      "Epoch 431/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0358\n",
      "Epoch 431: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9900 - loss: 0.0358 - val_accuracy: 0.9731 - val_loss: 0.1118\n",
      "Epoch 432/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0358\n",
      "Epoch 432: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9904 - loss: 0.0358 - val_accuracy: 0.9722 - val_loss: 0.1124\n",
      "Epoch 433/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9891 - loss: 0.0358\n",
      "Epoch 433: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9891 - loss: 0.0358 - val_accuracy: 0.9740 - val_loss: 0.1120\n",
      "Epoch 434/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9904 - loss: 0.0357\n",
      "Epoch 434: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0357 - val_accuracy: 0.9722 - val_loss: 0.1125\n",
      "Epoch 435/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9896 - loss: 0.0356\n",
      "Epoch 435: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9896 - loss: 0.0356 - val_accuracy: 0.9740 - val_loss: 0.1126\n",
      "Epoch 436/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9904 - loss: 0.0354\n",
      "Epoch 436: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0354 - val_accuracy: 0.9722 - val_loss: 0.1121\n",
      "Epoch 437/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9902 - loss: 0.0353\n",
      "Epoch 437: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9902 - loss: 0.0353 - val_accuracy: 0.9722 - val_loss: 0.1123\n",
      "Epoch 438/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9898 - loss: 0.0352\n",
      "Epoch 438: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9898 - loss: 0.0352 - val_accuracy: 0.9722 - val_loss: 0.1122\n",
      "Epoch 439/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0352\n",
      "Epoch 439: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0352 - val_accuracy: 0.9722 - val_loss: 0.1122\n",
      "Epoch 440/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0351\n",
      "Epoch 440: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9898 - loss: 0.0351 - val_accuracy: 0.9748 - val_loss: 0.1124\n",
      "Epoch 441/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0350\n",
      "Epoch 441: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0350 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 442/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0349\n",
      "Epoch 442: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9894 - loss: 0.0349 - val_accuracy: 0.9731 - val_loss: 0.1133\n",
      "Epoch 443/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0348\n",
      "Epoch 443: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0348 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 444/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0347\n",
      "Epoch 444: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9902 - loss: 0.0347 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 445/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9902 - loss: 0.0346\n",
      "Epoch 445: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0346 - val_accuracy: 0.9731 - val_loss: 0.1130\n",
      "Epoch 446/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0345\n",
      "Epoch 446: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9900 - loss: 0.0345 - val_accuracy: 0.9722 - val_loss: 0.1128\n",
      "Epoch 447/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0344\n",
      "Epoch 447: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0344 - val_accuracy: 0.9740 - val_loss: 0.1128\n",
      "Epoch 448/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9904 - loss: 0.0344\n",
      "Epoch 448: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9904 - loss: 0.0344 - val_accuracy: 0.9722 - val_loss: 0.1130\n",
      "Epoch 449/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0343\n",
      "Epoch 449: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9900 - loss: 0.0343 - val_accuracy: 0.9740 - val_loss: 0.1125\n",
      "Epoch 450/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0342\n",
      "Epoch 450: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0342 - val_accuracy: 0.9722 - val_loss: 0.1130\n",
      "Epoch 451/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0341\n",
      "Epoch 451: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0341 - val_accuracy: 0.9731 - val_loss: 0.1126\n",
      "Epoch 452/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0340\n",
      "Epoch 452: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9900 - loss: 0.0340 - val_accuracy: 0.9722 - val_loss: 0.1124\n",
      "Epoch 453/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9904 - loss: 0.0339\n",
      "Epoch 453: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9904 - loss: 0.0339 - val_accuracy: 0.9731 - val_loss: 0.1124\n",
      "Epoch 454/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0338\n",
      "Epoch 454: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0338 - val_accuracy: 0.9722 - val_loss: 0.1126\n",
      "Epoch 455/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0338\n",
      "Epoch 455: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0338 - val_accuracy: 0.9731 - val_loss: 0.1126\n",
      "Epoch 456/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0337\n",
      "Epoch 456: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9904 - loss: 0.0337 - val_accuracy: 0.9722 - val_loss: 0.1127\n",
      "Epoch 457/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0336\n",
      "Epoch 457: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0336 - val_accuracy: 0.9740 - val_loss: 0.1123\n",
      "Epoch 458/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9902 - loss: 0.0336\n",
      "Epoch 458: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0336 - val_accuracy: 0.9722 - val_loss: 0.1125\n",
      "Epoch 459/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9900 - loss: 0.0335\n",
      "Epoch 459: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9900 - loss: 0.0335 - val_accuracy: 0.9748 - val_loss: 0.1124\n",
      "Epoch 460/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0335\n",
      "Epoch 460: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0335 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 461/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0335\n",
      "Epoch 461: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9898 - loss: 0.0335 - val_accuracy: 0.9757 - val_loss: 0.1129\n",
      "Epoch 462/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9913 - loss: 0.0335\n",
      "Epoch 462: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9913 - loss: 0.0335 - val_accuracy: 0.9714 - val_loss: 0.1139\n",
      "Epoch 463/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0334\n",
      "Epoch 463: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0334 - val_accuracy: 0.9757 - val_loss: 0.1130\n",
      "Epoch 464/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0332\n",
      "Epoch 464: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0332 - val_accuracy: 0.9722 - val_loss: 0.1129\n",
      "Epoch 465/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0331\n",
      "Epoch 465: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9902 - loss: 0.0331 - val_accuracy: 0.9740 - val_loss: 0.1130\n",
      "Epoch 466/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0329\n",
      "Epoch 466: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9902 - loss: 0.0329 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 467/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0328\n",
      "Epoch 467: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0328 - val_accuracy: 0.9740 - val_loss: 0.1130\n",
      "Epoch 468/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0328\n",
      "Epoch 468: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0328 - val_accuracy: 0.9731 - val_loss: 0.1130\n",
      "Epoch 469/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9907 - loss: 0.0327\n",
      "Epoch 469: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9907 - loss: 0.0327 - val_accuracy: 0.9722 - val_loss: 0.1125\n",
      "Epoch 470/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9909 - loss: 0.0326\n",
      "Epoch 470: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9909 - loss: 0.0326 - val_accuracy: 0.9740 - val_loss: 0.1125\n",
      "Epoch 471/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0326\n",
      "Epoch 471: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9902 - loss: 0.0326 - val_accuracy: 0.9722 - val_loss: 0.1127\n",
      "Epoch 472/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0326\n",
      "Epoch 472: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9902 - loss: 0.0326 - val_accuracy: 0.9757 - val_loss: 0.1124\n",
      "Epoch 473/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9913 - loss: 0.0326\n",
      "Epoch 473: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9913 - loss: 0.0326 - val_accuracy: 0.9714 - val_loss: 0.1131\n",
      "Epoch 474/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0327\n",
      "Epoch 474: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9900 - loss: 0.0327 - val_accuracy: 0.9748 - val_loss: 0.1124\n",
      "Epoch 475/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9915 - loss: 0.0327\n",
      "Epoch 475: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9915 - loss: 0.0327 - val_accuracy: 0.9714 - val_loss: 0.1136\n",
      "Epoch 476/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0328\n",
      "Epoch 476: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9896 - loss: 0.0328 - val_accuracy: 0.9748 - val_loss: 0.1130\n",
      "Epoch 477/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9920 - loss: 0.0328\n",
      "Epoch 477: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9920 - loss: 0.0328 - val_accuracy: 0.9714 - val_loss: 0.1143\n",
      "Epoch 478/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9896 - loss: 0.0329\n",
      "Epoch 478: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9896 - loss: 0.0329 - val_accuracy: 0.9757 - val_loss: 0.1130\n",
      "Epoch 479/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9920 - loss: 0.0327\n",
      "Epoch 479: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9920 - loss: 0.0327 - val_accuracy: 0.9714 - val_loss: 0.1135\n",
      "Epoch 480/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9896 - loss: 0.0324\n",
      "Epoch 480: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9896 - loss: 0.0324 - val_accuracy: 0.9757 - val_loss: 0.1120\n",
      "Epoch 481/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9920 - loss: 0.0322\n",
      "Epoch 481: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9920 - loss: 0.0322 - val_accuracy: 0.9714 - val_loss: 0.1130\n",
      "Epoch 482/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0319\n",
      "Epoch 482: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0319 - val_accuracy: 0.9740 - val_loss: 0.1129\n",
      "Epoch 483/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9909 - loss: 0.0318\n",
      "Epoch 483: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9909 - loss: 0.0318 - val_accuracy: 0.9731 - val_loss: 0.1132\n",
      "Epoch 484/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0316\n",
      "Epoch 484: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0316 - val_accuracy: 0.9740 - val_loss: 0.1129\n",
      "Epoch 485/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0315\n",
      "Epoch 485: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0315 - val_accuracy: 0.9731 - val_loss: 0.1129\n",
      "Epoch 486/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9907 - loss: 0.0315\n",
      "Epoch 486: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9907 - loss: 0.0315 - val_accuracy: 0.9731 - val_loss: 0.1130\n",
      "Epoch 487/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0314\n",
      "Epoch 487: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0314 - val_accuracy: 0.9740 - val_loss: 0.1130\n",
      "Epoch 488/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9909 - loss: 0.0314\n",
      "Epoch 488: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9909 - loss: 0.0314 - val_accuracy: 0.9714 - val_loss: 0.1130\n",
      "Epoch 489/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0313\n",
      "Epoch 489: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0313 - val_accuracy: 0.9740 - val_loss: 0.1131\n",
      "Epoch 490/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9915 - loss: 0.0313\n",
      "Epoch 490: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9915 - loss: 0.0313 - val_accuracy: 0.9714 - val_loss: 0.1134\n",
      "Epoch 491/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9904 - loss: 0.0313\n",
      "Epoch 491: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9904 - loss: 0.0313 - val_accuracy: 0.9766 - val_loss: 0.1130\n",
      "Epoch 492/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9922 - loss: 0.0313\n",
      "Epoch 492: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9922 - loss: 0.0313 - val_accuracy: 0.9714 - val_loss: 0.1137\n",
      "Epoch 493/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0313\n",
      "Epoch 493: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9900 - loss: 0.0313 - val_accuracy: 0.9757 - val_loss: 0.1128\n",
      "Epoch 494/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0314\n",
      "Epoch 494: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9924 - loss: 0.0314 - val_accuracy: 0.9722 - val_loss: 0.1139\n",
      "Epoch 495/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0316\n",
      "Epoch 495: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9902 - loss: 0.0316 - val_accuracy: 0.9731 - val_loss: 0.1131\n",
      "Epoch 496/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0317\n",
      "Epoch 496: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9924 - loss: 0.0317 - val_accuracy: 0.9722 - val_loss: 0.1150\n",
      "Epoch 497/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9898 - loss: 0.0319\n",
      "Epoch 497: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9898 - loss: 0.0319 - val_accuracy: 0.9731 - val_loss: 0.1134\n",
      "Epoch 498/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0319\n",
      "Epoch 498: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9924 - loss: 0.0319 - val_accuracy: 0.9731 - val_loss: 0.1155\n",
      "Epoch 499/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9891 - loss: 0.0319\n",
      "Epoch 499: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9891 - loss: 0.0319 - val_accuracy: 0.9740 - val_loss: 0.1134\n",
      "Epoch 500/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0319\n",
      "Epoch 500: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9924 - loss: 0.0319 - val_accuracy: 0.9722 - val_loss: 0.1150\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "num_classes = 3\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=f'MLP.keras', \n",
    "        verbose=1, \n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True, \n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=8192, verbose=1, validation_split=0.2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4329cbf6-b303-465d-b0b0-1d318d3fb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"MLP.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e136186b-e082-40d5-97d0-f00d41c89b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 01:03:10.789191: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:10.855512: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:11.001752: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 408 bytes spill stores, 444 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/60\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:26\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.2290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 01:03:12.136874: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:12.137791: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 392 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:12.154719: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9739 - loss: 0.0901\n",
      "Test results - Loss: 0.10442788153886795 - Accuracy: 97.34%\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad230d3-8fed-4733-a88c-fe3bafe8f866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step \n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "367e1c58-f9a1-4963-9f7d-c60abbe42580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_test_pred = to_categorical(np.argmax(model.predict(X_test), axis=1), num_classes=3)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e75d39d-2bd0-428a-bd6f-7855d15e6932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.98522459, 0.75      , 0.93413174])\n",
      "specificity=array([0.88986784, 0.99246907, 0.99315068])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7d8f6cfa1040>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"negative\", \"hyperthyroid\", \"hypothyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21033cf1-491b-4cec-add6-bef52f928f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
