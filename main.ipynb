{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cea473-23b3-4806-8a99-0c1e59dc8dea",
   "metadata": {},
   "source": [
    "# Thyroid Disease Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c385569-6843-4602-bccd-8efdb9bf0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "df = pd.read_csv(\"thyroidDF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14924645-c9c9-49bc-b268-80d1f13ed085",
   "metadata": {},
   "source": [
    "## Data feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93fd52-25c5-4241-9086-cefdf7707859",
   "metadata": {},
   "source": [
    "### Initital columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb966f50-3ed5-4c90-b4fd-362f1287db2b",
   "metadata": {},
   "source": [
    "These are all the columns present in the dataset:\n",
    "\n",
    "1. age - age of the patient (int)\n",
    "2. sex - sex patient identifies (str)\n",
    "3. on_thyroxine - whether patient is on thyroxine (bool)\n",
    "4. query on thyroxine - *whether patient is on thyroxine (bool)\n",
    "5. on antithyroid meds - whether patient is on antithyroid meds (bool)\n",
    "6. sick - whether patient is sick (bool)\n",
    "7. pregnant - whether patient is pregnant (bool)\n",
    "8. thyroid_surgery - whether patient has undergone thyroid surgery (bool)\n",
    "9. I131_treatment - whether patient is undergoing I131 treatment (bool)\n",
    "10. query_hypothyroid - whether patient believes they have hypothyroid (bool)\n",
    "11. query_hyperthyroid - whether patient believes they have hyperthyroid (bool)\n",
    "12. lithium - whether patient * lithium (bool)\n",
    "13. goitre - whether patient has goitre (bool)\n",
    "14. tumor - whether patient has tumor (bool)\n",
    "15. hypopituitary - whether patient * hyperpituitary gland (float)\n",
    "16. psych - whether patient * psych (bool)\n",
    "17. TSH_measured - whether TSH was measured in the blood (bool)\n",
    "18. TSH - TSH level in blood from lab work (float)\n",
    "19. T3_measured - whether T3 was measured in the blood (bool)\n",
    "20. T3 - T3 level in blood from lab work (float)\n",
    "21. TT4_measured - whether TT4 was measured in the blood (bool)\n",
    "22. TT4 - TT4 level in blood from lab work (float)\n",
    "23. T4U_measured - whether T4U was measured in the blood (bool)\n",
    "24. T4U - T4U level in blood from lab work (float)\n",
    "25. FTI_measured - whether FTI was measured in the blood (bool)\n",
    "26. FTI - FTI level in blood from lab work (float)\n",
    "27. TBG_measured - whether TBG was measured in the blood (bool)\n",
    "28. TBG - TBG level in blood from lab work (float)\n",
    "29. referral_source - (str)\n",
    "30. target - hyperthyroidism medical diagnosis (str)\n",
    "31. patient_id - unique id of the patient (str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5bbd0-7678-4447-872d-18d8a0ac7f1e",
   "metadata": {},
   "source": [
    "### Targets from dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13899f3a-f706-4023-aaea-5a787d2cc06c",
   "metadata": {},
   "source": [
    "This are the initial target defined in df:\n",
    "1. hyperthyroid conditions:\n",
    "- A   hyperthyroid\n",
    "- B   T3 toxic\n",
    "- C   toxic goitre\n",
    "- D   secondary toxic\n",
    "  \n",
    "2. hypothyroid conditions:\n",
    "- E   hypothyroid\n",
    "- F   primary hypothyroid\n",
    "- G   compensated hypothyroid\n",
    "- H   secondary hypothyroid\n",
    "\n",
    "3. binding protein:\n",
    "- I   increased binding protein\n",
    "- J   decreased binding protein\n",
    "\n",
    "4. general health:\n",
    "- K   concurrent non-thyroidal illness\n",
    "\n",
    "5. replacement therapy:\n",
    "- L   consistent with replacement therapy\n",
    "- M   underreplaced\n",
    "- N   overreplaced\n",
    "\n",
    "6. antithyroid treatment:\n",
    "- O   antithyroid drugs\n",
    "- P   I131 treatment\n",
    "- Q   surgery\n",
    "\n",
    "7. miscellaneous:\n",
    "- R   discordant assay results\n",
    "- S   elevated TBG\n",
    "- T   elevated thyroid hormones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbb3fb6-96c4-47f2-acbf-4e498ed355d6",
   "metadata": {},
   "source": [
    "Since there is little data we are focusing on distinguishing patients with <b>hyperthyroid</b>, <b>hypothyroid</b> condtions or <b>none of these</b>.\n",
    "\n",
    "There needs to be conversion to new <b>targets</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeb2bb81-0b12-4693-835e-621d2d7320c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_targets = [\"-\"]\n",
    "hyperthyroid_targets = [\"A\", \"B\", \"C\", \"D\"]\n",
    "hypothyroid_targets = [\"E\", \"F\", \"G\", \"H\"]\n",
    "\n",
    "def convert_to_new_target(old_target):\n",
    "    if any([x in old_target for x in negative_targets]):\n",
    "        return \"negative\"\n",
    "    if any([x in old_target for x in hyperthyroid_targets]):\n",
    "        return \"hyperthyroid\"\n",
    "    if any([x in old_target for x in hypothyroid_targets]):\n",
    "        return \"hypothyroid\"\n",
    "    return None\n",
    "\n",
    "df[\"target\"] = df[\"target\"].map(convert_to_new_target)\n",
    "df.dropna(subset = ['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2541212b-7851-4b5f-a7c5-c65753d72824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "negative        6771\n",
       "hypothyroid      667\n",
       "hyperthyroid     241\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caad193-bfc0-4474-a8d0-0461d1405b38",
   "metadata": {},
   "source": [
    "### Removing redundant columns\n",
    "Columns that have artifial values and information about measurement that is repesented by empty value in associated column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfd29c10-ed57-4a5d-8769-9e551f98c65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_columns = [\n",
    "    'TSH_measured',\n",
    "    'T3_measured',\n",
    "    'TT4_measured',\n",
    "    'T4U_measured',\n",
    "    'FTI_measured',\n",
    "    'TBG_measured',\n",
    "    'referral_source',\n",
    "    'patient_id',\n",
    "]\n",
    "df.drop(rem_columns, axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9134cf6a-0053-4e4b-b0e8-be01cc9d616d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'on_thyroxine', 'query_on_thyroxine',\n",
       "       'on_antithyroid_meds', 'sick', 'pregnant', 'thyroid_surgery',\n",
       "       'I131_treatment', 'query_hypothyroid', 'query_hyperthyroid', 'lithium',\n",
       "       'goitre', 'tumor', 'hypopituitary', 'psych', 'TSH', 'T3', 'TT4', 'T4U',\n",
       "       'FTI', 'TBG', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20fd66-48a9-4206-8bba-fae71d29380a",
   "metadata": {},
   "source": [
    "### Data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f4cfde7-00b7-41fd-bc67-14f6fff47fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>7679.0</td>\n",
       "      <td>77.640839</td>\n",
       "      <td>1293.909497</td>\n",
       "      <td>1.000</td>\n",
       "      <td>37.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>65526.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>6955.0</td>\n",
       "      <td>5.500684</td>\n",
       "      <td>25.978304</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>530.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>5470.0</td>\n",
       "      <td>2.010773</td>\n",
       "      <td>0.818738</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>7325.0</td>\n",
       "      <td>105.497565</td>\n",
       "      <td>33.125317</td>\n",
       "      <td>2.000</td>\n",
       "      <td>87.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>6998.0</td>\n",
       "      <td>0.967297</td>\n",
       "      <td>0.164388</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>7005.0</td>\n",
       "      <td>110.941312</td>\n",
       "      <td>37.167537</td>\n",
       "      <td>1.400</td>\n",
       "      <td>93.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>839.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>259.0</td>\n",
       "      <td>22.955019</td>\n",
       "      <td>6.088392</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        mean          std    min    25%     50%     75%       max\n",
       "age  7679.0   77.640839  1293.909497  1.000  37.00   55.00   67.00  65526.00\n",
       "TSH  6955.0    5.500684    25.978304  0.005   0.55    1.40    2.70    530.00\n",
       "T3   5470.0    2.010773     0.818738  0.050   1.60    1.90    2.30     18.00\n",
       "TT4  7325.0  105.497565    33.125317  2.000  87.00  103.00  121.00    430.00\n",
       "T4U  6998.0    0.967297     0.164388  0.190   0.87    0.96    1.06      2.12\n",
       "FTI  7005.0  110.941312    37.167537  1.400  93.00  108.00  125.00    839.00\n",
       "TBG   259.0   22.955019     6.088392  0.100  20.00   23.00   27.00     45.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc6868-0ae2-422a-8df8-c8e64c32be33",
   "metadata": {},
   "source": [
    "## Fixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2922f-73df-4a42-a125-d2e63721715a",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc96b4-814a-41fc-b129-437192c2e8b0",
   "metadata": {},
   "source": [
    "Without deep analysis we can see there is a problem with age column since max is 65526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f863b88-f898-4225-afd3-c65a093af62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7355</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7356</th>\n",
       "      <td>97</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>455</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5710</th>\n",
       "      <td>65511</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6392</th>\n",
       "      <td>65512</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8105</th>\n",
       "      <td>65526</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    target\n",
       "7355     97  negative\n",
       "790      97  negative\n",
       "7356     97  negative\n",
       "2976    455  negative\n",
       "5710  65511  negative\n",
       "6392  65512  negative\n",
       "8105  65526  negative"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"age\")[-7:][[\"age\", \"target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a99ea855-c2c3-43a7-8a25-b647f452f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['age'] < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18fb72ec-c6ca-4ae1-88c0-501baf11c3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>7675.0</td>\n",
       "      <td>52.013029</td>\n",
       "      <td>18.654684</td>\n",
       "      <td>1.000</td>\n",
       "      <td>37.00</td>\n",
       "      <td>55.00</td>\n",
       "      <td>67.00</td>\n",
       "      <td>97.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>6951.0</td>\n",
       "      <td>5.503416</td>\n",
       "      <td>25.985525</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.40</td>\n",
       "      <td>2.70</td>\n",
       "      <td>530.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>5467.0</td>\n",
       "      <td>2.010633</td>\n",
       "      <td>0.818893</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>7321.0</td>\n",
       "      <td>105.490324</td>\n",
       "      <td>33.132392</td>\n",
       "      <td>2.000</td>\n",
       "      <td>87.00</td>\n",
       "      <td>103.00</td>\n",
       "      <td>121.00</td>\n",
       "      <td>430.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>6994.0</td>\n",
       "      <td>0.967268</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.06</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>7001.0</td>\n",
       "      <td>110.937565</td>\n",
       "      <td>37.176408</td>\n",
       "      <td>1.400</td>\n",
       "      <td>93.00</td>\n",
       "      <td>108.00</td>\n",
       "      <td>125.00</td>\n",
       "      <td>839.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>259.0</td>\n",
       "      <td>22.955019</td>\n",
       "      <td>6.088392</td>\n",
       "      <td>0.100</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>45.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count        mean        std    min    25%     50%     75%     max\n",
       "age  7675.0   52.013029  18.654684  1.000  37.00   55.00   67.00   97.00\n",
       "TSH  6951.0    5.503416  25.985525  0.005   0.55    1.40    2.70  530.00\n",
       "T3   5467.0    2.010633   0.818893  0.050   1.60    1.90    2.30   18.00\n",
       "TT4  7321.0  105.490324  33.132392  2.000  87.00  103.00  121.00  430.00\n",
       "T4U  6994.0    0.967268   0.164410  0.190   0.87    0.96    1.06    2.12\n",
       "FTI  7001.0  110.937565  37.176408  1.400  93.00  108.00  125.00  839.00\n",
       "TBG   259.0   22.955019   6.088392  0.100  20.00   23.00   27.00   45.00"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19658207-fff5-4a0b-a2f7-eccf41fb20df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>7421</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>5006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on_thyroxine</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>6825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on_antithyroid_meds</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sick</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnant</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I131_treatment</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_hyperthyroid</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lithium</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goitre</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tumor</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypopituitary</th>\n",
       "      <td>7675</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>7675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psych</th>\n",
       "      <td>7675</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>7294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>7675</td>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>6767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count unique       top  freq\n",
       "sex                  7421      2         F  5006\n",
       "on_thyroxine         7675      2         f  6825\n",
       "query_on_thyroxine   7675      2         f  7552\n",
       "on_antithyroid_meds  7675      2         f  7583\n",
       "sick                 7675      2         f  7388\n",
       "pregnant             7675      2         f  7635\n",
       "thyroid_surgery      7675      2         f  7569\n",
       "I131_treatment       7675      2         f  7534\n",
       "query_hypothyroid    7675      2         f  7155\n",
       "query_hyperthyroid   7675      2         f  7115\n",
       "lithium              7675      2         f  7589\n",
       "goitre               7675      2         f  7601\n",
       "tumor                7675      2         f  7472\n",
       "hypopituitary        7675      1         f  7675\n",
       "psych                7675      2         f  7294\n",
       "target               7675      3  negative  6767"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a74c71-fcda-4aff-8dc9-9be304cf39c8",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d218ad32-06c9-41dc-a255-5437732395c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>3.309446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>9.433225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>28.768730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>4.612378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>8.872964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>8.781759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TBG</th>\n",
       "      <td>96.625407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing Values %\n",
       "sex          3.309446\n",
       "TSH          9.433225\n",
       "T3          28.768730\n",
       "TT4          4.612378\n",
       "T4U          8.872964\n",
       "FTI          8.781759\n",
       "TBG         96.625407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847d42b-7e10-4038-81dc-1ff49bc04e53",
   "metadata": {},
   "source": [
    "Drop TBG since almost whole column is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fef1622c-4a43-4409-89dd-0f76f739b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"TBG\"], axis=1 ,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6206922d-4bc7-4af4-960e-6771ab272313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>on_thyroxine</th>\n",
       "      <th>query_on_thyroxine</th>\n",
       "      <th>on_antithyroid_meds</th>\n",
       "      <th>sick</th>\n",
       "      <th>pregnant</th>\n",
       "      <th>thyroid_surgery</th>\n",
       "      <th>I131_treatment</th>\n",
       "      <th>query_hypothyroid</th>\n",
       "      <th>...</th>\n",
       "      <th>goitre</th>\n",
       "      <th>tumor</th>\n",
       "      <th>hypopituitary</th>\n",
       "      <th>psych</th>\n",
       "      <th>TSH</th>\n",
       "      <th>T3</th>\n",
       "      <th>TT4</th>\n",
       "      <th>T4U</th>\n",
       "      <th>FTI</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.9</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9166</th>\n",
       "      <td>70</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>119.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9167</th>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.83</td>\n",
       "      <td>77.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9168</th>\n",
       "      <td>22</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>99.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9170</th>\n",
       "      <td>47</td>\n",
       "      <td>F</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.85</td>\n",
       "      <td>88.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9171</th>\n",
       "      <td>31</td>\n",
       "      <td>M</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>65.0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7675 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age sex on_thyroxine query_on_thyroxine on_antithyroid_meds sick  \\\n",
       "0      29   F            f                  f                   f    f   \n",
       "1      29   F            f                  f                   f    f   \n",
       "2      41   F            f                  f                   f    f   \n",
       "3      36   F            f                  f                   f    f   \n",
       "5      60   F            f                  f                   f    f   \n",
       "...   ...  ..          ...                ...                 ...  ...   \n",
       "9166   70   F            f                  f                   f    f   \n",
       "9167   56   M            f                  f                   f    f   \n",
       "9168   22   M            f                  f                   f    f   \n",
       "9170   47   F            f                  f                   f    f   \n",
       "9171   31   M            f                  f                   f    f   \n",
       "\n",
       "     pregnant thyroid_surgery I131_treatment query_hypothyroid  ... goitre  \\\n",
       "0           f               f              f                 t  ...      f   \n",
       "1           f               f              f                 f  ...      f   \n",
       "2           f               f              f                 f  ...      f   \n",
       "3           f               f              f                 f  ...      f   \n",
       "5           f               f              f                 f  ...      f   \n",
       "...       ...             ...            ...               ...  ...    ...   \n",
       "9166        f               f              f                 f  ...      f   \n",
       "9167        f               f              f                 f  ...      f   \n",
       "9168        f               f              f                 f  ...      f   \n",
       "9170        f               f              f                 f  ...      f   \n",
       "9171        f               f              f                 t  ...      f   \n",
       "\n",
       "     tumor hypopituitary psych  TSH   T3    TT4   T4U    FTI    target  \n",
       "0        f             f     f  0.3  NaN    NaN   NaN    NaN  negative  \n",
       "1        f             f     f  1.6  1.9  128.0   NaN    NaN  negative  \n",
       "2        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "3        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "5        f             f     f  NaN  NaN    NaN   NaN    NaN  negative  \n",
       "...    ...           ...   ...  ...  ...    ...   ...    ...       ...  \n",
       "9166     f             f     f  NaN  NaN   88.0  0.74  119.0  negative  \n",
       "9167     f             f     f  NaN  NaN   64.0  0.83   77.0  negative  \n",
       "9168     f             f     f  NaN  NaN   91.0  0.92   99.0  negative  \n",
       "9170     f             f     f  NaN  NaN   75.0  0.85   88.0  negative  \n",
       "9171     f             f     f  NaN  NaN   66.0  1.02   65.0  negative  \n",
       "\n",
       "[7675 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a4cb293-178a-44da-a0d7-deb4454fcb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('f', 0, inplace=True)\n",
    "df.replace('t', 1, inplace=True)\n",
    "df.replace('M', 0, inplace=True)\n",
    "df.replace('F', 1, inplace=True)\n",
    "\n",
    "target_map = {\n",
    "    \"negative\": 0,\n",
    "    \"hyperthyroid\": 1,\n",
    "    \"hypothyroid\": 2,\n",
    "}\n",
    "\n",
    "df[\"target\"] = df[\"target\"].map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23ab1b3c-c3bd-47fc-b926-5f5150054674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>3.309446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TSH</th>\n",
       "      <td>9.433225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>28.768730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TT4</th>\n",
       "      <td>4.612378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4U</th>\n",
       "      <td>8.872964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FTI</th>\n",
       "      <td>8.781759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Missing Values %\n",
       "sex          3.309446\n",
       "TSH          9.433225\n",
       "T3          28.768730\n",
       "TT4          4.612378\n",
       "T4U          8.872964\n",
       "FTI          8.781759"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f162555-3d09-40a3-a55e-ec64ff80b9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "\n",
    "seed(1337)\n",
    "\n",
    "def get_impute_value(df, col, target):\n",
    "    match col:\n",
    "        case \"age\" | \"TSH\" | \"T3\" | \"TT4\" | \"T4U\" | \"FTI\" | \"target\":\n",
    "            median_for_target = df.loc[df['target'] == target][col].median()\n",
    "            return median_for_target\n",
    "        case _:\n",
    "            prob = df.loc[df['target'] == target][col].value_counts(normalize=True)\n",
    "            return 1 if random() < prob[1] else 0\n",
    "\n",
    "def imputed_df(df):\n",
    "    df = df.copy()\n",
    "    null_cols = df.columns[df.isnull().any()]\n",
    "    for null_col in null_cols:\n",
    "        print(f\"Imputing column: {null_col}\")\n",
    "        df[null_col] = df.apply(\n",
    "            lambda row: get_impute_value(df, null_col, row[\"target\"]) if pd.isnull(row[null_col]) else row[null_col],\n",
    "            axis=1\n",
    "        )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e295ad91-a7ea-4534-a063-7990484a1a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing column: sex\n",
      "Imputing column: TSH\n",
      "Imputing column: T3\n",
      "Imputing column: TT4\n",
      "Imputing column: T4U\n",
      "Imputing column: FTI\n"
     ]
    }
   ],
   "source": [
    "df = imputed_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1698dbab-43e0-4913-9918-f945b900a95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Values %]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_cols = df.columns[df.isnull().any()]\n",
    "nulls_df = df.loc[:, null_cols]\n",
    "nulls_df = pd.DataFrame(nulls_df.isna().sum()/df.shape[0]*100)\n",
    "nulls_df = nulls_df.rename(columns={0: 'Missing Values %'})\n",
    "nulls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93f2a5-ab65-4b94-a54a-bbe92c0256da",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f47a658-a265-45d2-acbc-94388dd4da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('target', axis=1).copy()\n",
    "y = df['target'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be826f-4d8a-4bdb-bd68-50da18acbbb6",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "404616de-5fee-4f08-ad06-f20a77ae4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63a0c6ef-42e4-483d-b4fa-3d5a440cc6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874934861907243"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "430edea8-1b14-4a31-aff8-313c8a55de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.9929078 , 0.86666667, 0.9760479 ])\n",
      "specificity=array([0.94713656, 0.99462076, 0.99885845])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7d8f8d87fda0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"negative\", \"hyperthyroid\", \"hypothyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "220fb68f-2eed-4c7b-b04e-05cb84cce3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9875609529413497)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test , y_test_pred,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9465832-9628-415e-a0a2-c86efba0c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"decisiontree.bin\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d03c4e-0124-494e-b108-4e02d91f81ab",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7b9a3f-3fff-42da-9f17-2582e13031da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier(5)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9e9df1f-33c0-4e3b-a16d-382a37af465b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948931735278791"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75e375ef-61ea-49b7-a336-769422f27550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.99349882, 0.75      , 0.56886228])\n",
      "specificity=array([0.61674009, 0.99623453, 0.99771689])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7d8f8c5c1d90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"negative\", \"hyperthyroid\", \"hypothyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ac9b1bd-65da-4d7a-9d23-cc90d08acbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9440210008223214)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(y_test , y_test_pred,average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd860e01-556f-4e48-bd3a-cc9c856cd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"knn.bin\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781d8ba-bf09-416a-9e5a-522b39bc470b",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f40de43-e8d0-4f5d-8632-26ca594d917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5eb77b8-69bb-4ace-a95e-d434d25ef311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 01:02:25.334946: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-29 01:02:25.345309: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732838545.357271  562292 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732838545.360815  562292 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-29 01:02:25.374408: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/szymon/hackathon2025/repo/venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1732838547.957807  562292 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6258 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732838549.084847  562426 service.cc:148] XLA service 0x7d8d4c008cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732838549.084874  562426 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-11-29 01:02:29.101734: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1732838549.185255  562426 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2024-11-29 01:02:29.770444: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 104 bytes spill stores, 124 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.814126: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.840054: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.888334: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 300 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.929197: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 44 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.943212: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 264 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:29.994772: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188_0', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.176390: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 824 bytes spill stores, 544 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.233230: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_312', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.260615: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.356037: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_195', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.431749: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.447590: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 284 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.646611: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352_0', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.653714: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_181', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:30.931460: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 276 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.020537: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.230165: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 200 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.354748: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 76 bytes spill stores, 76 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.518648: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 2060 bytes spill stores, 1928 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.585083: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350_0', 736 bytes spill stores, 664 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:31.767370: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.188325: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.271433: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 40 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.278288: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.289724: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.356172: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 1412 bytes spill stores, 1300 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.365949: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.398318: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_188', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.411269: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352_0', 664 bytes spill stores, 664 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.414151: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_350', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.527604: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_352', 928 bytes spill stores, 932 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:32.609170: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_354', 4480 bytes spill stores, 4476 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.0799 - loss: 6.6570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732838554.033496  562426 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-11-29 01:02:34.760038: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:34.864461: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 272 bytes spill stores, 284 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:34.910714: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:34.972888: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.032849: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 256 bytes spill stores, 256 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.066584: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.137740: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49_0', 784 bytes spill stores, 736 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.271617: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56_0', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.389964: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 248 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.410293: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_63', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.597168: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.640705: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 280 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2024-11-29 01:02:35.665293: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 1304 bytes spill stores, 1444 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.88021, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.0799 - loss: 6.6570 - val_accuracy: 0.8802 - val_loss: 2.0003\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.8842 - loss: 2.1206\n",
      "Epoch 2: val_accuracy improved from 0.88021 to 0.89670, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8842 - loss: 2.1206 - val_accuracy: 0.8967 - val_loss: 1.5516\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8983 - loss: 1.6150\n",
      "Epoch 3: val_accuracy improved from 0.89670 to 0.90712, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8983 - loss: 1.6150 - val_accuracy: 0.9071 - val_loss: 0.8478\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9029 - loss: 0.8710\n",
      "Epoch 4: val_accuracy did not improve from 0.90712\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9029 - loss: 0.8710 - val_accuracy: 0.0599 - val_loss: 3.0437\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0606 - loss: 3.0357\n",
      "Epoch 5: val_accuracy improved from 0.90712 to 0.90885, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.0606 - loss: 3.0357 - val_accuracy: 0.9089 - val_loss: 0.8125\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9118 - loss: 0.8669\n",
      "Epoch 6: val_accuracy improved from 0.90885 to 0.90972, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.9118 - loss: 0.8669 - val_accuracy: 0.9097 - val_loss: 1.4291\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9094 - loss: 1.5379\n",
      "Epoch 7: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9094 - loss: 1.5379 - val_accuracy: 0.9097 - val_loss: 1.7824\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9114 - loss: 1.9324\n",
      "Epoch 8: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9114 - loss: 1.9324 - val_accuracy: 0.9089 - val_loss: 1.9151\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9129 - loss: 2.0953\n",
      "Epoch 9: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9129 - loss: 2.0953 - val_accuracy: 0.9080 - val_loss: 1.9481\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9075 - loss: 2.1417\n",
      "Epoch 10: val_accuracy did not improve from 0.90972\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9075 - loss: 2.1417 - val_accuracy: 0.8290 - val_loss: 2.1420\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8388 - loss: 2.3296\n",
      "Epoch 11: val_accuracy improved from 0.90972 to 0.91840, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.8388 - loss: 2.3296 - val_accuracy: 0.9184 - val_loss: 1.9604\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9192 - loss: 2.1612\n",
      "Epoch 12: val_accuracy did not improve from 0.91840\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9192 - loss: 2.1612 - val_accuracy: 0.9123 - val_loss: 1.9924\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9138 - loss: 2.1881\n",
      "Epoch 13: val_accuracy did not improve from 0.91840\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9138 - loss: 2.1881 - val_accuracy: 0.9141 - val_loss: 1.9102\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9155 - loss: 2.0979\n",
      "Epoch 14: val_accuracy did not improve from 0.91840\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9155 - loss: 2.0979 - val_accuracy: 0.9184 - val_loss: 1.7443\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9201 - loss: 1.9211\n",
      "Epoch 15: val_accuracy improved from 0.91840 to 0.92622, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.9201 - loss: 1.9211 - val_accuracy: 0.9262 - val_loss: 1.6204\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9266 - loss: 1.7792\n",
      "Epoch 16: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.9266 - loss: 1.7792 - val_accuracy: 0.9253 - val_loss: 1.4952\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9294 - loss: 1.6350\n",
      "Epoch 17: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9294 - loss: 1.6350 - val_accuracy: 0.9184 - val_loss: 1.2585\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9216 - loss: 1.3796\n",
      "Epoch 18: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9216 - loss: 1.3796 - val_accuracy: 0.9167 - val_loss: 1.0969\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9185 - loss: 1.1934\n",
      "Epoch 19: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9185 - loss: 1.1934 - val_accuracy: 0.9184 - val_loss: 0.8727\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9192 - loss: 0.9372\n",
      "Epoch 20: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9192 - loss: 0.9372 - val_accuracy: 0.9201 - val_loss: 0.5785\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9231 - loss: 0.6066\n",
      "Epoch 21: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9231 - loss: 0.6066 - val_accuracy: 0.9106 - val_loss: 0.3867\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9185 - loss: 0.3765\n",
      "Epoch 22: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9185 - loss: 0.3765 - val_accuracy: 0.7240 - val_loss: 0.9220\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7246 - loss: 0.9137\n",
      "Epoch 23: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.7246 - loss: 0.9137 - val_accuracy: 0.7179 - val_loss: 0.8697\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7168 - loss: 0.8594\n",
      "Epoch 24: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7168 - loss: 0.8594 - val_accuracy: 0.8724 - val_loss: 0.4560\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8725 - loss: 0.4389\n",
      "Epoch 25: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.8725 - loss: 0.4389 - val_accuracy: 0.9097 - val_loss: 0.4757\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9177 - loss: 0.4760\n",
      "Epoch 26: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9177 - loss: 0.4760 - val_accuracy: 0.9201 - val_loss: 0.5548\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9207 - loss: 0.5725\n",
      "Epoch 27: val_accuracy did not improve from 0.92622\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9207 - loss: 0.5725 - val_accuracy: 0.9227 - val_loss: 0.5810\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9242 - loss: 0.6104\n",
      "Epoch 28: val_accuracy improved from 0.92622 to 0.92795, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9242 - loss: 0.6104 - val_accuracy: 0.9280 - val_loss: 0.5596\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9314 - loss: 0.5957\n",
      "Epoch 29: val_accuracy did not improve from 0.92795\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9314 - loss: 0.5957 - val_accuracy: 0.9201 - val_loss: 0.6040\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9181 - loss: 0.6358\n",
      "Epoch 30: val_accuracy improved from 0.92795 to 0.93490, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9181 - loss: 0.6358 - val_accuracy: 0.9349 - val_loss: 0.5363\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9351 - loss: 0.5654\n",
      "Epoch 31: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9351 - loss: 0.5654 - val_accuracy: 0.9332 - val_loss: 0.4406\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9331 - loss: 0.4628\n",
      "Epoch 32: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9331 - loss: 0.4628 - val_accuracy: 0.9245 - val_loss: 0.3827\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9268 - loss: 0.3929\n",
      "Epoch 33: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9268 - loss: 0.3929 - val_accuracy: 0.9106 - val_loss: 0.3242\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9207 - loss: 0.3225\n",
      "Epoch 34: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9207 - loss: 0.3225 - val_accuracy: 0.8602 - val_loss: 0.3724\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8664 - loss: 0.3623\n",
      "Epoch 35: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8664 - loss: 0.3623 - val_accuracy: 0.7995 - val_loss: 0.4850\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.8019 - loss: 0.4755\n",
      "Epoch 36: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step - accuracy: 0.8019 - loss: 0.4755 - val_accuracy: 0.8602 - val_loss: 0.3706\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8608 - loss: 0.3630\n",
      "Epoch 37: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8608 - loss: 0.3630 - val_accuracy: 0.9089 - val_loss: 0.2929\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9181 - loss: 0.2930\n",
      "Epoch 38: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9181 - loss: 0.2930 - val_accuracy: 0.9219 - val_loss: 0.3101\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9285 - loss: 0.3191\n",
      "Epoch 39: val_accuracy did not improve from 0.93490\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9285 - loss: 0.3191 - val_accuracy: 0.9314 - val_loss: 0.3308\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9348 - loss: 0.3454\n",
      "Epoch 40: val_accuracy improved from 0.93490 to 0.94878, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9348 - loss: 0.3454 - val_accuracy: 0.9488 - val_loss: 0.3498\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9429 - loss: 0.3659\n",
      "Epoch 41: val_accuracy improved from 0.94878 to 0.95052, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9429 - loss: 0.3659 - val_accuracy: 0.9505 - val_loss: 0.3430\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9461 - loss: 0.3578\n",
      "Epoch 42: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9461 - loss: 0.3578 - val_accuracy: 0.9401 - val_loss: 0.2999\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9398 - loss: 0.3123\n",
      "Epoch 43: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9398 - loss: 0.3123 - val_accuracy: 0.9245 - val_loss: 0.2687\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9329 - loss: 0.2772\n",
      "Epoch 44: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9329 - loss: 0.2772 - val_accuracy: 0.9089 - val_loss: 0.2685\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9168 - loss: 0.2719\n",
      "Epoch 45: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9168 - loss: 0.2719 - val_accuracy: 0.8828 - val_loss: 0.3117\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8810 - loss: 0.3118\n",
      "Epoch 46: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8810 - loss: 0.3118 - val_accuracy: 0.8854 - val_loss: 0.3069\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8842 - loss: 0.3078\n",
      "Epoch 47: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8842 - loss: 0.3078 - val_accuracy: 0.9115 - val_loss: 0.2607\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9194 - loss: 0.2658\n",
      "Epoch 48: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9194 - loss: 0.2658 - val_accuracy: 0.9280 - val_loss: 0.2514\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9348 - loss: 0.2615\n",
      "Epoch 49: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9348 - loss: 0.2615 - val_accuracy: 0.9401 - val_loss: 0.2600\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9416 - loss: 0.2716\n",
      "Epoch 50: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9416 - loss: 0.2716 - val_accuracy: 0.9505 - val_loss: 0.2738\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9457 - loss: 0.2843\n",
      "Epoch 51: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9457 - loss: 0.2843 - val_accuracy: 0.9479 - val_loss: 0.2670\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9446 - loss: 0.2757\n",
      "Epoch 52: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9446 - loss: 0.2757 - val_accuracy: 0.9418 - val_loss: 0.2439\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9416 - loss: 0.2508\n",
      "Epoch 53: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9416 - loss: 0.2508 - val_accuracy: 0.9332 - val_loss: 0.2400\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9370 - loss: 0.2446\n",
      "Epoch 54: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9370 - loss: 0.2446 - val_accuracy: 0.9201 - val_loss: 0.2550\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9231 - loss: 0.2577\n",
      "Epoch 55: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9231 - loss: 0.2577 - val_accuracy: 0.9158 - val_loss: 0.2607\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9201 - loss: 0.2631\n",
      "Epoch 56: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9201 - loss: 0.2631 - val_accuracy: 0.9236 - val_loss: 0.2446\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9325 - loss: 0.2481\n",
      "Epoch 57: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9325 - loss: 0.2481 - val_accuracy: 0.9358 - val_loss: 0.2332\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9390 - loss: 0.2381\n",
      "Epoch 58: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9390 - loss: 0.2381 - val_accuracy: 0.9462 - val_loss: 0.2353\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9446 - loss: 0.2407\n",
      "Epoch 59: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9446 - loss: 0.2407 - val_accuracy: 0.9488 - val_loss: 0.2443\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9479 - loss: 0.2492\n",
      "Epoch 60: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9479 - loss: 0.2492 - val_accuracy: 0.9479 - val_loss: 0.2380\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2427\n",
      "Epoch 61: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9470 - loss: 0.2427 - val_accuracy: 0.9427 - val_loss: 0.2286\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9424 - loss: 0.2328\n",
      "Epoch 62: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9424 - loss: 0.2328 - val_accuracy: 0.9392 - val_loss: 0.2286\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9396 - loss: 0.2323\n",
      "Epoch 63: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9396 - loss: 0.2323 - val_accuracy: 0.9349 - val_loss: 0.2348\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9372 - loss: 0.2377\n",
      "Epoch 64: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9372 - loss: 0.2377 - val_accuracy: 0.9366 - val_loss: 0.2333\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9387 - loss: 0.2362\n",
      "Epoch 65: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9387 - loss: 0.2362 - val_accuracy: 0.9410 - val_loss: 0.2244\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9411 - loss: 0.2283\n",
      "Epoch 66: val_accuracy did not improve from 0.95052\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9411 - loss: 0.2283 - val_accuracy: 0.9453 - val_loss: 0.2220\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9446 - loss: 0.2266\n",
      "Epoch 67: val_accuracy improved from 0.95052 to 0.95139, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9446 - loss: 0.2266 - val_accuracy: 0.9514 - val_loss: 0.2255\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9481 - loss: 0.2302\n",
      "Epoch 68: val_accuracy improved from 0.95139 to 0.95226, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9481 - loss: 0.2302 - val_accuracy: 0.9523 - val_loss: 0.2251\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9492 - loss: 0.2296\n",
      "Epoch 69: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9492 - loss: 0.2296 - val_accuracy: 0.9488 - val_loss: 0.2195\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9468 - loss: 0.2234\n",
      "Epoch 70: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9468 - loss: 0.2234 - val_accuracy: 0.9444 - val_loss: 0.2184\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9427 - loss: 0.2217\n",
      "Epoch 71: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9427 - loss: 0.2217 - val_accuracy: 0.9444 - val_loss: 0.2212\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9424 - loss: 0.2240\n",
      "Epoch 72: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9424 - loss: 0.2240 - val_accuracy: 0.9444 - val_loss: 0.2201\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9422 - loss: 0.2232\n",
      "Epoch 73: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9422 - loss: 0.2232 - val_accuracy: 0.9436 - val_loss: 0.2150\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9429 - loss: 0.2187\n",
      "Epoch 74: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9429 - loss: 0.2187 - val_accuracy: 0.9479 - val_loss: 0.2135\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2176\n",
      "Epoch 75: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9470 - loss: 0.2176 - val_accuracy: 0.9514 - val_loss: 0.2148\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9496 - loss: 0.2186\n",
      "Epoch 76: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9496 - loss: 0.2186 - val_accuracy: 0.9523 - val_loss: 0.2139\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9505 - loss: 0.2173\n",
      "Epoch 77: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9505 - loss: 0.2173 - val_accuracy: 0.9479 - val_loss: 0.2107\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2138\n",
      "Epoch 78: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9470 - loss: 0.2138 - val_accuracy: 0.9453 - val_loss: 0.2104\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9431 - loss: 0.2134\n",
      "Epoch 79: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9431 - loss: 0.2134 - val_accuracy: 0.9436 - val_loss: 0.2112\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9437 - loss: 0.2136\n",
      "Epoch 80: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9437 - loss: 0.2136 - val_accuracy: 0.9444 - val_loss: 0.2097\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9433 - loss: 0.2114\n",
      "Epoch 81: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9433 - loss: 0.2114 - val_accuracy: 0.9479 - val_loss: 0.2076\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9468 - loss: 0.2091\n",
      "Epoch 82: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9468 - loss: 0.2091 - val_accuracy: 0.9523 - val_loss: 0.2079\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9507 - loss: 0.2091\n",
      "Epoch 83: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9507 - loss: 0.2091 - val_accuracy: 0.9523 - val_loss: 0.2073\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9505 - loss: 0.2084\n",
      "Epoch 84: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9505 - loss: 0.2084 - val_accuracy: 0.9505 - val_loss: 0.2052\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9487 - loss: 0.2062\n",
      "Epoch 85: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9487 - loss: 0.2062 - val_accuracy: 0.9470 - val_loss: 0.2044\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9464 - loss: 0.2049\n",
      "Epoch 86: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9464 - loss: 0.2049 - val_accuracy: 0.9462 - val_loss: 0.2047\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9459 - loss: 0.2047\n",
      "Epoch 87: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9459 - loss: 0.2047 - val_accuracy: 0.9470 - val_loss: 0.2032\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.2031\n",
      "Epoch 88: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9470 - loss: 0.2031 - val_accuracy: 0.9488 - val_loss: 0.2012\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9479 - loss: 0.2014\n",
      "Epoch 89: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9479 - loss: 0.2014 - val_accuracy: 0.9523 - val_loss: 0.2006\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9503 - loss: 0.2006\n",
      "Epoch 90: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9503 - loss: 0.2006 - val_accuracy: 0.9523 - val_loss: 0.2001\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9507 - loss: 0.1997\n",
      "Epoch 91: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9507 - loss: 0.1997 - val_accuracy: 0.9497 - val_loss: 0.1990\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9496 - loss: 0.1979\n",
      "Epoch 92: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9496 - loss: 0.1979 - val_accuracy: 0.9488 - val_loss: 0.1980\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9479 - loss: 0.1969\n",
      "Epoch 93: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9479 - loss: 0.1969 - val_accuracy: 0.9497 - val_loss: 0.1965\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9470 - loss: 0.1958\n",
      "Epoch 94: val_accuracy did not improve from 0.95226\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9470 - loss: 0.1958 - val_accuracy: 0.9505 - val_loss: 0.1948\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9487 - loss: 0.1943\n",
      "Epoch 95: val_accuracy improved from 0.95226 to 0.95312, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9487 - loss: 0.1943 - val_accuracy: 0.9531 - val_loss: 0.1939\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9496 - loss: 0.1932\n",
      "Epoch 96: val_accuracy improved from 0.95312 to 0.95486, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9496 - loss: 0.1932 - val_accuracy: 0.9549 - val_loss: 0.1931\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9509 - loss: 0.1920\n",
      "Epoch 97: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9509 - loss: 0.1920 - val_accuracy: 0.9540 - val_loss: 0.1921\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9509 - loss: 0.1905\n",
      "Epoch 98: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9509 - loss: 0.1905 - val_accuracy: 0.9505 - val_loss: 0.1915\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9503 - loss: 0.1891\n",
      "Epoch 99: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9503 - loss: 0.1891 - val_accuracy: 0.9514 - val_loss: 0.1912\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9500 - loss: 0.1879\n",
      "Epoch 100: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9500 - loss: 0.1879 - val_accuracy: 0.9523 - val_loss: 0.1897\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9507 - loss: 0.1862\n",
      "Epoch 101: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9507 - loss: 0.1862 - val_accuracy: 0.9531 - val_loss: 0.1884\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9507 - loss: 0.1849\n",
      "Epoch 102: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9507 - loss: 0.1849 - val_accuracy: 0.9540 - val_loss: 0.1876\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9509 - loss: 0.1835\n",
      "Epoch 103: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9509 - loss: 0.1835 - val_accuracy: 0.9540 - val_loss: 0.1867\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9513 - loss: 0.1820\n",
      "Epoch 104: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9513 - loss: 0.1820 - val_accuracy: 0.9514 - val_loss: 0.1857\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9513 - loss: 0.1806\n",
      "Epoch 105: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9513 - loss: 0.1806 - val_accuracy: 0.9523 - val_loss: 0.1845\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9511 - loss: 0.1792\n",
      "Epoch 106: val_accuracy did not improve from 0.95486\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9511 - loss: 0.1792 - val_accuracy: 0.9540 - val_loss: 0.1834\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.9520 - loss: 0.1776\n",
      "Epoch 107: val_accuracy improved from 0.95486 to 0.95573, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9520 - loss: 0.1776 - val_accuracy: 0.9557 - val_loss: 0.1823\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9524 - loss: 0.1763\n",
      "Epoch 108: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9524 - loss: 0.1763 - val_accuracy: 0.9557 - val_loss: 0.1813\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9520 - loss: 0.1748\n",
      "Epoch 109: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9520 - loss: 0.1748 - val_accuracy: 0.9540 - val_loss: 0.1802\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9516 - loss: 0.1733\n",
      "Epoch 110: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9516 - loss: 0.1733 - val_accuracy: 0.9540 - val_loss: 0.1795\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9516 - loss: 0.1718\n",
      "Epoch 111: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9516 - loss: 0.1718 - val_accuracy: 0.9549 - val_loss: 0.1788\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9524 - loss: 0.1704\n",
      "Epoch 112: val_accuracy did not improve from 0.95573\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9524 - loss: 0.1704 - val_accuracy: 0.9557 - val_loss: 0.1785\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9535 - loss: 0.1689\n",
      "Epoch 113: val_accuracy improved from 0.95573 to 0.95747, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9535 - loss: 0.1689 - val_accuracy: 0.9575 - val_loss: 0.1774\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9537 - loss: 0.1675\n",
      "Epoch 114: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9537 - loss: 0.1675 - val_accuracy: 0.9540 - val_loss: 0.1767\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9529 - loss: 0.1659\n",
      "Epoch 115: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9529 - loss: 0.1659 - val_accuracy: 0.9531 - val_loss: 0.1755\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9529 - loss: 0.1645\n",
      "Epoch 116: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9529 - loss: 0.1645 - val_accuracy: 0.9557 - val_loss: 0.1743\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9540 - loss: 0.1630\n",
      "Epoch 117: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9540 - loss: 0.1630 - val_accuracy: 0.9557 - val_loss: 0.1734\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9553 - loss: 0.1615\n",
      "Epoch 118: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9553 - loss: 0.1615 - val_accuracy: 0.9557 - val_loss: 0.1726\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9553 - loss: 0.1600\n",
      "Epoch 119: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9553 - loss: 0.1600 - val_accuracy: 0.9566 - val_loss: 0.1717\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9559 - loss: 0.1584\n",
      "Epoch 120: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9559 - loss: 0.1584 - val_accuracy: 0.9557 - val_loss: 0.1711\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9559 - loss: 0.1571\n",
      "Epoch 121: val_accuracy did not improve from 0.95747\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9559 - loss: 0.1571 - val_accuracy: 0.9575 - val_loss: 0.1697\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9561 - loss: 0.1557\n",
      "Epoch 122: val_accuracy improved from 0.95747 to 0.95833, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9561 - loss: 0.1557 - val_accuracy: 0.9583 - val_loss: 0.1695\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9563 - loss: 0.1546\n",
      "Epoch 123: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9563 - loss: 0.1546 - val_accuracy: 0.9557 - val_loss: 0.1677\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9553 - loss: 0.1539\n",
      "Epoch 124: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9553 - loss: 0.1539 - val_accuracy: 0.9575 - val_loss: 0.1697\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9589 - loss: 0.1533\n",
      "Epoch 125: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9589 - loss: 0.1533 - val_accuracy: 0.9557 - val_loss: 0.1657\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9555 - loss: 0.1509\n",
      "Epoch 126: val_accuracy did not improve from 0.95833\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9555 - loss: 0.1509 - val_accuracy: 0.9575 - val_loss: 0.1649\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9563 - loss: 0.1491\n",
      "Epoch 127: val_accuracy improved from 0.95833 to 0.95920, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9563 - loss: 0.1491 - val_accuracy: 0.9592 - val_loss: 0.1672\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9600 - loss: 0.1490\n",
      "Epoch 128: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9600 - loss: 0.1490 - val_accuracy: 0.9575 - val_loss: 0.1637\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9559 - loss: 0.1476\n",
      "Epoch 129: val_accuracy did not improve from 0.95920\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9559 - loss: 0.1476 - val_accuracy: 0.9592 - val_loss: 0.1621\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9589 - loss: 0.1452\n",
      "Epoch 130: val_accuracy improved from 0.95920 to 0.96267, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9589 - loss: 0.1452 - val_accuracy: 0.9627 - val_loss: 0.1632\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9605 - loss: 0.1449\n",
      "Epoch 131: val_accuracy did not improve from 0.96267\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9605 - loss: 0.1449 - val_accuracy: 0.9575 - val_loss: 0.1597\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9568 - loss: 0.1435\n",
      "Epoch 132: val_accuracy did not improve from 0.96267\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9568 - loss: 0.1435 - val_accuracy: 0.9583 - val_loss: 0.1582\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9585 - loss: 0.1414\n",
      "Epoch 133: val_accuracy improved from 0.96267 to 0.96354, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9585 - loss: 0.1414 - val_accuracy: 0.9635 - val_loss: 0.1595\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9609 - loss: 0.1412\n",
      "Epoch 134: val_accuracy did not improve from 0.96354\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9609 - loss: 0.1412 - val_accuracy: 0.9592 - val_loss: 0.1565\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9583 - loss: 0.1395\n",
      "Epoch 135: val_accuracy did not improve from 0.96354\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9583 - loss: 0.1395 - val_accuracy: 0.9601 - val_loss: 0.1560\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9596 - loss: 0.1378\n",
      "Epoch 136: val_accuracy improved from 0.96354 to 0.96528, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9596 - loss: 0.1378 - val_accuracy: 0.9653 - val_loss: 0.1573\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9633 - loss: 0.1375\n",
      "Epoch 137: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9633 - loss: 0.1375 - val_accuracy: 0.9592 - val_loss: 0.1542\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9585 - loss: 0.1360\n",
      "Epoch 138: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9585 - loss: 0.1360 - val_accuracy: 0.9618 - val_loss: 0.1535\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9598 - loss: 0.1343\n",
      "Epoch 139: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9598 - loss: 0.1343 - val_accuracy: 0.9627 - val_loss: 0.1550\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9629 - loss: 0.1342\n",
      "Epoch 140: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9629 - loss: 0.1342 - val_accuracy: 0.9609 - val_loss: 0.1518\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9598 - loss: 0.1325\n",
      "Epoch 141: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9598 - loss: 0.1325 - val_accuracy: 0.9644 - val_loss: 0.1511\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9613 - loss: 0.1308\n",
      "Epoch 142: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9613 - loss: 0.1308 - val_accuracy: 0.9653 - val_loss: 0.1522\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9639 - loss: 0.1305\n",
      "Epoch 143: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9639 - loss: 0.1305 - val_accuracy: 0.9601 - val_loss: 0.1488\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9609 - loss: 0.1294\n",
      "Epoch 144: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9609 - loss: 0.1294 - val_accuracy: 0.9627 - val_loss: 0.1484\n",
      "Epoch 145/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9629 - loss: 0.1274\n",
      "Epoch 145: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9629 - loss: 0.1274 - val_accuracy: 0.9644 - val_loss: 0.1491\n",
      "Epoch 146/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9648 - loss: 0.1269\n",
      "Epoch 146: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9648 - loss: 0.1269 - val_accuracy: 0.9618 - val_loss: 0.1469\n",
      "Epoch 147/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9622 - loss: 0.1259\n",
      "Epoch 147: val_accuracy did not improve from 0.96528\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9622 - loss: 0.1259 - val_accuracy: 0.9635 - val_loss: 0.1470\n",
      "Epoch 148/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9646 - loss: 0.1242\n",
      "Epoch 148: val_accuracy improved from 0.96528 to 0.96615, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9646 - loss: 0.1242 - val_accuracy: 0.9661 - val_loss: 0.1476\n",
      "Epoch 149/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9657 - loss: 0.1235\n",
      "Epoch 149: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9657 - loss: 0.1235 - val_accuracy: 0.9627 - val_loss: 0.1456\n",
      "Epoch 150/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9642 - loss: 0.1229\n",
      "Epoch 150: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9642 - loss: 0.1229 - val_accuracy: 0.9644 - val_loss: 0.1458\n",
      "Epoch 151/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9666 - loss: 0.1213\n",
      "Epoch 151: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9666 - loss: 0.1213 - val_accuracy: 0.9644 - val_loss: 0.1450\n",
      "Epoch 152/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.9668 - loss: 0.1203\n",
      "Epoch 152: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9668 - loss: 0.1203 - val_accuracy: 0.9627 - val_loss: 0.1437\n",
      "Epoch 153/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9646 - loss: 0.1197\n",
      "Epoch 153: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9646 - loss: 0.1197 - val_accuracy: 0.9644 - val_loss: 0.1445\n",
      "Epoch 154/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9666 - loss: 0.1186\n",
      "Epoch 154: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9666 - loss: 0.1186 - val_accuracy: 0.9644 - val_loss: 0.1429\n",
      "Epoch 155/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9672 - loss: 0.1173\n",
      "Epoch 155: val_accuracy did not improve from 0.96615\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9672 - loss: 0.1173 - val_accuracy: 0.9644 - val_loss: 0.1426\n",
      "Epoch 156/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9668 - loss: 0.1164\n",
      "Epoch 156: val_accuracy improved from 0.96615 to 0.96701, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9668 - loss: 0.1164 - val_accuracy: 0.9670 - val_loss: 0.1433\n",
      "Epoch 157/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9672 - loss: 0.1157\n",
      "Epoch 157: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9672 - loss: 0.1157 - val_accuracy: 0.9635 - val_loss: 0.1415\n",
      "Epoch 158/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9674 - loss: 0.1147\n",
      "Epoch 158: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9674 - loss: 0.1147 - val_accuracy: 0.9635 - val_loss: 0.1418\n",
      "Epoch 159/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9674 - loss: 0.1137\n",
      "Epoch 159: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9674 - loss: 0.1137 - val_accuracy: 0.9635 - val_loss: 0.1411\n",
      "Epoch 160/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9683 - loss: 0.1127\n",
      "Epoch 160: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9683 - loss: 0.1127 - val_accuracy: 0.9627 - val_loss: 0.1405\n",
      "Epoch 161/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9683 - loss: 0.1119\n",
      "Epoch 161: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9683 - loss: 0.1119 - val_accuracy: 0.9661 - val_loss: 0.1410\n",
      "Epoch 162/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9685 - loss: 0.1110\n",
      "Epoch 162: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9685 - loss: 0.1110 - val_accuracy: 0.9627 - val_loss: 0.1402\n",
      "Epoch 163/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9681 - loss: 0.1101\n",
      "Epoch 163: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9681 - loss: 0.1101 - val_accuracy: 0.9627 - val_loss: 0.1401\n",
      "Epoch 164/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9687 - loss: 0.1092\n",
      "Epoch 164: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9687 - loss: 0.1092 - val_accuracy: 0.9635 - val_loss: 0.1395\n",
      "Epoch 165/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9694 - loss: 0.1083\n",
      "Epoch 165: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9694 - loss: 0.1083 - val_accuracy: 0.9644 - val_loss: 0.1388\n",
      "Epoch 166/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9689 - loss: 0.1075\n",
      "Epoch 166: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9689 - loss: 0.1075 - val_accuracy: 0.9653 - val_loss: 0.1391\n",
      "Epoch 167/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9700 - loss: 0.1067\n",
      "Epoch 167: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9700 - loss: 0.1067 - val_accuracy: 0.9653 - val_loss: 0.1380\n",
      "Epoch 168/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9692 - loss: 0.1059\n",
      "Epoch 168: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9692 - loss: 0.1059 - val_accuracy: 0.9661 - val_loss: 0.1386\n",
      "Epoch 169/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9707 - loss: 0.1051\n",
      "Epoch 169: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9707 - loss: 0.1051 - val_accuracy: 0.9644 - val_loss: 0.1372\n",
      "Epoch 170/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9698 - loss: 0.1043\n",
      "Epoch 170: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9698 - loss: 0.1043 - val_accuracy: 0.9670 - val_loss: 0.1372\n",
      "Epoch 171/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9705 - loss: 0.1035\n",
      "Epoch 171: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9705 - loss: 0.1035 - val_accuracy: 0.9661 - val_loss: 0.1365\n",
      "Epoch 172/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9705 - loss: 0.1027\n",
      "Epoch 172: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9705 - loss: 0.1027 - val_accuracy: 0.9670 - val_loss: 0.1362\n",
      "Epoch 173/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9713 - loss: 0.1019\n",
      "Epoch 173: val_accuracy did not improve from 0.96701\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9713 - loss: 0.1019 - val_accuracy: 0.9670 - val_loss: 0.1357\n",
      "Epoch 174/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9713 - loss: 0.1012\n",
      "Epoch 174: val_accuracy improved from 0.96701 to 0.96788, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9713 - loss: 0.1012 - val_accuracy: 0.9679 - val_loss: 0.1351\n",
      "Epoch 175/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9711 - loss: 0.1005\n",
      "Epoch 175: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9711 - loss: 0.1005 - val_accuracy: 0.9679 - val_loss: 0.1350\n",
      "Epoch 176/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9711 - loss: 0.0998\n",
      "Epoch 176: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9711 - loss: 0.0998 - val_accuracy: 0.9670 - val_loss: 0.1347\n",
      "Epoch 177/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9715 - loss: 0.0991\n",
      "Epoch 177: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9715 - loss: 0.0991 - val_accuracy: 0.9679 - val_loss: 0.1353\n",
      "Epoch 178/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9715 - loss: 0.0986\n",
      "Epoch 178: val_accuracy did not improve from 0.96788\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9715 - loss: 0.0986 - val_accuracy: 0.9644 - val_loss: 0.1340\n",
      "Epoch 179/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9698 - loss: 0.0992\n",
      "Epoch 179: val_accuracy improved from 0.96788 to 0.96962, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9698 - loss: 0.0992 - val_accuracy: 0.9696 - val_loss: 0.1420\n",
      "Epoch 180/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9744 - loss: 0.1027\n",
      "Epoch 180: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9744 - loss: 0.1027 - val_accuracy: 0.9635 - val_loss: 0.1408\n",
      "Epoch 181/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9607 - loss: 0.1093\n",
      "Epoch 181: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9607 - loss: 0.1093 - val_accuracy: 0.9688 - val_loss: 0.1385\n",
      "Epoch 182/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9748 - loss: 0.0998\n",
      "Epoch 182: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9748 - loss: 0.0998 - val_accuracy: 0.9688 - val_loss: 0.1339\n",
      "Epoch 183/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9733 - loss: 0.0964\n",
      "Epoch 183: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9733 - loss: 0.0964 - val_accuracy: 0.9644 - val_loss: 0.1342\n",
      "Epoch 184/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9659 - loss: 0.1010\n",
      "Epoch 184: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9659 - loss: 0.1010 - val_accuracy: 0.9688 - val_loss: 0.1336\n",
      "Epoch 185/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9744 - loss: 0.0951\n",
      "Epoch 185: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9744 - loss: 0.0951 - val_accuracy: 0.9696 - val_loss: 0.1353\n",
      "Epoch 186/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9757 - loss: 0.0949\n",
      "Epoch 186: val_accuracy did not improve from 0.96962\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9757 - loss: 0.0949 - val_accuracy: 0.9653 - val_loss: 0.1351\n",
      "Epoch 187/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9679 - loss: 0.0983\n",
      "Epoch 187: val_accuracy improved from 0.96962 to 0.97049, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9679 - loss: 0.0983 - val_accuracy: 0.9705 - val_loss: 0.1331\n",
      "Epoch 188/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9752 - loss: 0.0930\n",
      "Epoch 188: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9752 - loss: 0.0930 - val_accuracy: 0.9705 - val_loss: 0.1336\n",
      "Epoch 189/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9759 - loss: 0.0932\n",
      "Epoch 189: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9759 - loss: 0.0932 - val_accuracy: 0.9653 - val_loss: 0.1318\n",
      "Epoch 190/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9705 - loss: 0.0952\n",
      "Epoch 190: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9705 - loss: 0.0952 - val_accuracy: 0.9661 - val_loss: 0.1294\n",
      "Epoch 191/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9733 - loss: 0.0909\n",
      "Epoch 191: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9733 - loss: 0.0909 - val_accuracy: 0.9705 - val_loss: 0.1326\n",
      "Epoch 192/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9750 - loss: 0.0932\n",
      "Epoch 192: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9750 - loss: 0.0932 - val_accuracy: 0.9661 - val_loss: 0.1304\n",
      "Epoch 193/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9705 - loss: 0.0933\n",
      "Epoch 193: val_accuracy did not improve from 0.97049\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9705 - loss: 0.0933 - val_accuracy: 0.9670 - val_loss: 0.1293\n",
      "Epoch 194/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9726 - loss: 0.0889\n",
      "Epoch 194: val_accuracy improved from 0.97049 to 0.97396, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9726 - loss: 0.0889 - val_accuracy: 0.9740 - val_loss: 0.1348\n",
      "Epoch 195/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9765 - loss: 0.0924\n",
      "Epoch 195: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9765 - loss: 0.0924 - val_accuracy: 0.9670 - val_loss: 0.1301\n",
      "Epoch 196/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9700 - loss: 0.0917\n",
      "Epoch 196: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9700 - loss: 0.0917 - val_accuracy: 0.9688 - val_loss: 0.1279\n",
      "Epoch 197/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9739 - loss: 0.0874\n",
      "Epoch 197: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9739 - loss: 0.0874 - val_accuracy: 0.9731 - val_loss: 0.1335\n",
      "Epoch 198/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9770 - loss: 0.0913\n",
      "Epoch 198: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9770 - loss: 0.0913 - val_accuracy: 0.9670 - val_loss: 0.1272\n",
      "Epoch 199/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9705 - loss: 0.0898\n",
      "Epoch 199: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9705 - loss: 0.0898 - val_accuracy: 0.9661 - val_loss: 0.1252\n",
      "Epoch 200/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9728 - loss: 0.0861\n",
      "Epoch 200: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9728 - loss: 0.0861 - val_accuracy: 0.9714 - val_loss: 0.1320\n",
      "Epoch 201/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9770 - loss: 0.0904\n",
      "Epoch 201: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9770 - loss: 0.0904 - val_accuracy: 0.9679 - val_loss: 0.1248\n",
      "Epoch 202/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9724 - loss: 0.0869\n",
      "Epoch 202: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9724 - loss: 0.0869 - val_accuracy: 0.9679 - val_loss: 0.1239\n",
      "Epoch 203/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9733 - loss: 0.0856\n",
      "Epoch 203: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9733 - loss: 0.0856 - val_accuracy: 0.9722 - val_loss: 0.1305\n",
      "Epoch 204/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9781 - loss: 0.0884\n",
      "Epoch 204: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9781 - loss: 0.0884 - val_accuracy: 0.9679 - val_loss: 0.1241\n",
      "Epoch 205/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9744 - loss: 0.0843\n",
      "Epoch 205: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9744 - loss: 0.0843 - val_accuracy: 0.9679 - val_loss: 0.1243\n",
      "Epoch 206/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9737 - loss: 0.0845\n",
      "Epoch 206: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9737 - loss: 0.0845 - val_accuracy: 0.9722 - val_loss: 0.1283\n",
      "Epoch 207/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9783 - loss: 0.0852\n",
      "Epoch 207: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9783 - loss: 0.0852 - val_accuracy: 0.9688 - val_loss: 0.1239\n",
      "Epoch 208/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9757 - loss: 0.0822\n",
      "Epoch 208: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9757 - loss: 0.0822 - val_accuracy: 0.9679 - val_loss: 0.1243\n",
      "Epoch 209/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9728 - loss: 0.0835\n",
      "Epoch 209: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9728 - loss: 0.0835 - val_accuracy: 0.9722 - val_loss: 0.1257\n",
      "Epoch 210/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9776 - loss: 0.0824\n",
      "Epoch 210: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9776 - loss: 0.0824 - val_accuracy: 0.9705 - val_loss: 0.1239\n",
      "Epoch 211/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9765 - loss: 0.0811\n",
      "Epoch 211: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9765 - loss: 0.0811 - val_accuracy: 0.9679 - val_loss: 0.1232\n",
      "Epoch 212/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9742 - loss: 0.0820\n",
      "Epoch 212: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9742 - loss: 0.0820 - val_accuracy: 0.9731 - val_loss: 0.1237\n",
      "Epoch 213/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9772 - loss: 0.0804\n",
      "Epoch 213: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9772 - loss: 0.0804 - val_accuracy: 0.9740 - val_loss: 0.1237\n",
      "Epoch 214/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9785 - loss: 0.0804\n",
      "Epoch 214: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9785 - loss: 0.0804 - val_accuracy: 0.9688 - val_loss: 0.1218\n",
      "Epoch 215/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9742 - loss: 0.0805\n",
      "Epoch 215: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9742 - loss: 0.0805 - val_accuracy: 0.9679 - val_loss: 0.1221\n",
      "Epoch 216/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9765 - loss: 0.0789\n",
      "Epoch 216: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9765 - loss: 0.0789 - val_accuracy: 0.9740 - val_loss: 0.1238\n",
      "Epoch 217/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9794 - loss: 0.0795\n",
      "Epoch 217: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9794 - loss: 0.0795 - val_accuracy: 0.9688 - val_loss: 0.1215\n",
      "Epoch 218/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9750 - loss: 0.0790\n",
      "Epoch 218: val_accuracy did not improve from 0.97396\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9750 - loss: 0.0790 - val_accuracy: 0.9688 - val_loss: 0.1212\n",
      "Epoch 219/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9765 - loss: 0.0778\n",
      "Epoch 219: val_accuracy improved from 0.97396 to 0.97483, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9765 - loss: 0.0778 - val_accuracy: 0.9748 - val_loss: 0.1229\n",
      "Epoch 220/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9794 - loss: 0.0782\n",
      "Epoch 220: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9794 - loss: 0.0782 - val_accuracy: 0.9688 - val_loss: 0.1209\n",
      "Epoch 221/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9761 - loss: 0.0774\n",
      "Epoch 221: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9761 - loss: 0.0774 - val_accuracy: 0.9688 - val_loss: 0.1206\n",
      "Epoch 222/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9768 - loss: 0.0768\n",
      "Epoch 222: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9768 - loss: 0.0768 - val_accuracy: 0.9740 - val_loss: 0.1222\n",
      "Epoch 223/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 0.0771\n",
      "Epoch 223: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9800 - loss: 0.0771 - val_accuracy: 0.9679 - val_loss: 0.1204\n",
      "Epoch 224/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9765 - loss: 0.0761\n",
      "Epoch 224: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9765 - loss: 0.0761 - val_accuracy: 0.9688 - val_loss: 0.1203\n",
      "Epoch 225/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9765 - loss: 0.0758\n",
      "Epoch 225: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9765 - loss: 0.0758 - val_accuracy: 0.9731 - val_loss: 0.1219\n",
      "Epoch 226/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9805 - loss: 0.0760\n",
      "Epoch 226: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9805 - loss: 0.0760 - val_accuracy: 0.9705 - val_loss: 0.1200\n",
      "Epoch 227/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9768 - loss: 0.0750\n",
      "Epoch 227: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9768 - loss: 0.0750 - val_accuracy: 0.9705 - val_loss: 0.1200\n",
      "Epoch 228/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9768 - loss: 0.0747\n",
      "Epoch 228: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9768 - loss: 0.0747 - val_accuracy: 0.9748 - val_loss: 0.1213\n",
      "Epoch 229/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9802 - loss: 0.0748\n",
      "Epoch 229: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9802 - loss: 0.0748 - val_accuracy: 0.9696 - val_loss: 0.1197\n",
      "Epoch 230/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9774 - loss: 0.0740\n",
      "Epoch 230: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9774 - loss: 0.0740 - val_accuracy: 0.9705 - val_loss: 0.1197\n",
      "Epoch 231/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9776 - loss: 0.0736\n",
      "Epoch 231: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9776 - loss: 0.0736 - val_accuracy: 0.9740 - val_loss: 0.1204\n",
      "Epoch 232/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9798 - loss: 0.0735\n",
      "Epoch 232: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9798 - loss: 0.0735 - val_accuracy: 0.9705 - val_loss: 0.1191\n",
      "Epoch 233/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9776 - loss: 0.0729\n",
      "Epoch 233: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9776 - loss: 0.0729 - val_accuracy: 0.9705 - val_loss: 0.1190\n",
      "Epoch 234/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9783 - loss: 0.0726\n",
      "Epoch 234: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9783 - loss: 0.0726 - val_accuracy: 0.9740 - val_loss: 0.1199\n",
      "Epoch 235/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9802 - loss: 0.0725\n",
      "Epoch 235: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9802 - loss: 0.0725 - val_accuracy: 0.9705 - val_loss: 0.1189\n",
      "Epoch 236/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9781 - loss: 0.0722\n",
      "Epoch 236: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9781 - loss: 0.0722 - val_accuracy: 0.9714 - val_loss: 0.1191\n",
      "Epoch 237/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9789 - loss: 0.0716\n",
      "Epoch 237: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9789 - loss: 0.0716 - val_accuracy: 0.9740 - val_loss: 0.1192\n",
      "Epoch 238/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9802 - loss: 0.0714\n",
      "Epoch 238: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9802 - loss: 0.0714 - val_accuracy: 0.9705 - val_loss: 0.1183\n",
      "Epoch 239/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9781 - loss: 0.0712\n",
      "Epoch 239: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9781 - loss: 0.0712 - val_accuracy: 0.9722 - val_loss: 0.1187\n",
      "Epoch 240/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 0.0707\n",
      "Epoch 240: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9800 - loss: 0.0707 - val_accuracy: 0.9714 - val_loss: 0.1184\n",
      "Epoch 241/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9796 - loss: 0.0704\n",
      "Epoch 241: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9796 - loss: 0.0704 - val_accuracy: 0.9696 - val_loss: 0.1183\n",
      "Epoch 242/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9787 - loss: 0.0702\n",
      "Epoch 242: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9787 - loss: 0.0702 - val_accuracy: 0.9731 - val_loss: 0.1188\n",
      "Epoch 243/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9807 - loss: 0.0699\n",
      "Epoch 243: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9807 - loss: 0.0699 - val_accuracy: 0.9714 - val_loss: 0.1183\n",
      "Epoch 244/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9800 - loss: 0.0695\n",
      "Epoch 244: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9800 - loss: 0.0695 - val_accuracy: 0.9722 - val_loss: 0.1181\n",
      "Epoch 245/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9789 - loss: 0.0692\n",
      "Epoch 245: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9789 - loss: 0.0692 - val_accuracy: 0.9731 - val_loss: 0.1185\n",
      "Epoch 246/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9811 - loss: 0.0690\n",
      "Epoch 246: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9811 - loss: 0.0690 - val_accuracy: 0.9714 - val_loss: 0.1177\n",
      "Epoch 247/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0686\n",
      "Epoch 247: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9796 - loss: 0.0686 - val_accuracy: 0.9714 - val_loss: 0.1179\n",
      "Epoch 248/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9805 - loss: 0.0682\n",
      "Epoch 248: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9805 - loss: 0.0682 - val_accuracy: 0.9714 - val_loss: 0.1181\n",
      "Epoch 249/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9813 - loss: 0.0680\n",
      "Epoch 249: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9813 - loss: 0.0680 - val_accuracy: 0.9722 - val_loss: 0.1173\n",
      "Epoch 250/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9796 - loss: 0.0677\n",
      "Epoch 250: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9796 - loss: 0.0677 - val_accuracy: 0.9722 - val_loss: 0.1178\n",
      "Epoch 251/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9811 - loss: 0.0674\n",
      "Epoch 251: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9811 - loss: 0.0674 - val_accuracy: 0.9722 - val_loss: 0.1173\n",
      "Epoch 252/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9809 - loss: 0.0670\n",
      "Epoch 252: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9809 - loss: 0.0670 - val_accuracy: 0.9714 - val_loss: 0.1173\n",
      "Epoch 253/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9802 - loss: 0.0668\n",
      "Epoch 253: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9802 - loss: 0.0668 - val_accuracy: 0.9714 - val_loss: 0.1177\n",
      "Epoch 254/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9818 - loss: 0.0666\n",
      "Epoch 254: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9818 - loss: 0.0666 - val_accuracy: 0.9714 - val_loss: 0.1169\n",
      "Epoch 255/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9805 - loss: 0.0663\n",
      "Epoch 255: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9805 - loss: 0.0663 - val_accuracy: 0.9722 - val_loss: 0.1170\n",
      "Epoch 256/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9813 - loss: 0.0660\n",
      "Epoch 256: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9813 - loss: 0.0660 - val_accuracy: 0.9714 - val_loss: 0.1172\n",
      "Epoch 257/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9815 - loss: 0.0657\n",
      "Epoch 257: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9815 - loss: 0.0657 - val_accuracy: 0.9714 - val_loss: 0.1166\n",
      "Epoch 258/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9807 - loss: 0.0655\n",
      "Epoch 258: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9807 - loss: 0.0655 - val_accuracy: 0.9714 - val_loss: 0.1169\n",
      "Epoch 259/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0652\n",
      "Epoch 259: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9820 - loss: 0.0652 - val_accuracy: 0.9714 - val_loss: 0.1166\n",
      "Epoch 260/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9813 - loss: 0.0649\n",
      "Epoch 260: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9813 - loss: 0.0649 - val_accuracy: 0.9722 - val_loss: 0.1164\n",
      "Epoch 261/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9818 - loss: 0.0647\n",
      "Epoch 261: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9818 - loss: 0.0647 - val_accuracy: 0.9722 - val_loss: 0.1166\n",
      "Epoch 262/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9813 - loss: 0.0645\n",
      "Epoch 262: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9813 - loss: 0.0645 - val_accuracy: 0.9722 - val_loss: 0.1168\n",
      "Epoch 263/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9818 - loss: 0.0642\n",
      "Epoch 263: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9818 - loss: 0.0642 - val_accuracy: 0.9722 - val_loss: 0.1167\n",
      "Epoch 264/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9815 - loss: 0.0639\n",
      "Epoch 264: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9815 - loss: 0.0639 - val_accuracy: 0.9722 - val_loss: 0.1168\n",
      "Epoch 265/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9815 - loss: 0.0637\n",
      "Epoch 265: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9815 - loss: 0.0637 - val_accuracy: 0.9714 - val_loss: 0.1167\n",
      "Epoch 266/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.0634\n",
      "Epoch 266: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9818 - loss: 0.0634 - val_accuracy: 0.9714 - val_loss: 0.1164\n",
      "Epoch 267/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9820 - loss: 0.0632\n",
      "Epoch 267: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9820 - loss: 0.0632 - val_accuracy: 0.9722 - val_loss: 0.1166\n",
      "Epoch 268/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0629\n",
      "Epoch 268: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9820 - loss: 0.0629 - val_accuracy: 0.9722 - val_loss: 0.1168\n",
      "Epoch 269/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0627\n",
      "Epoch 269: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9820 - loss: 0.0627 - val_accuracy: 0.9705 - val_loss: 0.1165\n",
      "Epoch 270/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9815 - loss: 0.0624\n",
      "Epoch 270: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9815 - loss: 0.0624 - val_accuracy: 0.9722 - val_loss: 0.1165\n",
      "Epoch 271/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9822 - loss: 0.0622\n",
      "Epoch 271: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9822 - loss: 0.0622 - val_accuracy: 0.9722 - val_loss: 0.1162\n",
      "Epoch 272/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.0620\n",
      "Epoch 272: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9818 - loss: 0.0620 - val_accuracy: 0.9722 - val_loss: 0.1163\n",
      "Epoch 273/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0617\n",
      "Epoch 273: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9824 - loss: 0.0617 - val_accuracy: 0.9722 - val_loss: 0.1160\n",
      "Epoch 274/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9824 - loss: 0.0614\n",
      "Epoch 274: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9824 - loss: 0.0614 - val_accuracy: 0.9731 - val_loss: 0.1162\n",
      "Epoch 275/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9820 - loss: 0.0612\n",
      "Epoch 275: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9820 - loss: 0.0612 - val_accuracy: 0.9722 - val_loss: 0.1158\n",
      "Epoch 276/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0610\n",
      "Epoch 276: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9826 - loss: 0.0610 - val_accuracy: 0.9722 - val_loss: 0.1158\n",
      "Epoch 277/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0607\n",
      "Epoch 277: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0607 - val_accuracy: 0.9722 - val_loss: 0.1157\n",
      "Epoch 278/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9822 - loss: 0.0605\n",
      "Epoch 278: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9822 - loss: 0.0605 - val_accuracy: 0.9722 - val_loss: 0.1156\n",
      "Epoch 279/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0603\n",
      "Epoch 279: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0603 - val_accuracy: 0.9731 - val_loss: 0.1153\n",
      "Epoch 280/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9818 - loss: 0.0600\n",
      "Epoch 280: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9818 - loss: 0.0600 - val_accuracy: 0.9722 - val_loss: 0.1157\n",
      "Epoch 281/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0598\n",
      "Epoch 281: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0598 - val_accuracy: 0.9722 - val_loss: 0.1155\n",
      "Epoch 282/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9824 - loss: 0.0596\n",
      "Epoch 282: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9824 - loss: 0.0596 - val_accuracy: 0.9722 - val_loss: 0.1154\n",
      "Epoch 283/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9826 - loss: 0.0594\n",
      "Epoch 283: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9826 - loss: 0.0594 - val_accuracy: 0.9722 - val_loss: 0.1158\n",
      "Epoch 284/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0592\n",
      "Epoch 284: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9824 - loss: 0.0592 - val_accuracy: 0.9722 - val_loss: 0.1154\n",
      "Epoch 285/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9815 - loss: 0.0591\n",
      "Epoch 285: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9815 - loss: 0.0591 - val_accuracy: 0.9740 - val_loss: 0.1160\n",
      "Epoch 286/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9837 - loss: 0.0590\n",
      "Epoch 286: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9837 - loss: 0.0590 - val_accuracy: 0.9722 - val_loss: 0.1154\n",
      "Epoch 287/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9811 - loss: 0.0589\n",
      "Epoch 287: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9811 - loss: 0.0589 - val_accuracy: 0.9722 - val_loss: 0.1157\n",
      "Epoch 288/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9837 - loss: 0.0586\n",
      "Epoch 288: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9837 - loss: 0.0586 - val_accuracy: 0.9722 - val_loss: 0.1149\n",
      "Epoch 289/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9813 - loss: 0.0583\n",
      "Epoch 289: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9813 - loss: 0.0583 - val_accuracy: 0.9731 - val_loss: 0.1156\n",
      "Epoch 290/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9835 - loss: 0.0581\n",
      "Epoch 290: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9835 - loss: 0.0581 - val_accuracy: 0.9731 - val_loss: 0.1146\n",
      "Epoch 291/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9820 - loss: 0.0577\n",
      "Epoch 291: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9820 - loss: 0.0577 - val_accuracy: 0.9731 - val_loss: 0.1150\n",
      "Epoch 292/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9831 - loss: 0.0574\n",
      "Epoch 292: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9831 - loss: 0.0574 - val_accuracy: 0.9722 - val_loss: 0.1148\n",
      "Epoch 293/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0571\n",
      "Epoch 293: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9826 - loss: 0.0571 - val_accuracy: 0.9722 - val_loss: 0.1145\n",
      "Epoch 294/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9824 - loss: 0.0569\n",
      "Epoch 294: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9824 - loss: 0.0569 - val_accuracy: 0.9731 - val_loss: 0.1147\n",
      "Epoch 295/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0567\n",
      "Epoch 295: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9826 - loss: 0.0567 - val_accuracy: 0.9722 - val_loss: 0.1145\n",
      "Epoch 296/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9826 - loss: 0.0565\n",
      "Epoch 296: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9826 - loss: 0.0565 - val_accuracy: 0.9722 - val_loss: 0.1147\n",
      "Epoch 297/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9833 - loss: 0.0563\n",
      "Epoch 297: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9833 - loss: 0.0563 - val_accuracy: 0.9722 - val_loss: 0.1148\n",
      "Epoch 298/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9824 - loss: 0.0560\n",
      "Epoch 298: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9824 - loss: 0.0560 - val_accuracy: 0.9731 - val_loss: 0.1146\n",
      "Epoch 299/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9828 - loss: 0.0558\n",
      "Epoch 299: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9828 - loss: 0.0558 - val_accuracy: 0.9731 - val_loss: 0.1145\n",
      "Epoch 300/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9822 - loss: 0.0556\n",
      "Epoch 300: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9822 - loss: 0.0556 - val_accuracy: 0.9740 - val_loss: 0.1147\n",
      "Epoch 301/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9835 - loss: 0.0554\n",
      "Epoch 301: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9835 - loss: 0.0554 - val_accuracy: 0.9705 - val_loss: 0.1142\n",
      "Epoch 302/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0554\n",
      "Epoch 302: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9824 - loss: 0.0554 - val_accuracy: 0.9748 - val_loss: 0.1150\n",
      "Epoch 303/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9848 - loss: 0.0555\n",
      "Epoch 303: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9848 - loss: 0.0555 - val_accuracy: 0.9722 - val_loss: 0.1147\n",
      "Epoch 304/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9805 - loss: 0.0559\n",
      "Epoch 304: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9805 - loss: 0.0559 - val_accuracy: 0.9740 - val_loss: 0.1160\n",
      "Epoch 305/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9861 - loss: 0.0566\n",
      "Epoch 305: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9861 - loss: 0.0566 - val_accuracy: 0.9696 - val_loss: 0.1169\n",
      "Epoch 306/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9781 - loss: 0.0587\n",
      "Epoch 306: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9781 - loss: 0.0587 - val_accuracy: 0.9748 - val_loss: 0.1195\n",
      "Epoch 307/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0591\n",
      "Epoch 307: val_accuracy did not improve from 0.97483\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9861 - loss: 0.0591 - val_accuracy: 0.9705 - val_loss: 0.1170\n",
      "Epoch 308/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9768 - loss: 0.0593\n",
      "Epoch 308: val_accuracy improved from 0.97483 to 0.97569, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.9768 - loss: 0.0593 - val_accuracy: 0.9757 - val_loss: 0.1149\n",
      "Epoch 309/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9859 - loss: 0.0555\n",
      "Epoch 309: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9859 - loss: 0.0555 - val_accuracy: 0.9722 - val_loss: 0.1130\n",
      "Epoch 310/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9837 - loss: 0.0535\n",
      "Epoch 310: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9837 - loss: 0.0535 - val_accuracy: 0.9722 - val_loss: 0.1126\n",
      "Epoch 311/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9822 - loss: 0.0541\n",
      "Epoch 311: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9822 - loss: 0.0541 - val_accuracy: 0.9757 - val_loss: 0.1158\n",
      "Epoch 312/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9861 - loss: 0.0557\n",
      "Epoch 312: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9861 - loss: 0.0557 - val_accuracy: 0.9722 - val_loss: 0.1164\n",
      "Epoch 313/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9785 - loss: 0.0570\n",
      "Epoch 313: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.9785 - loss: 0.0570 - val_accuracy: 0.9740 - val_loss: 0.1151\n",
      "Epoch 314/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9863 - loss: 0.0544\n",
      "Epoch 314: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9863 - loss: 0.0544 - val_accuracy: 0.9722 - val_loss: 0.1119\n",
      "Epoch 315/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9835 - loss: 0.0527\n",
      "Epoch 315: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9835 - loss: 0.0527 - val_accuracy: 0.9748 - val_loss: 0.1127\n",
      "Epoch 316/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9833 - loss: 0.0525\n",
      "Epoch 316: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9833 - loss: 0.0525 - val_accuracy: 0.9748 - val_loss: 0.1137\n",
      "Epoch 317/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9868 - loss: 0.0532\n",
      "Epoch 317: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9868 - loss: 0.0532 - val_accuracy: 0.9731 - val_loss: 0.1127\n",
      "Epoch 318/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9807 - loss: 0.0538\n",
      "Epoch 318: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9807 - loss: 0.0538 - val_accuracy: 0.9740 - val_loss: 0.1135\n",
      "Epoch 319/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9868 - loss: 0.0527\n",
      "Epoch 319: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9868 - loss: 0.0527 - val_accuracy: 0.9748 - val_loss: 0.1119\n",
      "Epoch 320/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9839 - loss: 0.0516\n",
      "Epoch 320: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9839 - loss: 0.0516 - val_accuracy: 0.9714 - val_loss: 0.1106\n",
      "Epoch 321/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9841 - loss: 0.0511\n",
      "Epoch 321: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9841 - loss: 0.0511 - val_accuracy: 0.9740 - val_loss: 0.1119\n",
      "Epoch 322/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9868 - loss: 0.0512\n",
      "Epoch 322: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9868 - loss: 0.0512 - val_accuracy: 0.9722 - val_loss: 0.1128\n",
      "Epoch 323/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9820 - loss: 0.0516\n",
      "Epoch 323: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9820 - loss: 0.0516 - val_accuracy: 0.9731 - val_loss: 0.1125\n",
      "Epoch 324/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0518\n",
      "Epoch 324: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9865 - loss: 0.0518 - val_accuracy: 0.9722 - val_loss: 0.1129\n",
      "Epoch 325/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9818 - loss: 0.0517\n",
      "Epoch 325: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9818 - loss: 0.0517 - val_accuracy: 0.9757 - val_loss: 0.1128\n",
      "Epoch 326/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9868 - loss: 0.0513\n",
      "Epoch 326: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9868 - loss: 0.0513 - val_accuracy: 0.9731 - val_loss: 0.1115\n",
      "Epoch 327/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9833 - loss: 0.0506\n",
      "Epoch 327: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9833 - loss: 0.0506 - val_accuracy: 0.9722 - val_loss: 0.1115\n",
      "Epoch 328/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9863 - loss: 0.0500\n",
      "Epoch 328: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9863 - loss: 0.0500 - val_accuracy: 0.9731 - val_loss: 0.1119\n",
      "Epoch 329/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9848 - loss: 0.0495\n",
      "Epoch 329: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9848 - loss: 0.0495 - val_accuracy: 0.9731 - val_loss: 0.1113\n",
      "Epoch 330/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9854 - loss: 0.0493\n",
      "Epoch 330: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9854 - loss: 0.0493 - val_accuracy: 0.9722 - val_loss: 0.1113\n",
      "Epoch 331/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0493\n",
      "Epoch 331: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9865 - loss: 0.0493 - val_accuracy: 0.9731 - val_loss: 0.1120\n",
      "Epoch 332/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9837 - loss: 0.0496\n",
      "Epoch 332: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9837 - loss: 0.0496 - val_accuracy: 0.9748 - val_loss: 0.1120\n",
      "Epoch 333/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - loss: 0.0499\n",
      "Epoch 333: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9876 - loss: 0.0499 - val_accuracy: 0.9714 - val_loss: 0.1120\n",
      "Epoch 334/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9822 - loss: 0.0503\n",
      "Epoch 334: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9822 - loss: 0.0503 - val_accuracy: 0.9757 - val_loss: 0.1124\n",
      "Epoch 335/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9874 - loss: 0.0503\n",
      "Epoch 335: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9874 - loss: 0.0503 - val_accuracy: 0.9722 - val_loss: 0.1120\n",
      "Epoch 336/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9824 - loss: 0.0501\n",
      "Epoch 336: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.9824 - loss: 0.0501 - val_accuracy: 0.9748 - val_loss: 0.1107\n",
      "Epoch 337/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9878 - loss: 0.0491\n",
      "Epoch 337: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9878 - loss: 0.0491 - val_accuracy: 0.9731 - val_loss: 0.1106\n",
      "Epoch 338/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9850 - loss: 0.0482\n",
      "Epoch 338: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9850 - loss: 0.0482 - val_accuracy: 0.9740 - val_loss: 0.1106\n",
      "Epoch 339/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9868 - loss: 0.0476\n",
      "Epoch 339: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9868 - loss: 0.0476 - val_accuracy: 0.9714 - val_loss: 0.1105\n",
      "Epoch 340/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9878 - loss: 0.0476\n",
      "Epoch 340: val_accuracy did not improve from 0.97569\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9878 - loss: 0.0476 - val_accuracy: 0.9731 - val_loss: 0.1114\n",
      "Epoch 341/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9839 - loss: 0.0479\n",
      "Epoch 341: val_accuracy improved from 0.97569 to 0.97656, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9839 - loss: 0.0479 - val_accuracy: 0.9766 - val_loss: 0.1123\n",
      "Epoch 342/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9878 - loss: 0.0484\n",
      "Epoch 342: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9878 - loss: 0.0484 - val_accuracy: 0.9714 - val_loss: 0.1119\n",
      "Epoch 343/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9828 - loss: 0.0488\n",
      "Epoch 343: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9828 - loss: 0.0488 - val_accuracy: 0.9748 - val_loss: 0.1117\n",
      "Epoch 344/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.9878 - loss: 0.0484\n",
      "Epoch 344: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9878 - loss: 0.0484 - val_accuracy: 0.9722 - val_loss: 0.1118\n",
      "Epoch 345/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9839 - loss: 0.0481\n",
      "Epoch 345: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9839 - loss: 0.0481 - val_accuracy: 0.9731 - val_loss: 0.1103\n",
      "Epoch 346/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0471\n",
      "Epoch 346: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9883 - loss: 0.0471 - val_accuracy: 0.9740 - val_loss: 0.1099\n",
      "Epoch 347/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9868 - loss: 0.0464\n",
      "Epoch 347: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9868 - loss: 0.0464 - val_accuracy: 0.9740 - val_loss: 0.1105\n",
      "Epoch 348/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9863 - loss: 0.0462\n",
      "Epoch 348: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9863 - loss: 0.0462 - val_accuracy: 0.9722 - val_loss: 0.1106\n",
      "Epoch 349/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0464\n",
      "Epoch 349: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9883 - loss: 0.0464 - val_accuracy: 0.9731 - val_loss: 0.1110\n",
      "Epoch 350/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9839 - loss: 0.0467\n",
      "Epoch 350: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9839 - loss: 0.0467 - val_accuracy: 0.9757 - val_loss: 0.1110\n",
      "Epoch 351/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0466\n",
      "Epoch 351: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9883 - loss: 0.0466 - val_accuracy: 0.9731 - val_loss: 0.1108\n",
      "Epoch 352/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9850 - loss: 0.0462\n",
      "Epoch 352: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9850 - loss: 0.0462 - val_accuracy: 0.9714 - val_loss: 0.1100\n",
      "Epoch 353/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9881 - loss: 0.0456\n",
      "Epoch 353: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9881 - loss: 0.0456 - val_accuracy: 0.9740 - val_loss: 0.1102\n",
      "Epoch 354/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0452\n",
      "Epoch 354: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9865 - loss: 0.0452 - val_accuracy: 0.9740 - val_loss: 0.1104\n",
      "Epoch 355/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0451\n",
      "Epoch 355: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9865 - loss: 0.0451 - val_accuracy: 0.9722 - val_loss: 0.1100\n",
      "Epoch 356/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9885 - loss: 0.0451\n",
      "Epoch 356: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9885 - loss: 0.0451 - val_accuracy: 0.9731 - val_loss: 0.1104\n",
      "Epoch 357/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9861 - loss: 0.0450\n",
      "Epoch 357: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9861 - loss: 0.0450 - val_accuracy: 0.9722 - val_loss: 0.1106\n",
      "Epoch 358/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9881 - loss: 0.0449\n",
      "Epoch 358: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9881 - loss: 0.0449 - val_accuracy: 0.9731 - val_loss: 0.1104\n",
      "Epoch 359/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9861 - loss: 0.0446\n",
      "Epoch 359: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9861 - loss: 0.0446 - val_accuracy: 0.9714 - val_loss: 0.1105\n",
      "Epoch 360/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9874 - loss: 0.0444\n",
      "Epoch 360: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9874 - loss: 0.0444 - val_accuracy: 0.9731 - val_loss: 0.1105\n",
      "Epoch 361/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9872 - loss: 0.0442\n",
      "Epoch 361: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9872 - loss: 0.0442 - val_accuracy: 0.9722 - val_loss: 0.1101\n",
      "Epoch 362/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9872 - loss: 0.0440\n",
      "Epoch 362: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9872 - loss: 0.0440 - val_accuracy: 0.9722 - val_loss: 0.1104\n",
      "Epoch 363/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9881 - loss: 0.0440\n",
      "Epoch 363: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9881 - loss: 0.0440 - val_accuracy: 0.9740 - val_loss: 0.1105\n",
      "Epoch 364/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0438\n",
      "Epoch 364: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9865 - loss: 0.0438 - val_accuracy: 0.9731 - val_loss: 0.1102\n",
      "Epoch 365/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9885 - loss: 0.0437\n",
      "Epoch 365: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9885 - loss: 0.0437 - val_accuracy: 0.9740 - val_loss: 0.1102\n",
      "Epoch 366/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9865 - loss: 0.0436\n",
      "Epoch 366: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9865 - loss: 0.0436 - val_accuracy: 0.9731 - val_loss: 0.1102\n",
      "Epoch 367/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9883 - loss: 0.0435\n",
      "Epoch 367: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9883 - loss: 0.0435 - val_accuracy: 0.9740 - val_loss: 0.1104\n",
      "Epoch 368/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0433\n",
      "Epoch 368: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9865 - loss: 0.0433 - val_accuracy: 0.9740 - val_loss: 0.1106\n",
      "Epoch 369/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9883 - loss: 0.0432\n",
      "Epoch 369: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9883 - loss: 0.0432 - val_accuracy: 0.9731 - val_loss: 0.1109\n",
      "Epoch 370/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0432\n",
      "Epoch 370: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9861 - loss: 0.0432 - val_accuracy: 0.9722 - val_loss: 0.1107\n",
      "Epoch 371/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9891 - loss: 0.0430\n",
      "Epoch 371: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9891 - loss: 0.0430 - val_accuracy: 0.9740 - val_loss: 0.1110\n",
      "Epoch 372/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9863 - loss: 0.0428\n",
      "Epoch 372: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9863 - loss: 0.0428 - val_accuracy: 0.9731 - val_loss: 0.1109\n",
      "Epoch 373/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0427\n",
      "Epoch 373: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9889 - loss: 0.0427 - val_accuracy: 0.9740 - val_loss: 0.1112\n",
      "Epoch 374/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9868 - loss: 0.0425\n",
      "Epoch 374: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9868 - loss: 0.0425 - val_accuracy: 0.9740 - val_loss: 0.1113\n",
      "Epoch 375/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9891 - loss: 0.0424\n",
      "Epoch 375: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9891 - loss: 0.0424 - val_accuracy: 0.9740 - val_loss: 0.1114\n",
      "Epoch 376/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9868 - loss: 0.0422\n",
      "Epoch 376: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9868 - loss: 0.0422 - val_accuracy: 0.9748 - val_loss: 0.1111\n",
      "Epoch 377/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0421\n",
      "Epoch 377: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9894 - loss: 0.0421 - val_accuracy: 0.9731 - val_loss: 0.1117\n",
      "Epoch 378/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9861 - loss: 0.0421\n",
      "Epoch 378: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9861 - loss: 0.0421 - val_accuracy: 0.9748 - val_loss: 0.1115\n",
      "Epoch 379/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0420\n",
      "Epoch 379: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9896 - loss: 0.0420 - val_accuracy: 0.9731 - val_loss: 0.1120\n",
      "Epoch 380/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9865 - loss: 0.0420\n",
      "Epoch 380: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9865 - loss: 0.0420 - val_accuracy: 0.9757 - val_loss: 0.1120\n",
      "Epoch 381/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9889 - loss: 0.0420\n",
      "Epoch 381: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9889 - loss: 0.0420 - val_accuracy: 0.9714 - val_loss: 0.1118\n",
      "Epoch 382/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9861 - loss: 0.0420\n",
      "Epoch 382: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9861 - loss: 0.0420 - val_accuracy: 0.9757 - val_loss: 0.1118\n",
      "Epoch 383/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9891 - loss: 0.0423\n",
      "Epoch 383: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9891 - loss: 0.0423 - val_accuracy: 0.9696 - val_loss: 0.1129\n",
      "Epoch 384/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9850 - loss: 0.0429\n",
      "Epoch 384: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9850 - loss: 0.0429 - val_accuracy: 0.9748 - val_loss: 0.1131\n",
      "Epoch 385/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9887 - loss: 0.0440\n",
      "Epoch 385: val_accuracy did not improve from 0.97656\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9887 - loss: 0.0440 - val_accuracy: 0.9696 - val_loss: 0.1158\n",
      "Epoch 386/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9815 - loss: 0.0457\n",
      "Epoch 386: val_accuracy improved from 0.97656 to 0.97743, saving model to MLP.keras\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9815 - loss: 0.0457 - val_accuracy: 0.9774 - val_loss: 0.1156\n",
      "Epoch 387/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9887 - loss: 0.0458\n",
      "Epoch 387: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9887 - loss: 0.0458 - val_accuracy: 0.9688 - val_loss: 0.1140\n",
      "Epoch 388/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9822 - loss: 0.0444\n",
      "Epoch 388: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9822 - loss: 0.0444 - val_accuracy: 0.9766 - val_loss: 0.1102\n",
      "Epoch 389/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0412\n",
      "Epoch 389: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9896 - loss: 0.0412 - val_accuracy: 0.9731 - val_loss: 0.1100\n",
      "Epoch 390/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9891 - loss: 0.0403\n",
      "Epoch 390: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9891 - loss: 0.0403 - val_accuracy: 0.9696 - val_loss: 0.1119\n",
      "Epoch 391/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9852 - loss: 0.0418\n",
      "Epoch 391: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9852 - loss: 0.0418 - val_accuracy: 0.9757 - val_loss: 0.1136\n",
      "Epoch 392/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0436\n",
      "Epoch 392: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9889 - loss: 0.0436 - val_accuracy: 0.9688 - val_loss: 0.1155\n",
      "Epoch 393/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9824 - loss: 0.0444\n",
      "Epoch 393: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9824 - loss: 0.0444 - val_accuracy: 0.9766 - val_loss: 0.1129\n",
      "Epoch 394/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0424\n",
      "Epoch 394: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9889 - loss: 0.0424 - val_accuracy: 0.9722 - val_loss: 0.1112\n",
      "Epoch 395/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9863 - loss: 0.0402\n",
      "Epoch 395: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9863 - loss: 0.0402 - val_accuracy: 0.9722 - val_loss: 0.1106\n",
      "Epoch 396/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0395\n",
      "Epoch 396: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9883 - loss: 0.0395 - val_accuracy: 0.9757 - val_loss: 0.1122\n",
      "Epoch 397/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9889 - loss: 0.0408\n",
      "Epoch 397: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0408 - val_accuracy: 0.9679 - val_loss: 0.1144\n",
      "Epoch 398/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9833 - loss: 0.0424\n",
      "Epoch 398: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9833 - loss: 0.0424 - val_accuracy: 0.9766 - val_loss: 0.1138\n",
      "Epoch 399/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9889 - loss: 0.0416\n",
      "Epoch 399: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0416 - val_accuracy: 0.9722 - val_loss: 0.1120\n",
      "Epoch 400/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9870 - loss: 0.0397\n",
      "Epoch 400: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9870 - loss: 0.0397 - val_accuracy: 0.9722 - val_loss: 0.1105\n",
      "Epoch 401/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9889 - loss: 0.0389\n",
      "Epoch 401: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9889 - loss: 0.0389 - val_accuracy: 0.9766 - val_loss: 0.1114\n",
      "Epoch 402/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9894 - loss: 0.0401\n",
      "Epoch 402: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9894 - loss: 0.0401 - val_accuracy: 0.9688 - val_loss: 0.1137\n",
      "Epoch 403/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9846 - loss: 0.0411\n",
      "Epoch 403: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9846 - loss: 0.0411 - val_accuracy: 0.9766 - val_loss: 0.1126\n",
      "Epoch 404/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9889 - loss: 0.0401\n",
      "Epoch 404: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.9889 - loss: 0.0401 - val_accuracy: 0.9714 - val_loss: 0.1120\n",
      "Epoch 405/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9878 - loss: 0.0387\n",
      "Epoch 405: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9878 - loss: 0.0387 - val_accuracy: 0.9722 - val_loss: 0.1119\n",
      "Epoch 406/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0385\n",
      "Epoch 406: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9883 - loss: 0.0385 - val_accuracy: 0.9774 - val_loss: 0.1122\n",
      "Epoch 407/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0394\n",
      "Epoch 407: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9898 - loss: 0.0394 - val_accuracy: 0.9705 - val_loss: 0.1130\n",
      "Epoch 408/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9859 - loss: 0.0397\n",
      "Epoch 408: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9859 - loss: 0.0397 - val_accuracy: 0.9757 - val_loss: 0.1114\n",
      "Epoch 409/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0387\n",
      "Epoch 409: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0387 - val_accuracy: 0.9722 - val_loss: 0.1112\n",
      "Epoch 410/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9889 - loss: 0.0379\n",
      "Epoch 410: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9889 - loss: 0.0379 - val_accuracy: 0.9722 - val_loss: 0.1115\n",
      "Epoch 411/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - loss: 0.0380\n",
      "Epoch 411: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9876 - loss: 0.0380 - val_accuracy: 0.9757 - val_loss: 0.1119\n",
      "Epoch 412/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0387\n",
      "Epoch 412: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9898 - loss: 0.0387 - val_accuracy: 0.9714 - val_loss: 0.1134\n",
      "Epoch 413/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9865 - loss: 0.0388\n",
      "Epoch 413: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9865 - loss: 0.0388 - val_accuracy: 0.9766 - val_loss: 0.1124\n",
      "Epoch 414/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0380\n",
      "Epoch 414: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0380 - val_accuracy: 0.9722 - val_loss: 0.1117\n",
      "Epoch 415/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0374\n",
      "Epoch 415: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9894 - loss: 0.0374 - val_accuracy: 0.9722 - val_loss: 0.1118\n",
      "Epoch 416/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9878 - loss: 0.0375\n",
      "Epoch 416: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9878 - loss: 0.0375 - val_accuracy: 0.9748 - val_loss: 0.1117\n",
      "Epoch 417/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0381\n",
      "Epoch 417: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9900 - loss: 0.0381 - val_accuracy: 0.9714 - val_loss: 0.1123\n",
      "Epoch 418/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9876 - loss: 0.0381\n",
      "Epoch 418: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9876 - loss: 0.0381 - val_accuracy: 0.9740 - val_loss: 0.1116\n",
      "Epoch 419/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9904 - loss: 0.0375\n",
      "Epoch 419: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0375 - val_accuracy: 0.9722 - val_loss: 0.1115\n",
      "Epoch 420/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9896 - loss: 0.0368\n",
      "Epoch 420: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9896 - loss: 0.0368 - val_accuracy: 0.9722 - val_loss: 0.1116\n",
      "Epoch 421/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9894 - loss: 0.0369\n",
      "Epoch 421: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9894 - loss: 0.0369 - val_accuracy: 0.9757 - val_loss: 0.1116\n",
      "Epoch 422/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0373\n",
      "Epoch 422: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9900 - loss: 0.0373 - val_accuracy: 0.9731 - val_loss: 0.1125\n",
      "Epoch 423/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9874 - loss: 0.0374\n",
      "Epoch 423: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9874 - loss: 0.0374 - val_accuracy: 0.9748 - val_loss: 0.1111\n",
      "Epoch 424/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0369\n",
      "Epoch 424: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0369 - val_accuracy: 0.9722 - val_loss: 0.1113\n",
      "Epoch 425/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9896 - loss: 0.0364\n",
      "Epoch 425: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9896 - loss: 0.0364 - val_accuracy: 0.9722 - val_loss: 0.1119\n",
      "Epoch 426/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9896 - loss: 0.0363\n",
      "Epoch 426: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9896 - loss: 0.0363 - val_accuracy: 0.9748 - val_loss: 0.1123\n",
      "Epoch 427/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9904 - loss: 0.0365\n",
      "Epoch 427: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9904 - loss: 0.0365 - val_accuracy: 0.9714 - val_loss: 0.1130\n",
      "Epoch 428/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9883 - loss: 0.0366\n",
      "Epoch 428: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9883 - loss: 0.0366 - val_accuracy: 0.9748 - val_loss: 0.1126\n",
      "Epoch 429/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9902 - loss: 0.0364\n",
      "Epoch 429: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9902 - loss: 0.0364 - val_accuracy: 0.9722 - val_loss: 0.1123\n",
      "Epoch 430/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0360\n",
      "Epoch 430: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9894 - loss: 0.0360 - val_accuracy: 0.9722 - val_loss: 0.1118\n",
      "Epoch 431/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0358\n",
      "Epoch 431: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9900 - loss: 0.0358 - val_accuracy: 0.9731 - val_loss: 0.1118\n",
      "Epoch 432/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0358\n",
      "Epoch 432: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9904 - loss: 0.0358 - val_accuracy: 0.9722 - val_loss: 0.1124\n",
      "Epoch 433/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9891 - loss: 0.0358\n",
      "Epoch 433: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9891 - loss: 0.0358 - val_accuracy: 0.9740 - val_loss: 0.1120\n",
      "Epoch 434/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9904 - loss: 0.0357\n",
      "Epoch 434: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0357 - val_accuracy: 0.9722 - val_loss: 0.1125\n",
      "Epoch 435/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9896 - loss: 0.0356\n",
      "Epoch 435: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9896 - loss: 0.0356 - val_accuracy: 0.9740 - val_loss: 0.1126\n",
      "Epoch 436/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9904 - loss: 0.0354\n",
      "Epoch 436: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0354 - val_accuracy: 0.9722 - val_loss: 0.1121\n",
      "Epoch 437/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9902 - loss: 0.0353\n",
      "Epoch 437: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9902 - loss: 0.0353 - val_accuracy: 0.9722 - val_loss: 0.1123\n",
      "Epoch 438/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9898 - loss: 0.0352\n",
      "Epoch 438: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9898 - loss: 0.0352 - val_accuracy: 0.9722 - val_loss: 0.1122\n",
      "Epoch 439/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0352\n",
      "Epoch 439: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0352 - val_accuracy: 0.9722 - val_loss: 0.1122\n",
      "Epoch 440/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0351\n",
      "Epoch 440: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9898 - loss: 0.0351 - val_accuracy: 0.9748 - val_loss: 0.1124\n",
      "Epoch 441/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0350\n",
      "Epoch 441: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0350 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 442/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9894 - loss: 0.0349\n",
      "Epoch 442: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9894 - loss: 0.0349 - val_accuracy: 0.9731 - val_loss: 0.1133\n",
      "Epoch 443/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0348\n",
      "Epoch 443: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0348 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 444/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0347\n",
      "Epoch 444: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9902 - loss: 0.0347 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 445/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9902 - loss: 0.0346\n",
      "Epoch 445: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0346 - val_accuracy: 0.9731 - val_loss: 0.1130\n",
      "Epoch 446/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0345\n",
      "Epoch 446: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9900 - loss: 0.0345 - val_accuracy: 0.9722 - val_loss: 0.1128\n",
      "Epoch 447/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0344\n",
      "Epoch 447: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0344 - val_accuracy: 0.9740 - val_loss: 0.1128\n",
      "Epoch 448/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9904 - loss: 0.0344\n",
      "Epoch 448: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9904 - loss: 0.0344 - val_accuracy: 0.9722 - val_loss: 0.1130\n",
      "Epoch 449/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0343\n",
      "Epoch 449: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9900 - loss: 0.0343 - val_accuracy: 0.9740 - val_loss: 0.1125\n",
      "Epoch 450/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0342\n",
      "Epoch 450: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0342 - val_accuracy: 0.9722 - val_loss: 0.1130\n",
      "Epoch 451/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0341\n",
      "Epoch 451: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0341 - val_accuracy: 0.9731 - val_loss: 0.1126\n",
      "Epoch 452/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0340\n",
      "Epoch 452: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9900 - loss: 0.0340 - val_accuracy: 0.9722 - val_loss: 0.1124\n",
      "Epoch 453/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9904 - loss: 0.0339\n",
      "Epoch 453: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9904 - loss: 0.0339 - val_accuracy: 0.9731 - val_loss: 0.1124\n",
      "Epoch 454/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0338\n",
      "Epoch 454: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0338 - val_accuracy: 0.9722 - val_loss: 0.1126\n",
      "Epoch 455/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0338\n",
      "Epoch 455: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0338 - val_accuracy: 0.9731 - val_loss: 0.1126\n",
      "Epoch 456/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0337\n",
      "Epoch 456: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9904 - loss: 0.0337 - val_accuracy: 0.9722 - val_loss: 0.1127\n",
      "Epoch 457/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9900 - loss: 0.0336\n",
      "Epoch 457: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9900 - loss: 0.0336 - val_accuracy: 0.9740 - val_loss: 0.1123\n",
      "Epoch 458/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9902 - loss: 0.0336\n",
      "Epoch 458: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9902 - loss: 0.0336 - val_accuracy: 0.9722 - val_loss: 0.1125\n",
      "Epoch 459/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9900 - loss: 0.0335\n",
      "Epoch 459: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9900 - loss: 0.0335 - val_accuracy: 0.9748 - val_loss: 0.1124\n",
      "Epoch 460/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0335\n",
      "Epoch 460: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0335 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 461/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9898 - loss: 0.0335\n",
      "Epoch 461: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9898 - loss: 0.0335 - val_accuracy: 0.9757 - val_loss: 0.1129\n",
      "Epoch 462/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9913 - loss: 0.0335\n",
      "Epoch 462: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9913 - loss: 0.0335 - val_accuracy: 0.9714 - val_loss: 0.1139\n",
      "Epoch 463/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0334\n",
      "Epoch 463: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0334 - val_accuracy: 0.9757 - val_loss: 0.1130\n",
      "Epoch 464/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0332\n",
      "Epoch 464: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0332 - val_accuracy: 0.9722 - val_loss: 0.1129\n",
      "Epoch 465/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0331\n",
      "Epoch 465: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9902 - loss: 0.0331 - val_accuracy: 0.9740 - val_loss: 0.1130\n",
      "Epoch 466/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0329\n",
      "Epoch 466: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9902 - loss: 0.0329 - val_accuracy: 0.9722 - val_loss: 0.1132\n",
      "Epoch 467/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0328\n",
      "Epoch 467: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9904 - loss: 0.0328 - val_accuracy: 0.9740 - val_loss: 0.1130\n",
      "Epoch 468/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0328\n",
      "Epoch 468: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0328 - val_accuracy: 0.9731 - val_loss: 0.1130\n",
      "Epoch 469/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9907 - loss: 0.0327\n",
      "Epoch 469: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9907 - loss: 0.0327 - val_accuracy: 0.9722 - val_loss: 0.1125\n",
      "Epoch 470/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9909 - loss: 0.0326\n",
      "Epoch 470: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9909 - loss: 0.0326 - val_accuracy: 0.9740 - val_loss: 0.1125\n",
      "Epoch 471/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0326\n",
      "Epoch 471: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9902 - loss: 0.0326 - val_accuracy: 0.9722 - val_loss: 0.1127\n",
      "Epoch 472/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0326\n",
      "Epoch 472: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9902 - loss: 0.0326 - val_accuracy: 0.9757 - val_loss: 0.1124\n",
      "Epoch 473/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9913 - loss: 0.0326\n",
      "Epoch 473: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9913 - loss: 0.0326 - val_accuracy: 0.9714 - val_loss: 0.1131\n",
      "Epoch 474/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0327\n",
      "Epoch 474: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9900 - loss: 0.0327 - val_accuracy: 0.9748 - val_loss: 0.1124\n",
      "Epoch 475/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9915 - loss: 0.0327\n",
      "Epoch 475: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9915 - loss: 0.0327 - val_accuracy: 0.9714 - val_loss: 0.1136\n",
      "Epoch 476/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9896 - loss: 0.0328\n",
      "Epoch 476: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9896 - loss: 0.0328 - val_accuracy: 0.9748 - val_loss: 0.1130\n",
      "Epoch 477/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9920 - loss: 0.0328\n",
      "Epoch 477: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9920 - loss: 0.0328 - val_accuracy: 0.9714 - val_loss: 0.1143\n",
      "Epoch 478/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9896 - loss: 0.0329\n",
      "Epoch 478: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9896 - loss: 0.0329 - val_accuracy: 0.9757 - val_loss: 0.1130\n",
      "Epoch 479/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9920 - loss: 0.0327\n",
      "Epoch 479: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9920 - loss: 0.0327 - val_accuracy: 0.9714 - val_loss: 0.1135\n",
      "Epoch 480/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9896 - loss: 0.0324\n",
      "Epoch 480: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9896 - loss: 0.0324 - val_accuracy: 0.9757 - val_loss: 0.1120\n",
      "Epoch 481/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9920 - loss: 0.0322\n",
      "Epoch 481: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9920 - loss: 0.0322 - val_accuracy: 0.9714 - val_loss: 0.1130\n",
      "Epoch 482/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0319\n",
      "Epoch 482: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0319 - val_accuracy: 0.9740 - val_loss: 0.1129\n",
      "Epoch 483/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9909 - loss: 0.0318\n",
      "Epoch 483: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9909 - loss: 0.0318 - val_accuracy: 0.9731 - val_loss: 0.1132\n",
      "Epoch 484/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0316\n",
      "Epoch 484: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9904 - loss: 0.0316 - val_accuracy: 0.9740 - val_loss: 0.1129\n",
      "Epoch 485/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0315\n",
      "Epoch 485: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0315 - val_accuracy: 0.9731 - val_loss: 0.1129\n",
      "Epoch 486/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9907 - loss: 0.0315\n",
      "Epoch 486: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9907 - loss: 0.0315 - val_accuracy: 0.9731 - val_loss: 0.1130\n",
      "Epoch 487/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0314\n",
      "Epoch 487: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9904 - loss: 0.0314 - val_accuracy: 0.9740 - val_loss: 0.1130\n",
      "Epoch 488/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9909 - loss: 0.0314\n",
      "Epoch 488: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9909 - loss: 0.0314 - val_accuracy: 0.9714 - val_loss: 0.1130\n",
      "Epoch 489/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9904 - loss: 0.0313\n",
      "Epoch 489: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9904 - loss: 0.0313 - val_accuracy: 0.9740 - val_loss: 0.1131\n",
      "Epoch 490/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9915 - loss: 0.0313\n",
      "Epoch 490: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9915 - loss: 0.0313 - val_accuracy: 0.9714 - val_loss: 0.1134\n",
      "Epoch 491/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.9904 - loss: 0.0313\n",
      "Epoch 491: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9904 - loss: 0.0313 - val_accuracy: 0.9766 - val_loss: 0.1130\n",
      "Epoch 492/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9922 - loss: 0.0313\n",
      "Epoch 492: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9922 - loss: 0.0313 - val_accuracy: 0.9714 - val_loss: 0.1137\n",
      "Epoch 493/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9900 - loss: 0.0313\n",
      "Epoch 493: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9900 - loss: 0.0313 - val_accuracy: 0.9757 - val_loss: 0.1128\n",
      "Epoch 494/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0314\n",
      "Epoch 494: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9924 - loss: 0.0314 - val_accuracy: 0.9722 - val_loss: 0.1139\n",
      "Epoch 495/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9902 - loss: 0.0316\n",
      "Epoch 495: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9902 - loss: 0.0316 - val_accuracy: 0.9731 - val_loss: 0.1131\n",
      "Epoch 496/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0317\n",
      "Epoch 496: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9924 - loss: 0.0317 - val_accuracy: 0.9722 - val_loss: 0.1150\n",
      "Epoch 497/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9898 - loss: 0.0319\n",
      "Epoch 497: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9898 - loss: 0.0319 - val_accuracy: 0.9731 - val_loss: 0.1134\n",
      "Epoch 498/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0319\n",
      "Epoch 498: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9924 - loss: 0.0319 - val_accuracy: 0.9731 - val_loss: 0.1155\n",
      "Epoch 499/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9891 - loss: 0.0319\n",
      "Epoch 499: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9891 - loss: 0.0319 - val_accuracy: 0.9740 - val_loss: 0.1134\n",
      "Epoch 500/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9924 - loss: 0.0319\n",
      "Epoch 500: val_accuracy did not improve from 0.97743\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9924 - loss: 0.0319 - val_accuracy: 0.9722 - val_loss: 0.1150\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "num_classes = 3\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath=f'MLP.keras', \n",
    "        verbose=1, \n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True, \n",
    "        mode='max'\n",
    "    )\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=8192, verbose=1, validation_split=0.2, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4329cbf6-b303-465d-b0b0-1d318d3fb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"MLP.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e136186b-e082-40d5-97d0-f00d41c89b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 01:03:10.789191: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:10.855512: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:11.001752: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 408 bytes spill stores, 444 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/60\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:26\u001b[0m 1s/step - accuracy: 0.9375 - loss: 0.2290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 01:03:12.136874: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 252 bytes spill stores, 252 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:12.137791: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_49', 392 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2024-11-29 01:03:12.154719: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_56', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9739 - loss: 0.0901\n",
      "Test results - Loss: 0.10442788153886795 - Accuracy: 97.34%\n"
     ]
    }
   ],
   "source": [
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ad230d3-8fed-4733-a88c-fe3bafe8f866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step \n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "367e1c58-f9a1-4963-9f7d-c60abbe42580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_test_pred = to_categorical(np.argmax(model.predict(X_test), axis=1), num_classes=3)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e75d39d-2bd0-428a-bd6f-7855d15e6932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity=array([0.98522459, 0.75      , 0.93413174])\n",
      "specificity=array([0.88986784, 0.99246907, 0.99315068])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7d8f6cfa1040>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, multilabel_confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
    "mcm = multilabel_confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "tp / (tp + fn)\n",
    "\n",
    "# Sensitivity (Recall or True Positive Rate)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print(f\"{sensitivity=}\")\n",
    "\n",
    "# Specificity (True Negative Rate)\n",
    "specificity = tn / (tn + fp)\n",
    "print(f\"{specificity=}\")\n",
    "\n",
    "labels = [\"negative\", \"hyperthyroid\", \"hypothyroid\"]\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=labels)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21033cf1-491b-4cec-add6-bef52f928f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
